<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Robin Smith</title>
    <link>https://forgottenmaster.github.io/</link>
    <description>Recent content on Robin Smith</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 06 Apr 2024 17:33:50 +0100</lastBuildDate>
    <atom:link href="https://forgottenmaster.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>7. Spawning</title>
      <link>https://forgottenmaster.github.io/posts/unreal/learn-unreal/warehouse-wreckage/spawning/</link>
      <pubDate>Sat, 06 Apr 2024 17:33:50 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/unreal/learn-unreal/warehouse-wreckage/spawning/</guid>
      <description>We need to be able to spawn new instances of the blueprint class/prefab that we&amp;rsquo;ve created in the previous section in order to be able to have it act as a projectile for a physics based game. Luckily Unreal makes this easy enough to do.
Adding a Spawner First of all we&amp;rsquo;ll add a spawner to the level. This will be an invisible Actor which is only present to be able to get the transform from which to spawn the projectiles.</description>
    </item>
    <item>
      <title>6. Blueprint Classes</title>
      <link>https://forgottenmaster.github.io/posts/unreal/learn-unreal/warehouse-wreckage/blueprint-classes/</link>
      <pubDate>Sat, 06 Apr 2024 17:04:32 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/unreal/learn-unreal/warehouse-wreckage/blueprint-classes/</guid>
      <description>In Unity there&amp;rsquo;s the concept of having a prefab which is a template for a GameObject to be spawned. Unreal has a similar concept although they&amp;rsquo;re called Blueprint Classes. This small post is showing how to convert an existing actor into a blueprint class which can then be instantiated multiple times.
Adding a Sphere Firstly we can start off by adding a sphere actor as just a standalone actor in the scene.</description>
    </item>
    <item>
      <title>5. Adding an Impulse</title>
      <link>https://forgottenmaster.github.io/posts/unreal/learn-unreal/warehouse-wreckage/adding-an-impulse/</link>
      <pubDate>Sat, 06 Apr 2024 16:28:03 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/unreal/learn-unreal/warehouse-wreckage/adding-an-impulse/</guid>
      <description>After covering the basics of taking references to actors in order to retrieve data from them, as in the last post that printed the mass, the next logical step is to be able to modify them. In particular, for warehouse wreckage which will be a physics sandbox type game, we need to be able to fire objects with an impulse.
What is an Impulse? An impulse in physics is an instantaneous force relating to velocity change (which is an instantaneous amount), rather than acceleration which is a change over time.</description>
    </item>
    <item>
      <title>4. Object References</title>
      <link>https://forgottenmaster.github.io/posts/unreal/learn-unreal/warehouse-wreckage/object-references/</link>
      <pubDate>Sat, 06 Apr 2024 13:31:41 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/unreal/learn-unreal/warehouse-wreckage/object-references/</guid>
      <description>Next, if the blueprint Event Graph is going to do anything useful, it will be necessary to be able to interact with objects in the scene. In order to do that we&amp;rsquo;ll look at how to get a reference to an actor in the scene, along with how to get some information from it.
Adding Actor Reference In order to add a reference to an actor in the scene into a blueprint event graph, just click the object in the scene to select it, and swap over to the event graph.</description>
    </item>
    <item>
      <title>3. Physics Simulation</title>
      <link>https://forgottenmaster.github.io/posts/unreal/learn-unreal/warehouse-wreckage/physics-simulation/</link>
      <pubDate>Sat, 06 Apr 2024 13:04:11 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/unreal/learn-unreal/warehouse-wreckage/physics-simulation/</guid>
      <description>The next thing to briefly look at is how to enable physics on the actors in the scene. This first project will be a physics-based game so we&amp;rsquo;ll need to know how to set objects to participate in the physics simulation so we can knock them over.
Setting Physics Toggling on physics is super simple (barely an inconvenience) in Unreal, we simply select an object we want to tweak - either by cicking on it in the scene viewport, or by clicking on it in the Outliner window.</description>
    </item>
    <item>
      <title>2. Level Blueprint</title>
      <link>https://forgottenmaster.github.io/posts/unreal/learn-unreal/warehouse-wreckage/level-blueprint/</link>
      <pubDate>Sat, 06 Apr 2024 12:25:17 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/unreal/learn-unreal/warehouse-wreckage/level-blueprint/</guid>
      <description>After setting up the project and getting used to the editor, the next thing to do is to start getting comfy with the blueprint graph editor, and hook up some basic nodes to see how it all works. However, there is one thing that we will need to do from UE version 5.1 and onwards.
Duplicating the Starter Map From UE 5.1 and onwards, the StarterContent folder is re-initialised on startup and any changes would be lost.</description>
    </item>
    <item>
      <title>1. Setup</title>
      <link>https://forgottenmaster.github.io/posts/unreal/learn-unreal/warehouse-wreckage/setup/</link>
      <pubDate>Sat, 06 Apr 2024 11:25:58 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/unreal/learn-unreal/warehouse-wreckage/setup/</guid>
      <description>The first thing that is required in order to begin developing the game will be to ensure everything is set up correctly. Firstly, if not already, follow the steps outlined HERE in order to install and get familiar with basic functionality in the editor.
Creating the Project First step is to create the project. This is straightforward enough, just make sure the settings are as shown in the screenshot below.</description>
    </item>
    <item>
      <title>17. Textures Part 1</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/textures-1/</link>
      <pubDate>Sun, 31 Mar 2024 14:30:59 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/textures-1/</guid>
      <description>So far we have two animated but flat-color shaded quads in the scene, but we will often want to be able to render textures to models in an actual application. This post focuses on loading the image data into memory,and setting up pipeline barriers for getting the image into the correct format for the correct part of the pipeline. The next post will deal with actually setting up descriptors and allowing the shader to access the image through a sampler.</description>
    </item>
    <item>
      <title>Setup</title>
      <link>https://forgottenmaster.github.io/posts/unreal/learn-unreal/setup/</link>
      <pubDate>Sat, 30 Mar 2024 21:34:40 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/unreal/learn-unreal/setup/</guid>
      <description>In order to be able to develop with Unreal engine, we first need to get it set up and learn how to navigate the editor before starting to actually make anything in it. This post is just a quick overview of where to install it and basic controls.
Installation The Epic games launcher is required to be able to download versions of the Unreal engine which can be found HERE. It should detect your OS and present the appropriate download option for you.</description>
    </item>
    <item>
      <title>16. Depth Buffers</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/depth-buffer/</link>
      <pubDate>Sun, 23 Jul 2023 10:19:00 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/depth-buffer/</guid>
      <description>Now that we have the ability to render multiple meshes at different 3-D locations, we can notice that placing one mesh in front of another in world space does not necessarily cause the fragments behind to be hidden. Instead, the fragments appearing in front will be those of the triangles rendered last.
Without the concept of a depth buffer we would have to keep our triangles sorted by distance from the camera and then rendered back to front.</description>
    </item>
    <item>
      <title>15. Push Constants</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/push-constants/</link>
      <pubDate>Sun, 30 Apr 2023 14:56:27 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/push-constants/</guid>
      <description>Although dynamic UBOs allow us to keep all the model matrices together for our entities, in order to update the data in them we need to map the memory, write to the memory, and unmap the memory. This process of writing to memory is particularly slow and for frequently changing data (like a model matrix for our spinning cubes), it is too much of an overhead. However, Vulkan gives us a pathway to a small amount of fast memory which we can put our model matrices into.</description>
    </item>
    <item>
      <title>14. Dynamic Ubos</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/dynamic-ubos/</link>
      <pubDate>Sat, 22 Apr 2023 19:27:31 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/dynamic-ubos/</guid>
      <description>Now we have the ability to pass data through to the shaders using uniform buffers, we should extend our system to be able to render multiple meshes rather than a single one as we have currently. This will also demonstrate an issue we have currently with our implementation of using a single uniform buffer.
Define Meshes The first thing we need to do is to be able to define what a Mesh actually is.</description>
    </item>
    <item>
      <title>13. Uniform Buffer Objects</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/uniform-buffer-objects/</link>
      <pubDate>Thu, 09 Mar 2023 19:16:52 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/uniform-buffer-objects/</guid>
      <description>Currently although we are defining the 4 vertices of a square what we actually have is a rectangle due to the scaling of the window. This is more obvious on windows that are much wider, or taller than the other dimension. As an example let&amp;rsquo;s take a look at our current implementation in a window with a width of 1200 and a height of only 600
Solution In order to solve this issue we will need to transform the vertices in our vertex shader by a model-view-projection trio of matrices.</description>
    </item>
    <item>
      <title>12. Index Buffers</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/index-buffers/</link>
      <pubDate>Sat, 04 Mar 2023 23:06:01 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/index-buffers/</guid>
      <description>Currently, we are rendering a multi-color square as two triangles. However, two of the vertices are shared between both triangles and currently we have to duplicate all the vertex data for those shared vertices.
Using an index buffer allows us to only duplicate indices which then index into the vertex buffer for the actual data. An index may only be 2 bytes and so duplicating that as opposed to duplicating N bytes of vertex data results in large savings.</description>
    </item>
    <item>
      <title>11. Staging Buffers</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/staging-buffers/</link>
      <pubDate>Sat, 04 Mar 2023 19:56:24 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/staging-buffers/</guid>
      <description>Currently our vertex buffer is residing on the GPU but in a section of memory that is visible to the CPU. This incurs an overhead on the GPU so that it&amp;rsquo;s not able to process that memory as fast as it could if it knew the CPU could not see it.
Memory Types In Vulkan there are various pools of memory with different properties that we can use to allocate buffer memory.</description>
    </item>
    <item>
      <title>10. Vertex Buffers</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/vertex-buffers/</link>
      <pubDate>Wed, 01 Mar 2023 17:26:02 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/vertex-buffers/</guid>
      <description>We now have a triangle rendering to the screen which proves that our setup is all correct and valid. However currently our vertex positions and colors are baked into our vertex shader.
The next step is to allow our application to define the vertex data to be passed into our graphics pipeline, which we can do with vertex buffers.
Vertex Data The first thing we want to do is to define how we will store our vertex data.</description>
    </item>
    <item>
      <title>9. Drawing</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/drawing/</link>
      <pubDate>Sun, 26 Feb 2023 16:30:01 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/drawing/</guid>
      <description>We can finally start drawing on the screen now that we&amp;rsquo;ve got our framebuffers and recorded our command buffers. In order to render to the screen we will have to acquire an image from the swapchain, submit our command buffer that we recorded to the graphics queue, and then submit the image for presentation on the presentation queue. We will also however need to worry about synchronization.
Semaphores &amp;amp; Fences There are two kinds of synchronization primitives in Vulkan which are</description>
    </item>
    <item>
      <title>8. Framebuffers and Command Buffers</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/framebuffers-and-command-buffers/</link>
      <pubDate>Sun, 26 Feb 2023 00:10:55 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/framebuffers-and-command-buffers/</guid>
      <description>Before we are able to render anything to the screen we need to fill in a couple of final missing pieces. These are the framebuffers, and the command buffers.
A framebuffer is associated with a particular render pass, and defines the actual image views for the attachments that render pass expects. For example when we created the render pass, we may have specified we needed two attachments - a color attachment, and a depth attachment.</description>
    </item>
    <item>
      <title>7. Graphics Pipeline</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/graphics-pipeline/</link>
      <pubDate>Sat, 25 Feb 2023 16:21:48 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/graphics-pipeline/</guid>
      <description>Now that we have somewhere to render to (surface), and something to render with (swapchain), we need to define how to render. This is going to be the responsibility of the graphics pipeline which we must set up manually.
Overview An overview of the graphics pipeline is shown below
Overview of graphics pipelineVulkan Tutorial
The sections highlighted in yellow indicate those sections which we can program through shader modules. The green stages are fixed functionality, but we can tweak the settings for them.</description>
    </item>
    <item>
      <title>6. Shaders</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/shaders/</link>
      <pubDate>Tue, 21 Feb 2023 23:26:25 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/shaders/</guid>
      <description>Before we can set up the graphics pipeline we need to be able to render things to the surface, we need to be able to create shader modules, since shaders are going to do the work in a Vulkan application.
Shaders in Vulkan Unlike OpenGL which accepts shaders in source code form, a Vulkan application does not accept shader code in source code form but in a binary format known as SPIR-V.</description>
    </item>
    <item>
      <title>5. Swapchain</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/swapchain/</link>
      <pubDate>Sun, 19 Feb 2023 12:21:42 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/swapchain/</guid>
      <description>Now that we&amp;rsquo;ve been able to create the device surface, and get appropriate information about what formats, color spaces, and presentation modes are available, we are ready to tackle the next step in setting up a rendering environment. This next step will be what&amp;rsquo;s called a swapchain
What is screen tearing? On the front-end side of things, the device surface represents the rendering context and can be considered &amp;ldquo;the window&amp;rdquo; for our purposes.</description>
    </item>
    <item>
      <title>4. Surface</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/surface/</link>
      <pubDate>Sat, 18 Feb 2023 12:08:01 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/surface/</guid>
      <description>The next thing we need to set up is called a surface. Since Vulkan does not interact directly with the windowing system in use, due to OS differences etc.
Instead, Vulkan uses a common API to interact with a surface, and then uses extensions on a per platform basis to handle the differences of actually creating this surface type.
Surface extensions We could manually detect the platform we&amp;rsquo;re compiling on, and then enable the appropriate extension through the Vulkan API, for example we could do:</description>
    </item>
    <item>
      <title>3. Validation</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/validation/</link>
      <pubDate>Sat, 18 Feb 2023 01:04:25 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/validation/</guid>
      <description>By default, Vulkan will not perform any validation above or beyond checking preconditions for API calls. It won&amp;rsquo;t catch things like forgetting to destroy a Vulkan object, or other issues with parameters passed to API calls.
This is because Vulkan will do the bare minimum by default for efficiency - what we don&amp;rsquo;t use we don&amp;rsquo;t pay for, and this includes extra validations that can cause incorrect but safe behaviours.</description>
    </item>
    <item>
      <title>2. The Window, the Instance, and the Device</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/instance-and-device/</link>
      <pubDate>Wed, 15 Feb 2023 22:32:00 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/instance-and-device/</guid>
      <description>The first things we&amp;rsquo;ll need to do to start using Vulkan will be to open a window to render into, create a Vulkan instance, and create a logical Vulkan device from that instance.
The end result will just show a blank white window as shown below.
Event loop and window The first step which we can get out of the way before even touching Vulkan will be to create a window for us to eventually render into, and an event loop that winit uses to allow us to handle the events from the user such as resizing etc.</description>
    </item>
    <item>
      <title>1. Setup</title>
      <link>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/setup/</link>
      <pubDate>Mon, 09 Jan 2023 22:43:32 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/vulkan/lets-learn-vulkan/setup/</guid>
      <description>In order to follow along with the course, we&amp;rsquo;ll want to get the project up and running, and nail down the tech stack we&amp;rsquo;ll be using.
The Stack The language - Rust The Vulkan crate - ash The windowing crate - winit The window abstraction crate - raw-window-handle The glue crate - ash-window Install Rust If the Rust toolchain is already installed, we can skip this bit. Luckily Rust is easy enough to install.</description>
    </item>
    <item>
      <title>Wrap Up</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/wrapup/</link>
      <pubDate>Sat, 31 Dec 2022 17:18:57 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/wrapup/</guid>
      <description>After running through the book and getting a fully working feed-forward deep learning API and learning the inner workings of neural networks as a result, I&amp;rsquo;ve decided not to pursue this particular book any further.
The purpose of reading this particular book was to enable me to get a better understanding into what makes neural networks tick, and as a result it was necessary to write this series of posts with diagrams, maths, and code.</description>
    </item>
    <item>
      <title>5.2 - Convolutional Layers</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter5/convolutionlayers/</link>
      <pubDate>Fri, 30 Dec 2022 23:05:00 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter5/convolutionlayers/</guid>
      <description>The previous post explained the basic multichannel convolution operation but we now take a look at how we can integrate it into a neural network.
Architecture In a regular dense layer we can think of a first hidden layer outputting h1 neurons, or learned features.
If we have another hidden layer taking these h1 neurons as input and producing h2 neurons then this layer needs h1 x h2 weights in order to represent that each of the output neurons is some linear combination of all the input neurons.</description>
    </item>
    <item>
      <title>5.1 - Introduction</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter5/introduction/</link>
      <pubDate>Fri, 30 Dec 2022 22:17:58 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter5/introduction/</guid>
      <description>With the standard feed-forward architecture we used in the chapters until this point, we are able to input a collection of observations, with each observation having a number of features. In the case of the MNIST recognition network, these features were the greyscale pixel values.
However, such a &amp;ldquo;flat&amp;rdquo; model has no knowledge or concept of &amp;ldquo;closeness&amp;rdquo; of features to one another. This means that the learned features in a hidden layer, derived as some linear combination of the input pixel data could draw from pixels that are nowhere near each other in the input.</description>
    </item>
    <item>
      <title>Part 6 - Forward &amp; Backward</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/forwardbackward/</link>
      <pubDate>Fri, 07 Oct 2022 00:23:32 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/forwardbackward/</guid>
      <description>We now almost have all the pieces in place in the refactored API to be able to train a neural network using the described typestates in the previous sections. The final thing we need to be able to do is to run the forward and backward passes, and apply optimisation to the network weights.
In the last post I covered the Forward trait and described why it needs to be generic over a lifetime, so we have our way of performing the forward pass.</description>
    </item>
    <item>
      <title>Part 5 - Trainable State</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/trainable/</link>
      <pubDate>Tue, 26 Jul 2022 20:26:21 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/trainable/</guid>
      <description>Now we&amp;rsquo;ve bound a particular optimiser to the operation(s) it is placed into the trainable state. The optimiser instances will retain additional state for gradients, learning rates, etc. that aren&amp;rsquo;t needed in the previous initialised state.
In this typestate we are able to run training passes (as opposed to just predictions) on the network and also, once finished, to go back to the initialised state again from which we can get the trained parameters.</description>
    </item>
    <item>
      <title>Part 4 - Initialised Operations</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/initialised/</link>
      <pubDate>Sun, 24 Jul 2022 21:06:33 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/initialised/</guid>
      <description>After building the network/operation chain and initialising it with weights/parameters either via a known set (through previous training), or via a random seed (using Xavier initialisation) we have a network that is capable of making predictions given some input. We are also at this point able to read out the set of parameters for later use.
We are only able to get an operation into the initialised state by first starting with an uninitialised version and then initialising it, we can&amp;rsquo;t directly construct new instances of this type because it&amp;rsquo;s not publicly visible/nameable.</description>
    </item>
    <item>
      <title>Part 3 - Uninitialised Operations</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/uninitialisedoperations/</link>
      <pubDate>Sun, 24 Jul 2022 17:28:00 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/uninitialisedoperations/</guid>
      <description>Now that we&amp;rsquo;ve covered tensors which are the representation of data that is flowing through the API, we can start taking a look at the operations and how we are using typestates to ensure valid usage of the API without unnecessary bloat in code by keeping members around that aren&amp;rsquo;t necessary.
What are typestates anyway? If you aren&amp;rsquo;t used to programming with generics at the type level, you may not have encountered type states before, so a quick explanation of what they are might be necessary.</description>
    </item>
    <item>
      <title>Part 2 - Tensors</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/tensors/</link>
      <pubDate>Sun, 24 Jul 2022 12:48:11 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/tensors/</guid>
      <description>A &amp;ldquo;tensor&amp;rdquo; is just a fancy name for &amp;ldquo;n-dimensional array&amp;rdquo; (AKA ndarray) and provides a way of thinking about scalars, vectors, matrices, etc. in a uniform way. We refactor Eidetic to use this concept because we want operations to be able to specify how many dimensions are in the input and output data. This post will provide a brief overview of what a tensor is, and how we implement it in Eidetic.</description>
    </item>
    <item>
      <title>Part 1 - Rationale</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/rationale/</link>
      <pubDate>Sun, 24 Jul 2022 12:06:05 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/rationale/</guid>
      <description>The following sections will detail the implementation of the refactoring of our Eidetic API of which the new version can be found HERE.
This post in particular will provide a brief overview of the problems that the refactoring will address, then we will look at the step-by-step implementation of the new Eidetic API at a relatively high level since going into every line of code as we did with the initial implementation would take too long.</description>
    </item>
    <item>
      <title>Extra - Training on the MNIST</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/mnist/</link>
      <pubDate>Tue, 10 May 2022 23:38:37 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/mnist/</guid>
      <description>At this point we&amp;rsquo;ve successfully coded a neural network library that should be extensible enough to create various architectures of densely connected layers. We&amp;rsquo;ve also implemented various techniques that can be used to potentially get better results from the network.
The next chapters will be about entirely new network architectures, namely Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), but now that we have enough pieces for training a standard feed forward network, I want to try out training on something more substantial and meaningful than just a simple mathematical formula or comparison operation such as we&amp;rsquo;ve seen so far.</description>
    </item>
    <item>
      <title>4.6 - Dropout</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/dropout/</link>
      <pubDate>Sat, 07 May 2022 02:11:16 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/dropout/</guid>
      <description>Out of all the modifications for neural networks we&amp;rsquo;ve covered in order to try to get the network to train the problem &amp;ldquo;better&amp;rdquo;, we haven&amp;rsquo;t actually tried what people would think is the most obvious - adding more neurons to the network.
Introduction Adding more neurons to the network can take one of two forms:
Adding more neurons to a hidden layer in the network (expanding wide) Adding more hidden layers to the network (expanding long) More neurons means more parameters in the network that can be tweaked - if you recall, each weight is a contribution to the overall prediction and a set of neurons in the network can represent some &amp;ldquo;learned feature&amp;rdquo; that the network deems important.</description>
    </item>
    <item>
      <title>4.5 - Learning Rate Decay</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/learningratedecay/</link>
      <pubDate>Fri, 06 May 2022 01:12:54 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/learningratedecay/</guid>
      <description>As we&amp;rsquo;ve covered a couple of different optimisation strategies (stochastic gradient descent with and without momentum), it&amp;rsquo;s become clear that the learning rate hyperparameter is one of, if not the most important hyperparameters in deep learning.
A hyperparameter in deep learning is a tweakable value that needs to be tuned based on the problem at hand. These include the learning rate, momentum, epoch count, batch size, etc. and half of the success for training a network is finding the correct hyperparameter values.</description>
    </item>
    <item>
      <title>4.4 - Momentum</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/momentum/</link>
      <pubDate>Thu, 05 May 2022 19:13:16 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/momentum/</guid>
      <description>So far, when we have updated the weights on our parameters with their gradients, it has been an instantaneous update without any temporal relationship with previous optimisation steps.
This means that between two successive epochs, the velocity change can be instantaneous. One epoch the velocity could be low, and the next it might be super high.
In terms of the updating of the weights with respect to the gradients of parameters, we can imagine this from the graph visualisation as being the object moving up and down the line of the graph, or bouncing around the troughs and valleys created by such a graph (there was a diagram in a previous post showing the arrows bouncing around like that).</description>
    </item>
    <item>
      <title>4.3 - Softmax</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/softmax/</link>
      <pubDate>Mon, 02 May 2022 19:33:53 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/softmax/</guid>
      <description>This post will describe a new loss function called &amp;ldquo;cross entropy loss&amp;rdquo;. Due to often being coupled with the &amp;ldquo;softmax&amp;rdquo; function, this is often just called the &amp;ldquo;softmax cross entropy loss&amp;rdquo; function. There are a couple of components needed to support that.
Softmax Thus far we used the mean squared error as a loss function which has a nice property that the further the prediction is from the target, the steeper the gradient that&amp;rsquo;s sent in the backpropagation pass is which enables it to converge quite fast.</description>
    </item>
    <item>
      <title>4.2 - Refactorings</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/refactorings/</link>
      <pubDate>Mon, 02 May 2022 19:33:52 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/refactorings/</guid>
      <description>There are a couple of things that were required to be tweaked as we go on with development of the extensions to the network API. This post will briefly explain the changes that were required before we move onto the extensions themselves.
Changing the examples Eidetic at this point has examples that are showing the performance of a linear regression vs a basic neural network, both using the &amp;ldquo;mean squared error&amp;rdquo; loss function.</description>
    </item>
    <item>
      <title>4.1 - Overview</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/overview/</link>
      <pubDate>Tue, 26 Apr 2022 23:54:29 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/overview/</guid>
      <description>Although we have managed to put together a working neural network library that allows us to build various topologies of networks and to train them to fit a non-specified mathematical function that is represented by the input data and targets, the initial implementation is only somewhat successful. We can do better in a few ways, we will explore 5 such extensions to the basic approach in the following notes.
High level review We can imagine a neural network at a high level as a black box which consists of a bunch of weights internally (grouped into layers, but we can consider this as an implementation detail), taking a bunch of input data records and producing a bunch of predictions for those records.</description>
    </item>
    <item>
      <title>Hello World</title>
      <link>https://forgottenmaster.github.io/posts/arduino/helloworld/</link>
      <pubDate>Sun, 24 Apr 2022 00:37:11 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/arduino/helloworld/</guid>
      <description>This is the first in hopefully a long series of posts in Arduino development as I learn about embedded development, microcontrollers, and electrical engineering.
The hardware The target hardware for the experimentation will be this very Arduino Uno
And, a quick look at the back will tell us that this is the R3 model
However I believe any Arduino Uno model, as well as a few other Arduinos will work with the methods and APIs we&amp;rsquo;re using</description>
    </item>
    <item>
      <title>3.9 - Examples</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/examples/</link>
      <pubDate>Mon, 18 Apr 2022 18:47:56 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/examples/</guid>
      <description>Now that we have a basic framework for creating neural networks, we will re-visit the code examples for Linear Regression and Neural Networks that were quickly implemented in chapter 2 and reimplement them as examples for the Eidetic library.
Doing this will ensure that users of our API have some reference examples of how to create and train their networks, as well as ensuring that the API fulfils the required functionality from a users perspective.</description>
    </item>
    <item>
      <title>3.8 - Training</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/training/</link>
      <pubDate>Sat, 09 Apr 2022 16:46:42 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/training/</guid>
      <description>The last part to the puzzle, the last thing we need to do is to be able to encapsulate the algorithm for training a given network. In the book, this is represented as the &amp;ldquo;Trainer&amp;rdquo; type which binds together a network with an Optimiser instance. However there is only one implementation for this training process, and customisation is instead via the various trait objects in the Network setup, along with the selected Optimiser.</description>
    </item>
    <item>
      <title>3.7 - Network Cloning</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/networkcloning/</link>
      <pubDate>Tue, 05 Apr 2022 22:45:22 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/networkcloning/</guid>
      <description>As a small detour before we talk about the final structures required for a basic and usable deep learning API, we will talk about making the Network type cloneable. As we will find out when we talk about the trainer, we need to be able to take a snapshot of the neural network for us to restore to at a later date. This requires us to be able to call the clone() method to obtain a duplicate.</description>
    </item>
    <item>
      <title>3.6 - Optimiser</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/optimiser/</link>
      <pubDate>Tue, 05 Apr 2022 22:45:21 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/optimiser/</guid>
      <description>Now that the layers of abstraction are done, we need to work on actually running the training of the network. We will be encapsulating the optimisation process to allow for the API user to select the strategy they would like to use to apply the gradients to parameters when training the network. This will be a fairly short post as the actual optimisation process is pretty simple.
The Trait An Optimiser has a single job in the network training process.</description>
    </item>
    <item>
      <title>3.5 - The Network Type</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/network/</link>
      <pubDate>Sat, 26 Mar 2022 00:57:01 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/network/</guid>
      <description>In this post we&amp;rsquo;ll take a look at implementing the next (and final) level of abstraction for a neural network, the Network type itself.
Just as how we encapsulated multiple operations in a Layer type, the Network is nothing more than a wrapper around multiple layers. However this isn&amp;rsquo;t entirely true, since the Network will also require a loss function in order to calculate the final single value measure of error for the predictions.</description>
    </item>
    <item>
      <title>3.4 - Lifetime Refactor</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/lifetimerefactor/</link>
      <pubDate>Sat, 19 Mar 2022 17:45:27 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/lifetimerefactor/</guid>
      <description>After trying to make a start on the next level of abstraction with the NeuralNetwork class, it&amp;rsquo;s becoming apparent that dealing with lifetimes at the lower levels of abstraction (Operation and Layer trait objects) is causing it more difficult to make a nice abstraction at the upper level. We will take a brief interlude to look at what the problem is, and why we need to do a refactor at this point.</description>
    </item>
    <item>
      <title>3.3 - Loss Function</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/lossfunction/</link>
      <pubDate>Fri, 04 Mar 2022 20:49:46 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/lossfunction/</guid>
      <description>Overview Once we&amp;rsquo;ve passed the data through the hidden layers of our neural network, and passed it through a final output layer (which should have only 1 neuron to bring the dimensionality of the data back to 1 feature/column) we will end up with a vector of predictions as mentioned previously (1 for each observation in the data). We already have a vector of targets which are the actual measured answers.</description>
    </item>
    <item>
      <title>3.2 - Layers</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/layers/</link>
      <pubDate>Sun, 27 Feb 2022 17:47:20 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/layers/</guid>
      <description>Overview The lowest level of abstraction as mentioned in the previous post is the operation. These were the lowest level building block that is able to perform a single unit of calculation on the forward pass, and single unit of partial derivative calculation on the backward pass.
During that chapter, we could see how multiple of these operations are able to be chained together, though we don&amp;rsquo;t want to do this by hand.</description>
    </item>
    <item>
      <title>3.1 - Operations</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/operations/</link>
      <pubDate>Sun, 20 Feb 2022 17:56:32 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/operations/</guid>
      <description>This chapter is about creating the abstractions in code that enable us to think of neural networks in a modular fashion, and build up complex deep networks with simple structures.
I will be implementing the abstractions in an accompanying library called Eidetic which is free for use and distribution, since I&amp;rsquo;m only implementing this as a learning opportunity.
Overview The Operation construct will represent and encapsulate a single operation in a neural network, such as the sigmoid function, matrix multiplication, etc.</description>
    </item>
    <item>
      <title>2.3 - Neural Networks</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/neuralnetworks/</link>
      <pubDate>Sat, 19 Feb 2022 09:43:54 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/neuralnetworks/</guid>
      <description>The previous post showed the most basic type of machine learning and how to train it, but the concepts learnt there (forward output calculations, backward gradient calculations) will apply equally here too.
What was the problem with linear regression? The main problem with linear regression that we have is that we are training to find a set of weights, which are essentially the coefficients of an N-dimensional line. Therefore we&amp;rsquo;re trying to fit a straight line to a set of points.</description>
    </item>
    <item>
      <title>2.2 - Linear Regression</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/linearregression/</link>
      <pubDate>Fri, 11 Feb 2022 10:46:23 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/linearregression/</guid>
      <description>Introduction As mentioned previously, we can represent a training set as a matrix where each row of the matrix is a record/sample, and each column is a single feature of the record.
Additionally we can have a weighting assigned to each feature so some features are &amp;ldquo;worth&amp;rdquo; more than others.
However we also require a baseline value for when every numeric feature is a 0 (we would always give 0, but we may not want this).</description>
    </item>
    <item>
      <title>2.1 - Supervised Learning</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/supervisedlearning/</link>
      <pubDate>Sat, 22 Jan 2022 11:47:50 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/supervisedlearning/</guid>
      <description>Overview Supervised learning is a branch of machine learning that can be described as finding patterns or relationships in data that has already been measured. The &amp;ldquo;supervised&amp;rdquo; part of the phrase comes from the fact that we need to monitor the output that the system gives for a given input and to tell the system how close or far away it is from the measured result.
As a short example, suppose we looked at 100 houses and measured both their square footage and market value.</description>
    </item>
    <item>
      <title>Tarpaulin</title>
      <link>https://forgottenmaster.github.io/posts/rust/infrastructure/tarpaulin/</link>
      <pubDate>Wed, 22 Dec 2021 23:57:14 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/rust/infrastructure/tarpaulin/</guid>
      <description>Since setting up my Github workflows as detailed HERE, I have noticed that the code coverage report generation tool that was being used was generating a lot of false negatives and certain code (mainly .except calls) were causing a &amp;ldquo;Not covered&amp;rdquo; status to be reported.
Tarpaulin is an alternative tool that has better integration with Rust, however the downside of it is that due to the instrumentation required it&amp;rsquo;s only available on x86_64 processors and only on the Linux OS.</description>
    </item>
    <item>
      <title>Github Workflows</title>
      <link>https://forgottenmaster.github.io/posts/rust/infrastructure/githubworkflow/</link>
      <pubDate>Mon, 13 Dec 2021 20:08:57 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/rust/infrastructure/githubworkflow/</guid>
      <description>I recently found out how to add appropriate GitHub action workflows to a Rust project in a GitHub repository. It was surprisingly easy to do with GitHub actions, and even lets you put a badge in the README.md file.
Template Repository Luckily I stumbled upon this template repository containing exactly the GitHub workflows that I needed. In order to add the appropriate workflows to a Cargo project, clone this repository. The repository is located at: https://github.</description>
    </item>
    <item>
      <title>Introduction</title>
      <link>https://forgottenmaster.github.io/posts/rust/consteval/intro/</link>
      <pubDate>Thu, 09 Dec 2021 21:07:37 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/rust/consteval/intro/</guid>
      <description>What is constant evaluation? Constant evaluation is basically just performing calculations at compile time rather than runtime. There are some constructs that we would like to represent with a high level programming language such as Rust or C++, but for which, if we know the parameters to the function at compile time, then we can also calculate the output at compile time as well.
One thing to note is that it&amp;rsquo;s not possible to do everything at compile time since we generally require I/O, and whenever I/O is involved it can no longer be performed or baked in at compile time.</description>
    </item>
    <item>
      <title>Rabin-Karp Pattern Search</title>
      <link>https://forgottenmaster.github.io/posts/algorithms/searching/rabinkarp/</link>
      <pubDate>Fri, 19 Nov 2021 12:55:58 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/algorithms/searching/rabinkarp/</guid>
      <description>Overview We often find ourselves wanting to find a smaller sequence, inside of a larger sequence. This often manifests as searching for a substring in a larger text, or determining if a list of numbers is a sublist of another.
We can achieve this by looking at a sequence of characters at the beginning of the larger sequence (known as the haystack) that is the same length as the smaller sequence (known as the needle).</description>
    </item>
    <item>
      <title>Basic Concepts</title>
      <link>https://forgottenmaster.github.io/posts/git/basic_concepts/</link>
      <pubDate>Sat, 06 Nov 2021 20:51:06 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/git/basic_concepts/</guid>
      <description>This post will cover the basic concepts of Git from a high-level perspective. It will touch the object store and the index, along with touching on how objects in a repository are stored. The next post will then cover the usage of the Git CLI.
Repository A Git repository has 2 main storages:
The object store This storage is used to store the actual data, commits, etc. for the repository and is a permanent record of all the changes to the repository.</description>
    </item>
    <item>
      <title>Basic Usage</title>
      <link>https://forgottenmaster.github.io/posts/git/basic_usage/</link>
      <pubDate>Sat, 06 Nov 2021 18:36:42 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/posts/git/basic_usage/</guid>
      <description>This post will delve into the usage of the Git CLI from a user&amp;rsquo;s perspective. It will cover basic usage such as cloning, pulling, pushing, and will touch on how the commands interact with the index and object store of the repository.
Creating a repository There are two different ways to establish a repository in Git in contrast to other version control systems where there is only creating the initial repository.</description>
    </item>
    <item>
      <title>Traits</title>
      <link>https://forgottenmaster.github.io/posts/rust/whyrust/traits/</link>
      <pubDate>Sat, 23 Oct 2021 09:55:16 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/rust/whyrust/traits/</guid>
      <description>What are traits? Traits in Rust can basically be thought of as interfaces in C#. They can do everything that a C# interface can do except with a few more capabilities. We will start off by equating the common functionality of traits in Rust with C# interfaces, and then explore the additional capabilities we get with Rust traits.
Describes capabilities In C# interfaces, we can describe a set of function signatures which will tell the user the capabilities of that interface, such that they know when they call something what data to pass in, and what they should get back.</description>
    </item>
    <item>
      <title>Memory Safety</title>
      <link>https://forgottenmaster.github.io/posts/rust/whyrust/memorysafety/</link>
      <pubDate>Fri, 15 Oct 2021 01:31:14 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/rust/whyrust/memorysafety/</guid>
      <description>Here we&amp;rsquo;ll go over the memory model in Rust, along with the language design choices that allow for Rust to guarantee memory safety in any programs written using it. We will also be comparing Rust against C++ as they are both low level programming languages that are suitable for systems programming.
Memory guarantees The Rust language was designed with memory safety in mind and as such, the following guarantees are always true in a Rust program:</description>
    </item>
    <item>
      <title>1.6 - Matrix Inputs</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/matrixinputs/</link>
      <pubDate>Wed, 13 Oct 2021 23:47:35 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/matrixinputs/</guid>
      <description>The final piece of the puzzle in the mathematical foundations needed is to determine how we can trace back a derivative with respect to matrix inputs.
In the previous post we saw that calculating the derivative of two vectors of equal length when passing through the dot product operation results in the transpose of the other.
Matrix Multiplication Refresher The first step is to recall how we can multiply two matrices together.</description>
    </item>
    <item>
      <title>1.5 - Vector Inputs</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/vectorinputs/</link>
      <pubDate>Tue, 12 Oct 2021 00:41:56 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/vectorinputs/</guid>
      <description>After learning what a derivative of a function is, and how to apply the chain rule to a composite function, we then learned how to calculate the derivative of a function with multiple inputs by tracing back through the chain of functions following the route of one of the parameters while holding the others constant.
The next step in the prerequisite mathematics we need to build the foundations of a neural network is to determine what the derivative of a function means when one or more of the inputs is a vector.</description>
    </item>
    <item>
      <title>1.4 - Multiple Inputs</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/multipleinputs/</link>
      <pubDate>Sat, 09 Oct 2021 15:40:05 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/multipleinputs/</guid>
      <description>So far, we&amp;rsquo;ve learned how to chain functions together, and how to calculate the derivative of a function at a specific input value. We&amp;rsquo;ve also learned how to apply the chain rule when we are chaining multiple functions together. However, how do we calculate the derivative of a function, or chain of functions when a function has multiple inputs?.
As it turns out, we can calculate the derivative in the same way!</description>
    </item>
    <item>
      <title>Enums</title>
      <link>https://forgottenmaster.github.io/posts/rust/whyrust/enums/</link>
      <pubDate>Sun, 26 Sep 2021 17:14:48 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/rust/whyrust/enums/</guid>
      <description>C Style Enums In C#, C++, and a lot of other popular programming languages, we have access to a type called an &amp;ldquo;enumeration&amp;rdquo; (or enum for short). This is simply a type safe collection of named constant values.
For example in C++, making an enum whose variants represent a set of allowed colors for a hypothetical UI framework could be written as (with the values of the variants explicitly typed out for transparency):</description>
    </item>
    <item>
      <title>1.3 - Chain Rule</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/chainrule/</link>
      <pubDate>Sun, 26 Sep 2021 16:09:52 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/chainrule/</guid>
      <description>The chain rule allows us to determine the derivative of a composite function as a product of the derivatives of each of the individual functions.
The reason this is a product is because, say you had a function &amp;ldquo;f&amp;rdquo; which has a derivative of 3 at a particular value of x. That means, when you increase the input by 1, the output increases by 3.
Then say you have a second function &amp;ldquo;g&amp;rdquo; which has a derivative of 5 at the value output by &amp;ldquo;f&amp;rdquo; for x.</description>
    </item>
    <item>
      <title>1.2 - Derivatives</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/derivatives/</link>
      <pubDate>Sun, 26 Sep 2021 00:42:04 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/derivatives/</guid>
      <description>Maths The derivative of a function is the rate at which the output changes with respect to a change in the input at a specific value for the input. This last part is important as the derivative of a function is essentially the gradient, or tangent of the graph of that function at a specific point which can of course change depending on where you are on the number line.</description>
    </item>
    <item>
      <title>1.1 - Functions</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/functions/</link>
      <pubDate>Fri, 24 Sep 2021 01:45:11 +0100</pubDate>
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/functions/</guid>
      <description>There isn&amp;rsquo;t too much to say for what a function actually is, since being a programmer, we use them every day. However for completeness I&amp;rsquo;ll include a post here.
For our purposes it will help to think of a function as a black box, that takes one or more inputs and returns an output. We can then chain these functions together by taking the output of function 1, and passing it to the next function and so on.</description>
    </item>
    <item>
      <title>Résumé</title>
      <link>https://forgottenmaster.github.io/resume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://forgottenmaster.github.io/resume/</guid>
      <description>Ubisoft Reflections
Principal Gameplay Programmer - Unannounced Title
November 2023 - Present
After moving from Assassin&amp;rsquo;s Creed Nexus VR to an unannounced title, I continue my responsibilities as a principal gameplay programmer by learning the existing frameworks and features, collaborating with others to improve them, and add new functionality required by design (which I then take ownership of). I will continue to be a point of contact and driving force for improvements in this position.</description>
    </item>
  </channel>
</rss>
