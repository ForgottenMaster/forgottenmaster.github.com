<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Robin Firth</title>
    <link>https://forgottenmaster.github.io/</link>
    <description>Recent content on Robin Firth</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Sat, 19 Mar 2022 17:45:27 +0000</lastBuildDate><atom:link href="https://forgottenmaster.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Lifetime Refactor</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/lifetimerefactor/</link>
      <pubDate>Sat, 19 Mar 2022 17:45:27 +0000</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/lifetimerefactor/</guid>
      <description>After trying to make a start on the next level of abstraction with the NeuralNetwork class, it&amp;rsquo;s becoming apparent that dealing with lifetimes at the lower levels of abstraction (Operation and Layer trait objects) is causing it more difficult to make a nice abstraction at the upper level. We will take a brief interlude to look at what the problem is, and why we need to do a refactor at this point.</description>
    </item>
    
    <item>
      <title>Loss Function</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/lossfunction/</link>
      <pubDate>Fri, 04 Mar 2022 20:49:46 +0000</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/lossfunction/</guid>
      <description>Overview Once we&amp;rsquo;ve passed the data through the hidden layers of our neural network, and passed it through a final output layer (which should have only 1 neuron to bring the dimensionality of the data back to 1 feature/column) we will end up with a vector of predictions as mentioned previously (1 for each observation in the data). We already have a vector of targets which are the actual measured answers.</description>
    </item>
    
    <item>
      <title>Layers</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/layers/</link>
      <pubDate>Sun, 27 Feb 2022 17:47:20 +0000</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/layers/</guid>
      <description>Overview The lowest level of abstraction as mentioned in the previous post is the operation. These were the lowest level building block that is able to perform a single unit of calculation on the forward pass, and single unit of partial derivative calculation on the backward pass.
During that chapter, we could see how multiple of these operations are able to be chained together, though we don&amp;rsquo;t want to do this by hand.</description>
    </item>
    
    <item>
      <title>Operations</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/operations/</link>
      <pubDate>Sun, 20 Feb 2022 17:56:32 +0000</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/operations/</guid>
      <description>This chapter is about creating the abstractions in code that enable us to think of neural networks in a modular fashion, and build up complex deep networks with simple structures.
I will be implementing the abstractions in an accompanying library called Eidetic which is free for use and distribution, since I&amp;rsquo;m only implementing this as a learning opportunity.
Overview The Operation construct will represent and encapsulate a single operation in a neural network, such as the sigmoid function, matrix multiplication, etc.</description>
    </item>
    
    <item>
      <title>Neural Networks</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/neuralnetworks/</link>
      <pubDate>Sat, 19 Feb 2022 09:43:54 +0000</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/neuralnetworks/</guid>
      <description>The previous post showed the most basic type of machine learning and how to train it, but the concepts learnt there (forward output calculations, backward gradient calculations) will apply equally here too.
What was the problem with linear regression? The main problem with linear regression that we have is that we are training to find a set of weights, which are essentially the coefficients of an N-dimensional line. Therefore we&amp;rsquo;re trying to fit a straight line to a set of points.</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/linearregression/</link>
      <pubDate>Fri, 11 Feb 2022 10:46:23 +0000</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/linearregression/</guid>
      <description>Introduction As mentioned previously, we can represent a training set as a matrix where each row of the matrix is a record/sample, and each column is a single feature of the record.
Additionally we can have a weighting assigned to each feature so some features are &amp;ldquo;worth&amp;rdquo; more than others.
However we also require a baseline value for when every numeric feature is a 0 (we would always give 0, but we may not want this).</description>
    </item>
    
    <item>
      <title>Supervised Learning</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/supervisedlearning/</link>
      <pubDate>Sat, 22 Jan 2022 11:47:50 +0000</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/supervisedlearning/</guid>
      <description>Overview Supervised learning is a branch of machine learning that can be described as finding patterns or relationships in data that has already been measured. The &amp;ldquo;supervised&amp;rdquo; part of the phrase comes from the fact that we need to monitor the output that the system gives for a given input and to tell the system how close or far away it is from the measured result.
As a short example, suppose we looked at 100 houses and measured both their square footage and market value.</description>
    </item>
    
    <item>
      <title>Tarpaulin</title>
      <link>https://forgottenmaster.github.io/posts/rust/infrastructure/tarpaulin/</link>
      <pubDate>Wed, 22 Dec 2021 23:57:14 +0000</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/rust/infrastructure/tarpaulin/</guid>
      <description>Since setting up my Github workflows as detailed HERE, I have noticed that the code coverage report generation tool that was being used was generating a lot of false negatives and certain code (mainly .except calls) were causing a &amp;ldquo;Not covered&amp;rdquo; status to be reported.
Tarpaulin is an alternative tool that has better integration with Rust, however the downside of it is that due to the instrumentation required it&amp;rsquo;s only available on x86_64 processors and only on the Linux OS.</description>
    </item>
    
    <item>
      <title>Github Workflows</title>
      <link>https://forgottenmaster.github.io/posts/rust/infrastructure/githubworkflow/</link>
      <pubDate>Mon, 13 Dec 2021 20:08:57 +0000</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/rust/infrastructure/githubworkflow/</guid>
      <description>I recently found out how to add appropriate GitHub action workflows to a Rust project in a GitHub repository. It was surprisingly easy to do with GitHub actions, and even lets you put a badge in the README.md file.
Template Repository Luckily I stumbled upon this template repository containing exactly the GitHub workflows that I needed. In order to add the appropriate workflows to a Cargo project, clone this repository. The repository is located at: https://github.</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>https://forgottenmaster.github.io/posts/rust/consteval/intro/</link>
      <pubDate>Thu, 09 Dec 2021 21:07:37 +0000</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/rust/consteval/intro/</guid>
      <description>What is constant evaluation? Constant evaluation is basically just performing calculations at compile time rather than runtime. There are some constructs that we would like to represent with a high level programming language such as Rust or C++, but for which, if we know the parameters to the function at compile time, then we can also calculate the output at compile time as well.
One thing to note is that it&amp;rsquo;s not possible to do everything at compile time since we generally require I/O, and whenever I/O is involved it can no longer be performed or baked in at compile time.</description>
    </item>
    
    <item>
      <title>Rabin-Karp Pattern Search</title>
      <link>https://forgottenmaster.github.io/posts/algorithms/searching/rabinkarp/</link>
      <pubDate>Fri, 19 Nov 2021 12:55:58 +0000</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/algorithms/searching/rabinkarp/</guid>
      <description>Overview We often find ourselves wanting to find a smaller sequence, inside of a larger sequence. This often manifests as searching for a substring in a larger text, or determining if a list of numbers is a sublist of another.
We can achieve this by looking at a sequence of characters at the beginning of the larger sequence (known as the haystack) that is the same length as the smaller sequence (known as the needle).</description>
    </item>
    
    <item>
      <title>Basic Concepts</title>
      <link>https://forgottenmaster.github.io/posts/git/basic_concepts/</link>
      <pubDate>Sat, 06 Nov 2021 20:51:06 +0000</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/git/basic_concepts/</guid>
      <description>Repository A Git repository has 2 main storages:
  The object store This storage is used to store the actual data, commits, etc. for the repository and is a permanent record of all the changes to the repository. This is efficiently copied across when cloning an existing repository to provide an exact duplicate of not only the data but the whole history.
  The index This is a temporary store which records changes that have yet to be made to the repository but have been staged.</description>
    </item>
    
    <item>
      <title>Basic Usage</title>
      <link>https://forgottenmaster.github.io/posts/git/basic_usage/</link>
      <pubDate>Sat, 06 Nov 2021 18:36:42 +0000</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/git/basic_usage/</guid>
      <description>Creating a repository There are two different ways to establish a repository in Git in contrast to other version control systems where there is only creating the initial repository.
In Git we are also able to clone a repository to create copy containing all of the objects and history from an existing repository.
Initialising a new repository This is how we can create a brand new Git repository which may either be empty, or contain data/files already.</description>
    </item>
    
    <item>
      <title>Traits</title>
      <link>https://forgottenmaster.github.io/posts/rust/whyrust/traits/</link>
      <pubDate>Sat, 23 Oct 2021 09:55:16 +0100</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/rust/whyrust/traits/</guid>
      <description>What are traits? Traits in Rust can basically be thought of as interfaces in C#. They can do everything that a C# interface can do except with a few more capabilities. We will start off by equating the common functionality of traits in Rust with C# interfaces, and then explore the additional capabilities we get with Rust traits.
Describes capabilities In C# interfaces, we can describe a set of function signatures which will tell the user the capabilities of that interface, such that they know when they call something what data to pass in, and what they should get back.</description>
    </item>
    
    <item>
      <title>Memory Safety</title>
      <link>https://forgottenmaster.github.io/posts/rust/whyrust/memorysafety/</link>
      <pubDate>Fri, 15 Oct 2021 01:31:14 +0100</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/rust/whyrust/memorysafety/</guid>
      <description>Move By Default In most programming languages, there is no one true &amp;ldquo;owner&amp;rdquo; of any given piece of data. Data can be allocated on the stack or the heap but references to it can be passed around as needed. There isn&amp;rsquo;t a way of the compiler to track ownership of a piece of data in most languages such as C++.
Additionally expensive copies of structures may be done without us realising, or being able to opt out of it.</description>
    </item>
    
    <item>
      <title>Matrix Inputs</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/matrixinputs/</link>
      <pubDate>Wed, 13 Oct 2021 23:47:35 +0100</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/matrixinputs/</guid>
      <description>The final piece of the puzzle in the mathematical foundations needed is to determine how we can trace back a derivative with respect to matrix inputs.
In the previous post we saw that calculating the derivative of two vectors of equal length when passing through the dot product operation results in the transpose of the other.
Matrix Multiplication Refresher The first step is to recall how we can multiply two matrices together.</description>
    </item>
    
    <item>
      <title>Vector Inputs</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/vectorinputs/</link>
      <pubDate>Tue, 12 Oct 2021 00:41:56 +0100</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/vectorinputs/</guid>
      <description>After learning what a derivative of a function is, and how to apply the chain rule to a composite function, we then learned how to calculate the derivative of a function with multiple inputs by tracing back through the chain of functions following the route of one of the parameters while holding the others constant.
The next step in the prerequisite mathematics we need to build the foundations of a neural network is to determine what the derivative of a function means when one or more of the inputs is a vector.</description>
    </item>
    
    <item>
      <title>Multiple Inputs</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/multipleinputs/</link>
      <pubDate>Sat, 09 Oct 2021 15:40:05 +0100</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/multipleinputs/</guid>
      <description>So far, we&amp;rsquo;ve learned how to chain functions together, and how to calculate the derivative of a function at a specific input value. We&amp;rsquo;ve also learned how to apply the chain rule when we are chaining multiple functions together. However, how do we calculate the derivative of a function, or chain of functions when a function has multiple inputs?.
As it turns out, we can calculate the derivative in the same way!</description>
    </item>
    
    <item>
      <title>Enums</title>
      <link>https://forgottenmaster.github.io/posts/rust/whyrust/enums/</link>
      <pubDate>Sun, 26 Sep 2021 17:14:48 +0100</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/rust/whyrust/enums/</guid>
      <description>C Style Enums In C#, C++, and a lot of other popular programming languages, we have access to a type called an &amp;ldquo;enumeration&amp;rdquo; (or enum for short). This is simply a type safe collection of named constant values.
For example in C++, making an enum whose variants represent a set of allowed colors for a hypothetical UI framework could be written as (with the values of the variants explicitly typed out for transparency):</description>
    </item>
    
    <item>
      <title>Chain Rule</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/chainrule/</link>
      <pubDate>Sun, 26 Sep 2021 16:09:52 +0100</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/chainrule/</guid>
      <description>The chain rule allows us to determine the derivative of a composite function as a product of the derivatives of each of the individual functions.
The reason this is a product is because, say you had a function &amp;ldquo;f&amp;rdquo; which has a derivative of 3 at a particular value of x. That means, when you increase the input by 1, the output increases by 3.
Then say you have a second function &amp;ldquo;g&amp;rdquo; which has a derivative of 5 at the value output by &amp;ldquo;f&amp;rdquo; for x.</description>
    </item>
    
    <item>
      <title>Derivatives</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/derivatives/</link>
      <pubDate>Sun, 26 Sep 2021 00:42:04 +0100</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/derivatives/</guid>
      <description>Maths The derivative of a function is the rate at which the output changes with respect to a change in the input at a specific value for the input. This last part is important as the derivative of a function is essentially the gradient, or tangent of the graph of that function at a specific point which can of course change depending on where you are on the number line.</description>
    </item>
    
    <item>
      <title>Functions</title>
      <link>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/functions/</link>
      <pubDate>Fri, 24 Sep 2021 01:45:11 +0100</pubDate>
      
      <guid>https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/functions/</guid>
      <description>There isn&amp;rsquo;t too much to say for what a function actually is, since being a programmer, we use them every day. However for completeness I&amp;rsquo;ll include a post here.
For our purposes it will help to think of a function as a black box, that takes one or more inputs and returns an output. We can then chain these functions together by taking the output of function 1, and passing it to the next function and so on.</description>
    </item>
    
    
    <item>
      <title>Résumé</title>
      <link>https://forgottenmaster.github.io/resume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://forgottenmaster.github.io/resume/</guid>
      <description>Ubisoft Reflections
Senior Gameplay Programmer - Assassin&amp;rsquo;s Creed VR
April 2020 - Present
As a senior gameplay programmer I have been responsible for architecting and implementing large systems in Assassin&amp;rsquo;s Creed VR. Where the product statement isn&amp;rsquo;t clear or is incomplete, I have worked with other disciplines to ensure the final design does what is required. During this time I have gained experience in Unity3D including DOTS and best practices for efficient programs.</description>
    </item>
    
  </channel>
</rss>
