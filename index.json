[{"content":"We now almost have all the pieces in place in the refactored API to be able to train a neural network using the described typestates in the previous sections. The final thing we need to be able to do is to run the forward and backward passes, and apply optimisation to the network weights.\nIn the last post I covered the Forward trait and described why it needs to be generic over a lifetime, so we have our way of performing the forward pass.\nThis post then will be a description of the object/typestate representing that forward pass, and the method on which to run the backward pass and optimisation on it.\nForwardOperation This trait is a simple one and provides a way to run the backward pass on a previously run forward pass.\nThe trait is as follows\npub trait ForwardOperation: Sealed { type Output; type Input; type Backward: BackwardOperation; fn backward(self, output_gradient: Self::Output) -\u0026gt; Result\u0026lt;(Self::Backward, Self::Input)\u0026gt;; } It has an associated type identifying the output gradient type, and an associated type identifying the gradient at the operation\u0026rsquo;s input given that output gradient.\nThe return type is a result which can either be a structure representing this completed backward pass, along with the actual value of the input gradient. If the output gradient shape is wrong, or due to some other error then the result can be an error.\nThe associated type Backward is constrained to implement the BackwardOperation trait to allow for functionality from the BackwardOperation trait to be usable in a generic context.\nBackwardOperation This trait is the simplest one of the lot and is the final typestate in the typestate machine. The trait is defined as such\npub trait Operation: Sealed { fn optimise(self); } It has a single method \u0026ldquo;optimise\u0026rdquo; which consumes the backward pass and applies optimisation to the weights of the network based on the calculated gradients.\nDue to this consuming the backward pass, the mutable borrow we took on the trainable network is no longer around and therefore we\u0026rsquo;re free to start a new forward pass for the next iteration.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/forwardbackward/","summary":"We now almost have all the pieces in place in the refactored API to be able to train a neural network using the described typestates in the previous sections. The final thing we need to be able to do is to run the forward and backward passes, and apply optimisation to the network weights.\nIn the last post I covered the Forward trait and described why it needs to be generic over a lifetime, so we have our way of performing the forward pass.","title":"Part 6 - Forward \u0026 Backward"},{"content":"Now we\u0026rsquo;ve bound a particular optimiser to the operation(s) it is placed into the trainable state. The optimiser instances will retain additional state for gradients, learning rates, etc. that aren\u0026rsquo;t needed in the previous initialised state.\nIn this typestate we are able to run training passes (as opposed to just predictions) on the network and also, once finished, to go back to the initialised state again from which we can get the trained parameters.\nTrainableOperation This trait is pretty straightforward actually. Let\u0026rsquo;s take a look at the definition in it\u0026rsquo;s entirety for a network/operation that is ready to be trained:\npub trait TrainableOperation: Sealed { type Initialised; fn into_initialised(self) -\u0026gt; Self::Initialised; fn init(\u0026amp;mut self, epochs: u16); fn end_epoch(\u0026amp;mut self); } There is only a single associated type with this one which is the type representing the previous state of operation (the initialised state). For functionality we have, as previously mentioned the two pieces of functionality we would like to use.\nDuring training For the actual training process we will need to initialise all the optimisers in the trainable network with the number of epochs that we\u0026rsquo;re running over. This is because those optimisers will use things such as learning rate decay, or otherwise be reliant on the total number of epochs we are running. Initialisation also lets them know we\u0026rsquo;re about to train in epoch 0, so can reset any data that needs to be (such as resetting those decaying learning rates back to starting values).\nAt the end of each epoch, the trainer will then call end_epoch to give any optimisers and such in the network the chance to update state before the next epoch (such as decaying learning rates).\nAfter training After we have finished running our training for however many epochs we\u0026rsquo;d like, we then need to be able to go back to the previous state in order to extract the trained weights for storage, or to make predictions without the overhead of the optimiser instances.\nThe into_initialised method will do this for us and it\u0026rsquo;s fairly straightforward. It simply consumes this trainable instance and produces an initialised form. In effect, this unwraps the trainable operation and strips away any additional data used during the training process (e.g. gradient velocities for momentum) and the resulting operation is lean, only containing what it needs to make predictions.\nWhere\u0026rsquo;s the forward pass? As you may have noticed, the trait defining an operation as being in the trainable state doesn\u0026rsquo;t actually have any functionality to run a forward pass (and thus to run a backward pass, etc.).\nThe reason for this is simple, we need to put that functionality into a separate trait, muchlike we did with the binding of the optimiser and it\u0026rsquo;s for the same reason - generics. However the generic parameters here are not generic types, but lifetimes.\nA consuming transition Let\u0026rsquo;s first take a moment to look at what the API would be if the typestate transition from \u0026ldquo;trainable\u0026rdquo; to \u0026ldquo;forward pass\u0026rdquo; consumed self in the same way as the transitions so far do:\nlet mut trainable = get_trainable_network(); // run one epoch...we\u0026#39;ll pretend we know the output gradients before we\u0026#39;ve run our // forward pass for brevity. trainable = trainable.forward(input).backward(output_gradients).optimise(); As you can see, the API is a little\u0026hellip;.clumsy since you have to remember to re-assign to the trainable binding after running an epoch, since each step consumes self. If we didn\u0026rsquo;t re-assign and instead did something like:\nlet mut trainable = get_trainable_network(); // Epoch 0 trainable.forward(input).backward(output_gradient).optimise(); // Epoch 1 trainable.forward(input).backward(output_gradient).optimise(); // COMPILER ERROR! trainable already used  A non-consuming transition Ideally, what we would like to do is to have the forward transition be non-consuming and only borrow the operation for its duration. We would like our API to look more like this:\nlet mut trainable = get_trainable_network(); trainable.forward(input_0).backward(output_gradient_0).optimise(); // Epoch 0 trainable.forward(input_1).backward(output_gradient_1).optimise(); // Epoch 1 // etc., more epochs  The problem here of course is that we\u0026rsquo;re only borrowing trainable for each forward pass, and the borrow will last until it\u0026rsquo;s been finished with. Rust doesn\u0026rsquo;t allow multiple mutable borrows in existence at the same time because if it did, we could theoretically write something like this:\nlet mut trainable = get_trainable_network(); let epoch_0 = trainable.forward(input_0).backward(output_gradient_0); // calculate gradients to update parameters with for epoch 0 but don\u0026#39;t apply let epoch_1 = trainable.forward(input_1).backward(output_gradient_1); // calculate gradients to update parameters with, but from the same parameters as epoch 0. epoch_0.optimise(); // apply gradients to parameters epoch_1.optimise(); // apply gradients to different parameters used to calculate them!  So to avoid this, we take out a mutable borrow for the duration of one epoch which means that we always start the next training pass from a state that we are in sole control over changing - the previous pass has either been dropped or applied.\nThis means we need to store a mutable borrow in the struct as we go through the typestates, and storing a mutable borrow in a struct requires an explicit lifetime.\nWhy can\u0026rsquo;t we add forward to TrainableOperation? At first you might try to add the forward operation to the trainable trait, which we\u0026rsquo;d think would look something like this:\npub trait TrainableOperation : Sealed { .. // previous stuff we\u0026#39;ve covered  type Forward; fn forward(\u0026amp;mut self) -\u0026gt; Self::Forward; } However, as mentioned the type of Self::Forward will require an explicit lifetime. The lifetime in this case will be the lifetime of the self borrow to forward. This implies that Forward itself needs to be generic over the lifetime, something like:\npub trait TrainableOperation : Sealed { .. // previous stuff we\u0026#39;ve covered  type Forward\u0026lt;\u0026#39;a\u0026gt;; fn forward\u0026lt;\u0026#39;a\u0026gt;(\u0026amp;\u0026#39;a mut self) -\u0026gt; Self::Forward\u0026lt;\u0026#39;a\u0026gt;; } That is, when we define the associated type Forward, we define it as having a single generic parameter (the lifetime \u0026lsquo;a), and in the forward method we set that lifetime to be the same one that \u0026amp;mut self has.\nThis, unfortunately, is not possible at the time of writing in Rust as it requires a feature called Generic Associated Types which is still being worked on.\nForward trait So, as mentioned before we can do this by putting our forward method into a trait which is itself generic over that lifetime. We can then implement this trait for all lifetimes and when we call \u0026ldquo;forward\u0026rdquo; on an implementor, Rust can determine which trait instance should be used.\nThe trait then looks as such:\npub trait Forward\u0026lt;\u0026#39;a\u0026gt;: Sealed + TrainableOperation { type Input; type Output; type Forward; fn forward(\u0026amp;\u0026#39;a mut self, input: Self::Input) -\u0026gt; Result\u0026lt;(Self::Forward, Self::Output)\u0026gt;; } I\u0026rsquo;ve put an additional bound here to make sure we only implement Forward on those operations that are in the TrainableOperation state. The trait as mentioned is generic over a single lifetime, \u0026lsquo;a, which we then use to explicitly say this is the lifetime of the \u0026amp;mut self borrow.\nFor the function itself, we take in an input and run the forward pass, the result of this will be a tuple where we get access to the output (so we can calculate loss), along with the operation in a state ready to begin the backward pass.\nThe tuple is wrapped in Result because this is a fallible function, because the input provided, as with making predictions, might be incorrectly shaped for the number of neurons expected.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/trainable/","summary":"Now we\u0026rsquo;ve bound a particular optimiser to the operation(s) it is placed into the trainable state. The optimiser instances will retain additional state for gradients, learning rates, etc. that aren\u0026rsquo;t needed in the previous initialised state.\nIn this typestate we are able to run training passes (as opposed to just predictions) on the network and also, once finished, to go back to the initialised state again from which we can get the trained parameters.","title":"Part 5 - Trainable State"},{"content":"After building the network/operation chain and initialising it with weights/parameters either via a known set (through previous training), or via a random seed (using Xavier initialisation) we have a network that is capable of making predictions given some input. We are also at this point able to read out the set of parameters for later use.\nWe are only able to get an operation into the initialised state by first starting with an uninitialised version and then initialising it, we can\u0026rsquo;t directly construct new instances of this type because it\u0026rsquo;s not publicly visible/nameable.\nThirdly we\u0026rsquo;re able to take this initialised operation and put it into \u0026ldquo;training\u0026rdquo; mode by binding an optimiser with it.\nThe InitialisedOperation trait First we\u0026rsquo;ll take a look at the overall trait definition:\npub trait InitialisedOperation: Sealed { type Input; type Output; type ParameterIter: Iterator\u0026lt;Item = ElementType\u0026gt;; fn iter(\u0026amp;self) -\u0026gt; Self::ParameterIter; fn predict(\u0026amp;self, input: Self::Input) -\u0026gt; Result\u0026lt;Self::Output\u0026gt;; } As before, and as with all the traits in Eidetic, this is a Sealed trait to allow us to control implementation details and correctness by implementing it only for operations inside the crate. In fact, it\u0026rsquo;s likely this won\u0026rsquo;t be pointed out again on any future traits.\nAs far as associated types go, we need:\n Input. The type of input to the operation, always a tensor type. Output. The type of output from the operation, again a tensor type. ParameterIter. The type of iterator that is produced from the iter method to gain access to the stream of trained weights.  The parameter iterator will produce elements/weights in the same order as they are expected when initialising a network of the exact same architecture via the with_iter method, so this can be used to store those weights off to a file or transmit across a network, or otherwise store for later use.\nThe other capability is the ability to make predictions for the operation via the predict method. This will take an input of the appropriate type, and produce an output. Note that this is a fallible method because the input may not be correctly shaped at runtime for the operation (e.g. an incorrect number of columns/features), so it returns a Result type which can either be propagated or unwrapped as needed.\nWhat\u0026rsquo;s the next step? You may have noticed that this trait does not have any way to move the type state over to the next state (the trainable state). This is because the output type will depend on the specific optimiser chosen to use to put this operation into a trainable state.\nThis means the trainable state is generic over the optimiser in use, since it needs to hold onto an instance of an optimiser to support adaptive optimisation and things like learning rate decay.\nSince each type can only have one implementation of a given trait, and since we want the initialised operation to be moved to trainable with any compatible optimiser, we need a separate trait that itself is generic over the optimiser type.\nThe WithOptimiser trait This secondary trait is the WithOptimiser trait which looks as follows:\npub trait WithOptimiser\u0026lt;T\u0026gt;: Sealed { type Trainable; fn with_optimiser(self, optimiser: T) -\u0026gt; Self::Trainable; } T in this case is the optimiser type we\u0026rsquo;re using (actually, we\u0026rsquo;ll see later it\u0026rsquo;s an optimiser factory because it needs to support making an optimiser that\u0026rsquo;s generic over a particular type). This lets us implement versions of this trait for each optimiser type in existance which lets the caller pick the optimiser to use.\nThe Trainable type here is the type which is used for this operation that is bound to an instance of that optimiser. For operations that don\u0026rsquo;t have any parameters and thus, no need for optimisation, they will always produce the same type to represent their trainable form regardless of the optimiser chosen.\nFor types that do require storing an optimiser for later use during the optimisation stage, the correct implementation can be chosen here.\nRust will pick the correct version of the trait to use when the with_optimiser function is called because it will be able to see what the type of T is, due to it requiring an instance into the with_optimiser function. Therefore all of the following calls will be correct:\nlet null = initialised.with_optimiser(NullOptimiser::new()); let sgd = initialised.with_optimiser(SGD::new(FixedLearningRateHandler::new(0.01))); let sgd_momentum = initialised.with_optimiser(SGDMomentum::new(FixedLearningRateHandler::new(0.01), 0.9)); Why the optimiser isn\u0026rsquo;t an optimiser? As mentioned, the type T that is passed into the with_optimiser function (e.g. NullOptimiser, SGD, etc.) is not implementing the Optimiser trait directly but another trait called OptimiserFactory. We\u0026rsquo;ll look into more details when we cover the optimisation stage, but the reason for this is because an optimiser type will require storing state that is dependant on the type of parameter for a given optimisation (e.g. Tensor of a certain rank).\nHowever, for operation chains/composite operations we want to be able to take the type T and pass it to both/all operations in the chain - but they might store different parameter types. Therefore the type with_optimiser takes is a factory which can produce optimiser instances of various types when required by those operations.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/initialised/","summary":"After building the network/operation chain and initialising it with weights/parameters either via a known set (through previous training), or via a random seed (using Xavier initialisation) we have a network that is capable of making predictions given some input. We are also at this point able to read out the set of parameters for later use.\nWe are only able to get an operation into the initialised state by first starting with an uninitialised version and then initialising it, we can\u0026rsquo;t directly construct new instances of this type because it\u0026rsquo;s not publicly visible/nameable.","title":"Part 4 - Initialised Operations"},{"content":"Now that we\u0026rsquo;ve covered tensors which are the representation of data that is flowing through the API, we can start taking a look at the operations and how we are using typestates to ensure valid usage of the API without unnecessary bloat in code by keeping members around that aren\u0026rsquo;t necessary.\nWhat are typestates anyway? If you aren\u0026rsquo;t used to programming with generics at the type level, you may not have encountered type states before, so a quick explanation of what they are might be necessary. Let\u0026rsquo;s first think about runtime representation of states which is commonly seen when using a Finite State Machine (FSM).\nIn programming, using an FSM allows us to ensure that certain operations are invalid at runtime and that we can only transition from one state of value to certain other ones via specific events.\nFor example an FSM to make a coffee will look as follows:\nJust as states are a way to encode the current state and valid transitions of values at runtime, typestates are a way to encode the current state and valid transitions of types at compile time.\nThe advantage of typestates are that they don\u0026rsquo;t allow invalid code to even compile. For example in the above FSM, the starting state of the above FSM could be represented by a type and provides methods to transition to the \u0026ldquo;Grind Coffee\u0026rdquo; and \u0026ldquo;Fry Eggs\u0026rdquo; states.\nThis could be represented in Rust as:\nstruct Start; impl Start { pub fn make_food_and_drink(self) -\u0026gt; (GrindCoffee, FryEggs) { ... } } There are two things we need to ensure for typestates to work for us:\n There should be no other way to construct GrindCoffee or FryEggs other than going through the Start::make_food_and_drink method. This ensures that the caller must go through the appropriate flow of actions, starting from the beginning to even construct these types. When we call make_food_and_drink to progress the state machine to the new types, we should not be able to use that instance of Start again. This is where Rust\u0026rsquo;s ability to prevent use after move comes in useful.  Typestates in Eidetic Now that we have an idea of what typestates are and how they\u0026rsquo;re useful, we can take a look at how we use them in Eidetic. All operations will implement these traits and have ways to represent the various states at the type level, which means we\u0026rsquo;ll only be looking at these typestates at the trait level.\nAs a high level overview the typestate machine will look as follows:\nThis post and the following posts will run over the traits representing each of these states and describe their API.\nUninitialised The very first typestate we have is the Uninitialised one. This is the state that the network/operation is in at the point where the architecture is defined. We can\u0026rsquo;t modify a neural network after it\u0026rsquo;s initialised so we only allow chaining of operations that are in this typestate.\nTaking a look at the definition of the trait as a whole:\npub trait UninitialisedOperation: Sealed + Sized { type Initialised: InitialisedOperation; fn with_iter(self, mut iter: impl Iterator\u0026lt;Item = ElementType\u0026gt;) -\u0026gt; Result\u0026lt;Self::Initialised\u0026gt;; fn with_seed(self, seed: u64) -\u0026gt; Self::Initialised; } It is intended that any operation or layer that is in the public API is in the uninitialised state, which we can freely connect together to get the desired architecture and then initialise the network as a whole using the provided initialisation methods.\nThe first thing to notice is that the trait here is Sealed because there are a few additional methods in this trait that are hidden from the public API/documentation that we don\u0026rsquo;t have to want to worry about breaking in external code. A secondary reason to make this Sealed is so that we can ensure that all operations behave correctly according to the contracts defined, and limiting the scope of implementations to this crate only ensures we don\u0026rsquo;t have to worry about breaking changes so much. Thirdly we\u0026rsquo;re hiding away implementation details of our Tensors so there\u0026rsquo;s no optimal way for external code to manipulate tensors to even define their own operations (they would have to convert the tensor to an iterator, then manipulate it, then convert back).\nThe trait defines one associated type which is the type of the next state, the initialised state. We constrain this to be of a type implementing InitialisedOperation since this knowledge is used for chaining later.\nLet\u0026rsquo;s take a closer look at the two ways that the uninitialised network/operation can be initialised:\nWith a seed The easiest method to initialise a network is via a random seed value. This is used when we are initialising the network for the very first time before we\u0026rsquo;ve done any training on it to determine optimal weights.\nThe with_seed method simply takes a u64 which defines the seed to use, and internally initialises any weights using this seed. The return type of this method indicates that it\u0026rsquo;s infallible, and this is indeed correct.\nThe only way initialisation can fail is if there aren\u0026rsquo;t enough values in a given stream to initialise all parameters/weights in the network, which can only be the case for a finite stream.\nWith a random seed though, we are generating an infinite stream of random values to initialise with, which can\u0026rsquo;t possibly be exhausted. As a result, initialising a network/operation with a random seed can never fail.\nWith an iterator The second method is to initialise the network with a set of weights/parameters that have been determined via training in a previous run potentially.\nThis method will be useful in the case that we trained the network previously and stored out the trained parameter values to a file and are reconstructing the same network.\nIn this case, we take an iterator over elements of ElementType (f64 by default) and try to initialise the network. Unlike initialising with a seed, this method can fail because the provided iterator may not yield enough values to fully initialise the operation.\nTherefore this will return a Result containing (on success), the initialised instance.\nConclusion Both methods of initialisation result in yielding an initialised version of the operation/network if successful, and more importantly one of these methods is the only way for client code to construct one of these initialised instances.\nThe second thing to point out is that both methods are taking self by value which, thanks to Rust\u0026rsquo;s move semantics ensures that the uninitialised version can\u0026rsquo;t be used after it\u0026rsquo;s been initialised. This, in turn allows optimisations because the initialised version can steal the guts of the uninitialised version if needed (for example stealing a pointer or some memory or something).\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/uninitialisedoperations/","summary":"Now that we\u0026rsquo;ve covered tensors which are the representation of data that is flowing through the API, we can start taking a look at the operations and how we are using typestates to ensure valid usage of the API without unnecessary bloat in code by keeping members around that aren\u0026rsquo;t necessary.\nWhat are typestates anyway? If you aren\u0026rsquo;t used to programming with generics at the type level, you may not have encountered type states before, so a quick explanation of what they are might be necessary.","title":"Part 3 - Uninitialised Operations"},{"content":"A \u0026ldquo;tensor\u0026rdquo; is just a fancy name for \u0026ldquo;n-dimensional array\u0026rdquo; (AKA ndarray) and provides a way of thinking about scalars, vectors, matrices, etc. in a uniform way. We refactor Eidetic to use this concept because we want operations to be able to specify how many dimensions are in the input and output data. This post will provide a brief overview of what a tensor is, and how we implement it in Eidetic.\nWhat is a tensor? As mentioned in the introduction blurb, a tensor is just simply an array with N dimensions. In tensor terminology the N here is known as it\u0026rsquo;s rank. For ease of understanding, there\u0026rsquo;s a few ranks of tensor below with common terminology for that particular data type.\n Rank 0 =\u0026gt; \u0026ldquo;Scalar\u0026rdquo;. A rank 0 tensor has no dimensions and is a single point. In terms of elements this means it only has a single element contained within which is known as a scalar in mathematics. Rank 1 =\u0026gt; \u0026ldquo;Vector\u0026rdquo;. A rank 1 tensor has only a single dimension which represents its length. All elements are on this 1 dimension and the overal data structure is commonly referred to as a vector (list of things). Rank 2 =\u0026gt; \u0026ldquo;Matrix\u0026rdquo;. A rank 2 tensor has two dimensions which represent the width and height (alternatively rows and columns). Such a tensor is commonly referred to as a matrix.  Higher ranked tensors exist but don\u0026rsquo;t have common names for them and are harder to think about or draw in a diagram. A rank 3 tensor can be thought of as a \u0026ldquo;cube\u0026rdquo; or a \u0026ldquo;stack\u0026rdquo; of matrices stacked on top of one another and is commonly used to represent the layers in an image for example.\nAfter this they start to become more abstract - a rank 4 tensor can be thought of as a \u0026ldquo;vector of cubes\u0026rdquo; perhaps.\nWhy we need a tensor? We need a tensor type in the API because we want to constrain operations and layers to using the same type but allow them to be generic over the rank of the tensor. Using a common type allows us to avoid having to have separate types for scalars, vectors, matrices, etc.\nThe Tensor type Now we have a brief explanation of what tensors are, we can look at how they\u0026rsquo;re implemented inside Eidetic. The following are points that we\u0026rsquo;re trying to satisfy with our implementation:\n Tensors must be generic over a rank indicating its dimensionality. Tensors must hide all implementation details of how they\u0026rsquo;re stored internally. Tensors should be constructible through a new function which may or may not be fallible. The data inside a Tensor should be iterable.  The type that we settled upon for a Tensor is as follows:\npub struct Tensor\u0026lt;R: Rank\u0026gt;(pub(crate) Array\u0026lt;ElementType, R::Internal\u0026gt;); This is generic over a parameter type R which MUST implement the Rank trait. This is because we need to access the Internal associated type which allows us to define the ndarray::Array type we\u0026rsquo;re using to store the data internally.\nThis internal state is hidden from the public API by the pub(crate) visibility modifier which means that the public API has no reference at all to the ndarray crate or the Array type. We would easily be able to swap out the implementation for something else as long as the public API continues to use the Tensor type.\npub(crate) gives the crate the ability to work with the ndarray::Array type stored inside however which allows us to internally use all the methods provided by the ndarray crate for calculations while leaving it an implementation detail.\nConstructing a tensor type In order to construct the tensor type in Eidetic we need to provide a \u0026ldquo;new\u0026rdquo; function for each rank of tensor we\u0026rsquo;re constructing. This is due to the parameters and return type being different depending on the rank of the tensor we\u0026rsquo;re constructing (since some ranks are infallible).\nFor rank 0 tensors (scalars) they will take as input a single element, and return a Tensor - this is infallible because we can always create a tensor with a single element from a single element. The code for this look as such:\nimpl Tensor\u0026lt;rank::Zero\u0026gt; { pub fn new(elem: ElementType) -\u0026gt; Self { Self(arr0(elem)) } } For rank 1 tensors (vectors) they will take some type that we can get an iterator of elements from, and will put all those elements into a single dimension in the tensor (it\u0026rsquo;s length). This is infallible because we are simply taking the number of elements in the iterable as the length of the tensor:\nimpl Tensor\u0026lt;rank::One\u0026gt; { pub fn new(iter: impl IntoIterator\u0026lt;Item = ElementType\u0026gt;) -\u0026gt; Self { Self(Array::from_iter(iter)) } } Rank 2 and higher tensors are not infallible because this is where we have to take a flat iterator of elements, and reshape it to fit a desired shape. If there aren\u0026rsquo;t enough elements inside the iterator for the requested shape then this will be an error.\nI\u0026rsquo;ll only show one implementation here, but they will all follow the same format:\n Take the iterator provided and make a 1-dimensional array from it Reshape the array into an n-dimensional one using the provided/requested shape If this is an error from ndarray (invalid number of elements) then map the error type to our own Eidetic error type  The code for example for a rank 2 tensor looks like this:\nimpl Tensor\u0026lt;rank::Two\u0026gt; { pub fn new(shape: (usize, usize), iter: impl IntoIterator\u0026lt;Item = ElementType\u0026gt;) -\u0026gt; Result\u0026lt;Self\u0026gt; { let array: Array\u0026lt;ElementType, Ix1\u0026gt; = Array::from_iter(iter); let array: Array\u0026lt;ElementType, Ix2\u0026gt; = array.into_shape(shape).map_err(|_| Error(()))?; Ok(Self(array)) } } Note that we\u0026rsquo;re using the eidetic::Result type alias here since all results output by Eidetic will use the error type of eidetic::Error.\nReading from a tensor type Now that we have the ability to construct a tensor from an iterator, we need the ability to go the other way and iterate the values in a tensor which will allow us to interpret the output data from Eidetic as required.\nIn order to do this we will need to implement the IntoIterator trait provided by Rust, which will iterate over elements of the type eidetic::ElementType (f64 by default, f32 if Cargo feature is enabled). The definition of this trait looks like follows:\nimpl\u0026lt;R: Rank\u0026gt; IntoIterator for Tensor\u0026lt;R\u0026gt; { type Item = ElementType; type IntoIter = TensorIterator\u0026lt;R\u0026gt;; fn into_iter(self) -\u0026gt; Self::IntoIter { TensorIterator(self.0.into_iter()) } } The iterator type is a new type TensorIterator which simply wraps our ndarray::Array type\u0026rsquo;s iterator and delegates the next method. Taking the struct definition and the Iterator trait implementation together we get:\npub struct TensorIterator\u0026lt;R: Rank\u0026gt;(\u0026lt;Array\u0026lt;ElementType, R::Internal\u0026gt; as IntoIterator\u0026gt;::IntoIter); impl\u0026lt;R: Rank\u0026gt; Iterator for TensorIterator\u0026lt;R\u0026gt; { type Item = ElementType; fn next(\u0026amp;mut self) -\u0026gt; Option\u0026lt;Self::Item\u0026gt; { self.0.next() } } Again, the wrapped iterator from ndarray is kept private so it doesn\u0026rsquo;t leak into the public API.\nThe Rank type The other part of the Tensor type is the Rank types which are required for 2 reasons:\n A type for Tensors to be generic over indicating their dimensionality/rank A way to access the associated ndarray type indicating dimensionality as an implementation detail (e.g. Ix0, Ix1, Ix2, etc.)  We use a trait so that we can apply it as a trait bound for tensors. However we make sure to hide the internal type from the public documentation:\npub trait Rank: Clone + Sealed { #[doc(hidden)] type Internal: Dimension; } We are using the Sealed Traits pattern here because we have an implementation detail inside the trait and we don\u0026rsquo;t want to expose this to the public API. If this trait were allowed to be implemented by foreign types then they would have to be aware that ndarray is in use, etc. and changing this later would be a breaking change.\nFor the implementation, all of our rank types (0 through 5) follow the same structure, so here is the definition of rank 0 as an example:\n#[derive(Clone, Debug, Default, Eq, PartialEq)] pub struct Zero; impl Rank for Zero { type Internal = Ix0; } impl Sealed for Zero {} ","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/tensors/","summary":"A \u0026ldquo;tensor\u0026rdquo; is just a fancy name for \u0026ldquo;n-dimensional array\u0026rdquo; (AKA ndarray) and provides a way of thinking about scalars, vectors, matrices, etc. in a uniform way. We refactor Eidetic to use this concept because we want operations to be able to specify how many dimensions are in the input and output data. This post will provide a brief overview of what a tensor is, and how we implement it in Eidetic.","title":"Part 2 - Tensors"},{"content":"The following sections will detail the implementation of the refactoring of our Eidetic API of which the new version can be found HERE.\nThis post in particular will provide a brief overview of the problems that the refactoring will address, then we will look at the step-by-step implementation of the new Eidetic API at a relatively high level since going into every line of code as we did with the initial implementation would take too long.\nInstead, we will explain patterns in code as they come up with the understanding that the logic implemented internally is more or less the same as in the original implementation (e.g. dense network layers are still consisting of a dot product, bias addition, and activation function).\nWhy did we need to refactor? The following issues were present with the original implementation of Eidetic which we aim to solve with the new version:\n STD Reliance. Dynamic allocations were everywhere in the API which didn\u0026rsquo;t give the flexibility to choose whether you wanted to run in an environment that didn\u0026rsquo;t support dynamic allocations/the standard library. Boxes were used in trait signatures and so there was no way to turn this off. The new API runs without the standard library by default but can be opted into for additional functionality if needed. Error Detection. There was no error handling in the original implementation of Eidetic and everything was using unwrapping under the hood. The new version ensures that any fallible function returns a Result so that the caller gets to decide if it\u0026rsquo;s propagated up the stack or not. Invalid Function Call Sequences. The previous version of Eidetic didn\u0026rsquo;t enforce that a forward pass must be performed before a backward pass in a training epoch. The result was that the caller could run a backward pass on a network before a forward pass and that the code would panic as a result of the lack of error detection. This is a result of the one type we used for representing the network having all functions on it that might be invoked. The new API solves this by using typestates so that functions can only be run if the network is in the correct state at a type level (from calling a previous sequence of functions). Inflexible Networks. The previous API had networks that had their loss function baked in which meant that we couldn\u0026rsquo;t have networks that were intended to be fed from other networks (for example when we try to implement a GAN) and that each network was a complete, trainable network in its own right. We solve this in the new API by keeping the loss function separated from the network. Hardcoded Tensor Rank. In the old API, operations were forced to have rank 2 tensors only but the new API allows them to define their own input/output/parameter types which will allow us to easily later on add in layers with tensor ranks greater than 2 (for example convolutional layers). Code Bloat. The old API had all data required by an operation be stored in that operation regardless of whether it\u0026rsquo;s needed or not. For example running a forward pass would always store off the input and calculated output even if a backward pass isn\u0026rsquo;t needed. The new API uses typestates which means that each step of training is a separate type and so the operations are able to store just enough data that they will need but no more.  ","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/rationale/","summary":"The following sections will detail the implementation of the refactoring of our Eidetic API of which the new version can be found HERE.\nThis post in particular will provide a brief overview of the problems that the refactoring will address, then we will look at the step-by-step implementation of the new Eidetic API at a relatively high level since going into every line of code as we did with the initial implementation would take too long.","title":"Part 1 - Rationale"},{"content":"At this point we\u0026rsquo;ve successfully coded a neural network library that should be extensible enough to create various architectures of densely connected layers. We\u0026rsquo;ve also implemented various techniques that can be used to potentially get better results from the network.\nThe next chapters will be about entirely new network architectures, namely Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), but now that we have enough pieces for training a standard feed forward network, I want to try out training on something more substantial and meaningful than just a simple mathematical formula or comparison operation such as we\u0026rsquo;ve seen so far.\nThis post will cover using the library we\u0026rsquo;ve made to train a feed forward network that can recognize handwritten numbers from small images. Specifically, using the MNIST data set.\nWhat is the MNIST data set? The details of the MNIST data set can be read on the Wikipedia page but basically it\u0026rsquo;s a curated data set from several earlier image data sets, curated to be ideal for machine learning.\nThe images in the set are all handwritten digits, examples of the images are below for the digits 0-9:\nThe actual files are located here and there are 4 files, grouped into testing and training data. The files beginning with train are the ones corresponding to the training data, and the ones beginning with t10k are the testing ones. Among these two pairs, the ones labelled with images are the actual input image data, and the ones labelled with labels are the correct answers in the same order as the images.\nThe images are each 28x28 pixels, and the labels are simply numbers from 0-9 indicating the target value. However, these files are in a custom binary file format so we will first have to be able to parse them.\nThe IDX file format This is detailed nicely on the website but the IDX file format that the images and labels are in is a nice simple binary format for storing N-dimensional data with varying data types/element sizes.\nThe data format is as follows:\n(0x0000) magic number (4 bytes)\r(0x0004) size in dimension 0 (4 bytes)\r(0x0008) size in dimension 1 (4 bytes)\r(0x000c) size in dimension 1 (4 bytes)\r...\r(0x????) size in dimension N (4 bytes)\r(0x????) data element 1 (varying)\r(0x????) data element 2 (varying)\r...\r(0x????) data element N (varying)\rThat is, there\u0026rsquo;s a fixed size magic number followed by the sizes of the dimensions, and then finally the data follows.\nThe magic number is a 4 byte integer with the first 2 bytes always 0.\nThe third byte encodes the data type of the elements in the data segment which can be:\n 0x08: u8 (1 byte) 0x09: i8 (1 byte) 0x0B: i16 (2 bytes) 0x0C: i32 (4 bytes) 0x0D: f32 (4 bytes) 0x0E: f64 (8 bytes)  The last byte tells us how many dimensions there are in the data (e.g. 1 for vector, 2 for matrix, etc.)\nAll integers are encoded as big endian in the byte stream (most significant bit first).\nAs for coordinates, the index of the last dimension in the data changes the fastest (for example if storing a 2 dimensional matrix then the first row is read, then the second row, and so on).\nIn the following sections in this post we\u0026rsquo;ll implement a parser in Rust that can take a byte slice and produce a nicely parsed data structure that can then be used by our code to turn it into an ndarray::Array for processing for example.\nData type Starting out with the most basic type which is an enumeration that indicates the type of the data in the file, from the set detailed above. Firstly we\u0026rsquo;ll need to make some constants (not strictly necessary but always good to have something the compiler can verify, and collect them at the top of the file). These are simply encoded as u8\u0026rsquo;s:\nconst U8: u8 = 0x08; const I8: u8 = 0x09; const I16: u8 = 0x0B; const I32: u8 = 0x0C; const F32: u8 = 0x0D; const F64: u8 = 0x0E; For the actual data type, we know we\u0026rsquo;ll have the small well-defined set above and an enumeration type in Rust is perfect to represent this. We\u0026rsquo;ll need the Clone, Debug, and PartialEq traits for later, so we will make sure to tell the compiler to automatically derive implementations for these:\n#[derive(Clone, Debug, PartialEq)] pub enum DataType { U8, I8, I16, I32, F32, F64, } As far as functionality goes, we will need to be able to do the following two things with DataTypes:\n Ask an instance of DataType how many bytes it takes up, for when we read the data Be able to convert a u8 into a DataType  For now we\u0026rsquo;ll just assume that the u8 =\u0026gt; DataType conversion is infallible and panic if a value is given that can\u0026rsquo;t be converted. In a more robust library we should use TryFrom instead to allow better error handling. I don\u0026rsquo;t mind this just terminating the program if the data is malformed.\nFor #1, we simply need to use a match with the value and return the appropriate number of bytes which will either be 1, 2, 4, or 8:\npub fn number_of_bytes(\u0026amp;self) -\u0026gt; u8 { match self { Self::U8 | Self::I8 =\u0026gt; 1, Self::I16 =\u0026gt; 2, Self::I32 | Self::F32 =\u0026gt; 4, Self::F64 =\u0026gt; 8, } } For #2, we can use a match also - we\u0026rsquo;ll use the constants we defined for readability. We\u0026rsquo;ll use the panic! macro with a message if the u8 we\u0026rsquo;re converting from isn\u0026rsquo;t a valid data type value:\nimpl From\u0026lt;u8\u0026gt; for DataType { fn from(byte: u8) -\u0026gt; Self { match byte { U8 =\u0026gt; Self::U8, I8 =\u0026gt; Self::I8, I16 =\u0026gt; Self::I16, I32 =\u0026gt; Self::I32, F32 =\u0026gt; Self::F32, F64 =\u0026gt; Self::F64, _ =\u0026gt; panic!(\u0026#34;Invalid byte given to be converted into a data type: {byte}\u0026#34;), } } } Value We now have the ability to determine what the type of the data is, but no way to represent the actual data that has been read.\nFor this we\u0026rsquo;ll use another enum type called Value which will hold the actual value as a rust primitive type. However, because there are a range of DataType variants, we likewise need to define the same variants here. The definition is as follows (again we need to derive the Debug and PartialEq types):\n#[derive(Debug, PartialEq)] pub enum Value { U8(u8), I8(i8), I16(i16), I32(i32), F32(f32), F64(f64), } The first conversion we require is a conversion from a byte sequence/slice into a Value instance however if we\u0026rsquo;re just given a slice of bytes, how do we know how many we\u0026rsquo;re supposed to read and interpret or what data type to interpret as?.\nWe can\u0026rsquo;t is the short answer, we need another piece of information which is the data type. Conveniently, we\u0026rsquo;ve just added a way to represent the data type.\nThe conversion then, is not from a byte slice to Value, but from a tuple of byte slice and DataType.\nAgain, we\u0026rsquo;ll assume an infallible conversion hence implementing From rather than TryFrom, but in a more robust library we\u0026rsquo;d want to handle failure nicely.\nThe conversion itself will simply match on the data type, then depending on the length of the data type we will want to ensure there\u0026rsquo;s enough bytes to take, and then take those bytes and construct the appropriate data type from them, assuming a big endian ordering.\nRust has an associated method from_be_bytes for all primitive types, which will take the bytes and construct an instance of the appropriate type with them.\nThe one thing to be aware of is from_be_bytes takes a byte array however we have a byte slice. Helpfully though, there\u0026rsquo;s a fallible conversion from slice to fixed array, so we can use try_into. However since we ourselves are not handling errors, we\u0026rsquo;ll just unwrap this. The whole implementation then is as follows:\nimpl From\u0026lt;(\u0026amp;[u8], DataType)\u0026gt; for Value { fn from(tuple: (\u0026amp;[u8], DataType)) -\u0026gt; Self { let (data, data_type) = tuple; match data_type { DataType::U8 =\u0026gt; { assert!(data.len() \u0026gt;= 1); Value::U8(data[0]) } DataType::I8 =\u0026gt; { assert!(data.len() \u0026gt;= 1); Value::I8(i8::from_be_bytes(data[0..1].try_into().unwrap())) } DataType::I16 =\u0026gt; { assert!(data.len() \u0026gt;= 2); Value::I16(i16::from_be_bytes(data[0..2].try_into().unwrap())) } DataType::I32 =\u0026gt; { assert!(data.len() \u0026gt;= 4); Value::I32(i32::from_be_bytes(data[0..4].try_into().unwrap())) } DataType::F32 =\u0026gt; { assert!(data.len() \u0026gt;= 4); Value::F32(f32::from_be_bytes(data[0..4].try_into().unwrap())) } DataType::F64 =\u0026gt; { assert!(data.len() \u0026gt;= 8); Value::F64(f64::from_be_bytes(data[0..8].try_into().unwrap())) } } } } As far as extracting out the contained value, we actually only end up needing the u8 variant since the MNIST data is only bytes.\nWell therefore only implement for now the conversion into u8, but in a real package, we\u0026rsquo;d make sure there was a way to access the other variants too. Implementation is simply matching on variant and if it\u0026rsquo;s U8 then we can pull out the value inside:\nimpl From\u0026lt;\u0026amp;Value\u0026gt; for u8 { fn from(value: \u0026amp;Value) -\u0026gt; Self { match value { Value::U8(value) =\u0026gt; *value, _ =\u0026gt; unimplemented!(), } } } Magic number The first item in an IDX file is a \u0026ldquo;magic number\u0026rdquo; which indicates both the type of the data elements, and the number of dimensions.\nThe data type we have an enumeration for, and the number of dimensions is encoded in a single byte of the magic number, so we use a u8 for storage.\nThe definition then is:\n#[derive(Debug)] pub struct MagicNumber { data_type: DataType, num_dimensions: u8, } For conversions, we will need to:\n Convert from a 4 byte array to a MagicNumber Decompose a MagicNumber into its data type and number of dimensions  For #1, we need to implement the From\u0026lt;[u8; 4]\u0026gt; trait for MagicNumber, and the implementation will do the following:\n Check that bytes 0 and 1 are 0 Convert byte at index 2 into a DataType Construct and return the MagicNumber struct  This functionality is coded up as:\nimpl From\u0026lt;[u8; 4]\u0026gt; for MagicNumber { fn from(bytes: [u8; 4]) -\u0026gt; Self { assert_eq!(bytes[0], 0); assert_eq!(bytes[1], 0); let data_type = bytes[2].into(); let num_dimensions = bytes[3]; Self { data_type, num_dimensions, } } } Conversion #2 can be implemented also with a From trait, this time From, and we implement it on the type (DataType, u8).\nWe just return our fields, so the implementation is simple enough:\nimpl From\u0026lt;MagicNumber\u0026gt; for (DataType, u8) { fn from(magic_number: MagicNumber) -\u0026gt; Self { (magic_number.data_type, magic_number.num_dimensions) } } File contents Now we have the ability to parse the magic number containing the data type and number of dimensions, we can go ahead and parse the file contents as a whole. Once we\u0026rsquo;ve parsed the magic number from the beginning of the buffer, we need to:\n Read the next N groups of 4 bytes. Each one represents an unsigned integer indicating the size of one of the dimensions. After reading the dimension sizes, read the rest of the data into a buffer (as the appropriate Value type).  The structure we\u0026rsquo;ll be using to store everything in will be called the FileContents struct and will just need to store a Vec of sizes (for the dimensions - a Vec because we don\u0026rsquo;t know in advance how many dimensions there are), along with a Vec of Value type holding the data linearly.\nThe struct definition looks as follows:\n#[derive(Debug, PartialEq)] pub struct FileContents { sizes: Vec\u0026lt;u32\u0026gt;, // vector of dimension sizes  data: Vec\u0026lt;Value\u0026gt;, } We make readonly accessors for these, pretty simple:\nimpl FileContents { pub fn sizes(\u0026amp;self) -\u0026gt; \u0026amp;[u32] { \u0026amp;self.sizes } pub fn data(\u0026amp;self) -\u0026gt; \u0026amp;[Value] { \u0026amp;self.data } } One thing to note here is we\u0026rsquo;re returning a reference to a slice not a Vec, to indicate that we\u0026rsquo;re not transferring ownership and also to leave us open in future for modification (anything that is coerced to a slice can be returned/stored in the struct).\nThe first major task we want is parsing. We have a slice of bytes (u8\u0026rsquo;s) of arbitrary length and we need to parse this using the IDX protocol. Assuming we\u0026rsquo;re giving data in the correct format (and in this case for our toy library we don\u0026rsquo;t care about error handling and should just crash if something is wrong), then we can just use the From\u0026lt;\u0026amp;[u8]\u0026gt; trait:\nimpl From\u0026lt;\u0026amp;[u8]\u0026gt; for FileContents { fn from(bytes: \u0026amp;[u8]) -\u0026gt; Self { // implementation goes here  } } As for the implementation, we need to do the following sequence of actions:\n Parse the first 4 bytes as the magic number. Get the data type and number of dimensions from the magic number. For each dimension, read the next 4 bytes from the slice, giving us the size in that dimension. Read the rest of the data, parsing into the correct data type.  We\u0026rsquo;ll look at these one at a time.\nFor step #1, we have the MagicNumber type, so we\u0026rsquo;ll delegate our actual parsing to that type, but first we need to take the first 4 bytes from the slice. We can do this in Rust with a method on slices called split_at which takes an index and returns all elements below that index as one slice, and the rest as a second slice.\nOnce we\u0026rsquo;ve split the slice, we need to turn that first (now 4 byte long) slice into an array, specifically [u8; 4]. This is because we\u0026rsquo;ve implemented From\u0026lt;[u8; 4]\u0026gt; for MagicNumber so we need to convert before we can turn it into a MagicNumber. We can do this in Rust with the try_into implementation (which we just unwrap).\nFinally we can use the From implementation to turn it into a MagicNumber instance, which will fail if the slice isn\u0026rsquo;t right. Overall the code to cover step 1 is:\nlet (magic_number, bytes) = bytes.split_at(4); let magic_number: [u8; 4] = magic_number.try_into().unwrap(); let magic_number: MagicNumber = magic_number.into(); For step #2, we implemented Fromfor (DataType, u8), as in we already implemented a way to decompose the struct into its parts, so we can just do:\nlet (data_type, num_dimensions) = magic_number.into(); Step #3 is to read the sizes of the dimensions. We can do this as a fold actually, we take the iterator that runs from 0 to the number of dimensions and call fold on it, with a preallocated Vec. In the callback function we take the sizes Vec and the remaining bytes slice. This part looks as follows:\nlet (sizes, bytes) = (0..num_dimensions).fold( (Vec::with_capacity(num_dimensions as usize), bytes), |(mut sizes, bytes), _| { // update sizes and bytes and return here  } }; To take off the first 4 bytes, we use the split_at function again, assert that it\u0026rsquo;s the correct length, then parse it as a u32 before pushing it to the sizes vector.\nThe structure from what we already know then is:\nlet (size, bytes) = bytes.split_at(4); assert!(size.len() == 4); ??? sizes.push(size); (sizes, bytes) In the ??? line, we just need to know how to convert the size (which is a byte slice) into a u32. Rust helpfully provides an associated method for this called from_be_bytes which takes a [u8; 4] that it assumes is in big endian ordering and reinterprets it as a u32. This function only runs on a fixed 4 element array, and we have a slice so we need to use the try_into implementation thusly:\nlet size = u32::from_be_bytes(size.try_into().unwrap()); Now we have a vector of sizes for the dimensions, which is half the battle!. We just need to read the data in now.\nFirstly we need to know how many values we\u0026rsquo;re reading. We know that the data is basically an N-dimensional array and each instance of a given dimension is the same length, which means that the total number of elements in the vector/matrix/etc. is the product of all the sizes.\nWe can do this in Rust easily with the Iterator API:\nlet number_values: u32 = sizes.iter().cloned().product(); We can then get the number of bytes per value (which tells us how we need to split the slice), from the DataType:\nlet bytes_per_value: u8 = data_type.number_of_bytes(); For parsing the data, we are essentially following the same pattern as we did for parsing the dimension sizes, that is:\n Fold from 0 to number_values For each, we split the byte slice so that we break off bytes_per_value bytes Assert that the split off byte slice is the correct length Parse the bytes as the appropriate DataType, parse using the From\u0026lt;(\u0026amp;[u8], DataType)\u0026gt; implementation of Value Push this into the fold state which is returned  In its entirety this looks as follows:\nlet (data, _) = (0..number_values).fold( (Vec::with_capacity(number_values as usize), bytes), |(mut data, bytes), _| { let (value, bytes) = bytes.split_at(bytes_per_value as usize); assert!(value.len() == bytes_per_value as usize); let value: Value = (value, data_type.clone()).into(); data.push(value); (data, bytes) }, ); That\u0026rsquo;s it, we can now construct FileContents from the sizes and data we parsed!\nThere is however another thing we will need to be able to do, we need to be able to index into this data. Not strictly necessary because we can access the flat data slice, but it will let us instead of writing in calling code:\nfile_contents.data()[(image_index * width * height) + (y * width) + x] we want to simply be able to write:\nfile_contents[(image_index, y, x)] In Rust this operation, as with all other operations, is implemented via a trait. In this case, it\u0026rsquo;s the Index trait which takes a generic parameter indicating the type of key, and requires two items:\n The type of output An index function that takes a key of the key type, and returns a reference to the output type.  We can only index correctly into a 3 dimensional data, since we\u0026rsquo;re giving a key with 3 parts, so we need to assert the sizes vector is 3.\nWe then assert that each individual index is less than the corresponding dimension\u0026rsquo;s size.\nIf the provided key passes these assertions then the index into the flat data can be calculated as:\n$$ index(idx1, idx2, idx3) = (idx1 \\times size(2) \\times size(1)) + (idx2 \\times size(1)) + idx3 $$\nAnd finally we can index into the data array to get a reference to the value at that point.\nIn its entirety it looks like follows:\nimpl Index\u0026lt;(u32, u32, u32)\u0026gt; for FileContents { type Output = Value; fn index(\u0026amp;self, idx: (u32, u32, u32)) -\u0026gt; \u0026amp;Self::Output { assert_eq!(self.sizes.len(), 3); let (idx1, idx2, idx3) = idx; assert!(self.sizes[0] \u0026gt; idx1); assert!(self.sizes[1] \u0026gt; idx2); assert!(self.sizes[2] \u0026gt; idx3); let idx = (idx1 * self.sizes[2] * self.sizes[1]) + (idx2 * self.sizes[1]) + idx3; \u0026amp;self.data[idx as usize] } } Converting FileContents to input array Now we\u0026rsquo;ve been able to load the IDX files and have FileContents structures for all 4 of them. 2 of them will be 3-dimensional (the images) and 2 of them will be 1-dimensional (the labels).\nWe need to convert these into ndarray::Array types for use with our network, and this first function will be focussed on converting the image data to an Array of f64 elements.\nFirstly we can assign meaning to the dimensions in the FileContents structure. The dimensions for the images are:\n Dimension 0 =\u0026gt; the image axis, allows selecting the image we\u0026rsquo;re referring to Dimension 1 =\u0026gt; the y coordinate, allows selecting a row in the image Dimension 2 =\u0026gt; the x coordinate, allows selecting an individual element inside a row  So in code we do:\nlet number_images = contents.sizes()[0] as usize; let height = contents.sizes()[1] as usize; let width = contents.sizes()[2] as usize; Next we need to take the data and transform them into a Vecso that we can construct an array from them. We can do this by getting an iterator to the data, then mapping the iterator using a function, then collecting the results into a Vec.\nFor the mapping function, it will take a Value (which we know will be Value::U8), extracts the u8 from inside the enum, and then converts that from a 0-255 integer into a 0.0-1.0 float by dividing by 255.0. The whole procedure looks as follows:\nlet data = contents .data() .into_iter() .map(|value| { let value: u8 = value.into(); value as f64 / 255.0 }) .collect(); Now we have the Vec, we can create an Array from it with the standard from_shape_vec function we\u0026rsquo;ve been using. The number of rows is the number of images, and the number of columns will be the image width multiplied by the image height:\nArray::from_shape_vec((number_images, width * height), data).unwrap() Converting FileContents to target array Parsing the 1-dimensional FileContents that contain the labels is both simpler but also more complex. Simpler because there\u0026rsquo;s only one dimension to worry about, but more complex because the labels are given as integers in the range 0-9, however for a classification network to do a good job, we want to one-hot encode the labels.\nWhat this means is that we need to take the single value, and create 10 columns for it. In those columns, they\u0026rsquo;ll all be 0, except for the one column at the correct index.\nBelow we can show this mapping:\n 0 =\u0026gt; [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1 =\u0026gt; [0, 1, 0, 0, 0, 0, 0, 0, 0, 0] 2 =\u0026gt; [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] 3 =\u0026gt; [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] 4 =\u0026gt; [0, 0, 0, 0, 1, 0, 0, 0, 0, 0] 5 =\u0026gt; [0, 0, 0, 0, 0, 1, 0, 0, 0, 0] 6 =\u0026gt; [0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 7 =\u0026gt; [0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 8 =\u0026gt; [0, 0, 0, 0, 0, 0, 0, 0, 1, 0] 9 =\u0026gt; [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  What we want is a mapping where each element of the file contents maps to 10 elements in the output vector. We can do this by using the flat_map function instead of a regular map, and returning an iterator that produces 10 elements for each input element.\nIn order to create an iterator we can use the chain function to chain together 10 iterators. Each of the iterators will produce only a single value which will be either a 0 or a 1 indicating whether the input value is the associated value.\nThen we collect into an array again, this time with 10 columns.\nThe whole thing looks like follows:\nlet number_images = contents.sizes()[0] as usize; let data = contents .data() .into_iter() .flat_map(|value| { let value: u8 = value.into(); let value_0 = std::iter::once((value == 0) as u8 as f64); let value_1 = std::iter::once((value == 1) as u8 as f64); let value_2 = std::iter::once((value == 2) as u8 as f64); let value_3 = std::iter::once((value == 3) as u8 as f64); let value_4 = std::iter::once((value == 4) as u8 as f64); let value_5 = std::iter::once((value == 5) as u8 as f64); let value_6 = std::iter::once((value == 6) as u8 as f64); let value_7 = std::iter::once((value == 7) as u8 as f64); let value_8 = std::iter::once((value == 8) as u8 as f64); let value_9 = std::iter::once((value == 9) as u8 as f64); value_0 .chain(value_1) .chain(value_2) .chain(value_3) .chain(value_4) .chain(value_5) .chain(value_6) .chain(value_7) .chain(value_8) .chain(value_9) }) .collect(); Array::from_shape_vec((number_images, 10), data).unwrap() Building the network Now we have the data prepared and can build and train the network!\nSince we\u0026rsquo;ve seen several networks so far, it should be fairly obvious if I just paste the whole setup code:\n// hyperparameters const LEARNING_RATE: f64 = 0.001; const EPOCHS: u32 = 100000; const EVAL_EVERY: u32 = 2; const BATCH_SIZE: usize = 256; const SEED: u64 = 42; // Prepare Network let mut network = Network::new( vec![ Layer::new_with_seed(65, DenseLayerSetup::new_boxed(Tanh::new_boxed()), SEED), Layer::new_with_seed(10, DenseLayerSetup::new_boxed(Linear::new_boxed()), SEED), ], Loss::new(SoftmaxCrossEntropy::new_boxed()), ); let mut optimiser = SGD::new(LearningRateFixed::new(LEARNING_RATE)); We\u0026rsquo;re using a fixed learning rate with standard SGD optimiser. We use SoftmaxCrossEntropy loss because this is a classification problem.\nThis network uses two layers only, one with 65 neurons and an output layer with 10.\nCalculating accuracy In order to determine the accuracy of the network, we need to look at each of the rows of the predictions, alongside the targets for those predictions that we expect, we can use a zip for this. Then we need to filter the zipped iterator of tuples to only include elements where the prediction matches the target. We can then use the count function on the iterator to count the correct predictions:\nlet correct_count = predictions .rows() .into_iter() .zip(targets.rows().into_iter()) .filter(|(prediction, target)| { // perform check here  }) .count(); We can\u0026rsquo;t however just do (prediction == target) here, because the targets are guaranteed to be 0 and 1 values (and only have a single 1), but the predictions are more fuzzy than that.\nFrom the predictions, what we actually want to do is take the highest element as the guess. So to get the predicted number, we need to go over the elements and indices of the prediction row and track the index of the maximum element, which is done as so:\nlet mut predicted_max = f64::MIN; let mut predicted_number = 0; prediction.iter().enumerate().for_each(|(idx, value)| { if *value \u0026gt; predicted_max { predicted_max = *value; predicted_number = idx; } }); Finding the target number is easier, we\u0026rsquo;re just looking for the index that corresponds to the 1. This can be done with the following code:\nlet target_number = target .iter() .enumerate() .filter(|(_, value)| **value == 1.0) .map(|(idx, _)| idx) .next() .unwrap(); And finally we can check the two for equality.\nThe accuracy then is calculated using the standard percentage calculation of (correct/total) * 100.\nResults After coding all of this up and running it, we can see that it performs incredibly well on the training data, and not bad on the testing data:\nAccuracy (training): 99.61833333333333%\rAccuracy (testing): 97.59%\rThis is using only a simple feed forward neural network so once we learn about convolutional networks we should be able to achieve even greater accuracy!\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/mnist/","summary":"At this point we\u0026rsquo;ve successfully coded a neural network library that should be extensible enough to create various architectures of densely connected layers. We\u0026rsquo;ve also implemented various techniques that can be used to potentially get better results from the network.\nThe next chapters will be about entirely new network architectures, namely Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), but now that we have enough pieces for training a standard feed forward network, I want to try out training on something more substantial and meaningful than just a simple mathematical formula or comparison operation such as we\u0026rsquo;ve seen so far.","title":"Extra - Training on the MNIST"},{"content":"Out of all the modifications for neural networks we\u0026rsquo;ve covered in order to try to get the network to train the problem \u0026ldquo;better\u0026rdquo;, we haven\u0026rsquo;t actually tried what people would think is the most obvious - adding more neurons to the network.\nIntroduction Adding more neurons to the network can take one of two forms:\n Adding more neurons to a hidden layer in the network (expanding wide) Adding more hidden layers to the network (expanding long)  More neurons means more parameters in the network that can be tweaked - if you recall, each weight is a contribution to the overall prediction and a set of neurons in the network can represent some \u0026ldquo;learned feature\u0026rdquo; that the network deems important.\nWith a linear regression, we only have a single neuron which means only a single parameter we can tweak in order to change the prediction. It\u0026rsquo;s clear that there are some numbers in a linear regression that just can\u0026rsquo;t possibly be made because we only have a single multiplier and bias term (y = mx + c).\nSo if fewer data points means that the network is less accurate then surely adding more neurons to the network means it will be more accurate, right?\nWell\u0026hellip;.yes, and no. It\u0026rsquo;s true that more neurons means we can model more complex graphs, however it\u0026rsquo;s also true that the network will overfit to the training data, and not perform very well at all when used on testing data, or \u0026ldquo;in the wild\u0026rdquo;.\nDropout is a way to let us add additional capacity (extra neurons) to the network, while also making it less likely that the network overfits the training data.\nWhat is dropout? How do we avoid the network overfitting the training data?. We simply need to randomly set neurons in the network back to 0, or remove their contribution from the predictions in each epoch.\nThe intuition here is that it prevents a neuron\u0026rsquo;s weight from \u0026ldquo;locking on\u0026rdquo; to a particular value and keeping it for the entirety of training. When we set a neuron\u0026rsquo;s weight back to 0 then it must recalculate it from other neurons in the network in the optimisation step.\nThis continuous set back of random neurons, and their subsequent recalculation from other neurons means that the network ends up not overfitting (or at least less likely). After all, how can you overfit the data if you keep changing?\nOne thing to remember though, is when we\u0026rsquo;re making predictions for the purposes of testing, or when being used in the wild to predict using a trained network - we do not want to use dropout, since that changes the state of the network that we\u0026rsquo;ve worked so hard to train!\nHowever, during training, dropout ends up reducing the total magnitude of the values that are output from the layer by an average of (1-p) where p is the probability of dropping the neuron.\nThis makes sense, because when we dropout a neuron and set it to 0, the entire contribution to the weighted sum from that neuron is dropped to 0. The total weighted sum from the layer which was calculated from M neurons is now only calculated from \\(M \\times (1 - p)\\) neurons. When averaging this, it means that if the chance of dropout is 30% for example, it means that 30% of the overall sum passing forward is reduced by 30%.\nTherefore, when we\u0026rsquo;re making predictions, we want to not drop out the neuron, but we do want to simulate this reduction in magnitude so as to keep the values going forward at the expected range to not give false answers. We\u0026rsquo;ll do that by allowing the caller to specify they\u0026rsquo;re using inference mode, and we\u0026rsquo;ll just simulate the dropout by multiplying the whole output by the keep probability.\nImplementing Dropout In order to implement dropout, we\u0026rsquo;ll create a BasicOperation that is tacked onto the end of the operation sequence of the layer when we create the layer.\nHowever there is a minor problem, as we\u0026rsquo;ll soon see that will require a little refactor to our code in order to allow operations to retain state.\nUp until now, our operations were stateless and have just been used as a vtable, however in order for us to apply the same dropout mask as calculated in the forward pass (remember this is generated randomly each forward pass), we\u0026rsquo;ll need to make operations mutable.\nMaking Operation\u0026rsquo;s forward and backward passes mutable In order for us to allow implementations of operations to store state in their forward pass for use during their backward pass, we will need to change the trait implementation for our BasicOperation and ParameterOperation traits such that they look as follows:\npub trait BasicOperation\u0026lt;T\u0026gt;: BasicOperationClone\u0026lt;T\u0026gt; { fn calculate_output(\u0026amp;mut self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; fn calculate_input_gradient( \u0026amp;mut self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; } pub trait ParameterOperation\u0026lt;T\u0026gt;: ParameterOperationClone\u0026lt;T\u0026gt; { fn calculate_output(\u0026amp;mut self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; fn calculate_input_gradient( \u0026amp;mut self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; fn calculate_parameter_gradient( \u0026amp;mut self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; } That is, we simply need to change \u0026amp;self to \u0026amp;mut self in the calculation functions however this has the knock on effect then of requiring that all implementations are updated to the new signature (changing a trait is a breaking change in Rust).\nI won\u0026rsquo;t show that here, but again it\u0026rsquo;s just a case of changing all the \u0026amp;self\u0026rsquo;s to \u0026amp;mut self.\nThis has the unfortunate knock on effect of requiring that the Operation struct is also updated, this is because it will be calling these methods, which previously were able to be called on an immutable reference. Now, however it will need a mutable object to be able to call them, so we need to update Operation also.\nAgain, this is a simple enough change to not show here.\nIdentifying \u0026ldquo;inference\u0026rdquo; mode Up until now, when we go through the forward pass, we have no idea if we\u0026rsquo;re in training mode or inference mode. However for the dropout operation we will be required to know if we\u0026rsquo;re in training or inference mode.\nThis is because, as mentioned previously when we\u0026rsquo;re training the network, we want fully fledged dropout of neurons, however when we\u0026rsquo;re only in inference mode we don\u0026rsquo;t actually want to do any changes to the network, but still need to simulate the dropout.\nTo do this, we will firstly create an enum indicating the mode that we\u0026rsquo;re in. We could use a boolean for this as there\u0026rsquo;s only two states, however using an enumeration is more typesafe and more readable at the call site (it\u0026rsquo;s better to see Mode.Inference instead of just \u0026ldquo;true\u0026rdquo;).\n#[derive(Clone, Copy, PartialEq)] pub enum Mode { Training, Inference, } We will need to take the correct mode into the Network.forward method, since we only require the forward pass to be aware of what mode we\u0026rsquo;re operating in. We have three \u0026ldquo;entry point\u0026rdquo; functions when interacting with the Network. These are:\n forward - Called on a fully trained network to produce predictions. Should always run in inference mode train - Called when we are training the network (performs a forward and backward pass). Always runs in training mode forward_loss - Called both by training and also by examples on a trained network to see the loss values. Can run in both modes  Therefore we will move the contents into a helper function (inner_forward) which will take a Mode and can be called by both forward_loss and forward, passing the appropriate mode. We change forward to call this with a fixed mode of Inference, and change train to call it with a fixed mode of Training. forward_loss changes to take a Mode and pass it along. These 4 relevant functions now look as follows:\npub fn train(\u0026amp;mut self, batch: Array\u0026lt;T, Ix2\u0026gt;, targets: Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; T { let (_, loss) = self.forward_loss(batch, targets, Mode::Training); self.backward(); loss } pub fn forward(\u0026amp;mut self, batch: Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { self.inner_forward(batch, Mode::Inference) } pub fn forward_loss( \u0026amp;mut self, batch: Array\u0026lt;T, Ix2\u0026gt;, targets: Array\u0026lt;T, Ix2\u0026gt;, mode: Mode, ) -\u0026gt; (Array\u0026lt;T, Ix2\u0026gt;, T) { let predictions = self.inner_forward(batch, mode); let loss = self.loss.forward(predictions.clone(), targets); (predictions, loss) } fn inner_forward(\u0026amp;mut self, batch: Array\u0026lt;T, Ix2\u0026gt;, mode: Mode) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { self.layers .iter_mut() .fold(batch, |state, layer| layer.forward(\u0026amp;state)) } Next we will need to pass this along to the forward method of the Layer type inside inner_forward. Updating the code here is simple enough, we simply pass the mode into the layer.forward call.\nHowever as you might have guessed, this doesn\u0026rsquo;t compile!. Layer::forward is not expecting a second parameter, so we will need to make sure we add one:\n// Layer::forward pub fn forward(\u0026amp;mut self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, mode: Mode) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { // rest of the code is the same here } We\u0026rsquo;re one level deeper now, we need to thread this inference mode down to the next lowest level which is into the individual operations themselves, which we can do inside the Layer::forward method when we\u0026rsquo;re going over the operations. Adding this gives us a compile error once more, due to Operation::forward not expecting this parameter. So we add it:\n// Operation::forward pub fn forward(\u0026amp;mut self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, mode: Mode) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { // rest of code stays the same } Finally, we need to thread this mode into the calculate_output function on the BasicOperation or ParameterOperation trait object. However, you guessed it!, we need to update those traits to take the mode as a parameter into the calculate_output function:\n// BasicOperation fn calculate_output(\u0026amp;mut self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, mode: Mode) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; // ParameterOperation fn calculate_output( \u0026amp;mut self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, mode: Mode ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; As a result, we need to update all implementors to satisfy the new signature, but I won\u0026rsquo;t show that here.\nWith that, we\u0026rsquo;re finally done!, we are now able to specify the operating mode to the Network and it\u0026rsquo;s correctly passed down all the way to the individual operations. We can finally implement the Dropout operation!\nThe dropout operation We\u0026rsquo;ll implement the dropout operation as a BasicOperation that we will tack onto the end of the appropriate layers, and with the above changes should be pretty easy to implement. We will need to store some internal state however. Namely, we need to record the generated random mask from the forward pass that we use to drop out some of the neurons so that we can drop the associated gradients on the backward pass (the gradient associated with an input we nullify shouldn\u0026rsquo;t be passed back in the network).\nWe will also need to specify the proportion of the neurons that are kept which is the multiplier we use in inference mode, but also lets us generate the random mask in training mode.\nThe struct definition itself is pretty simple:\n#[derive(Clone)] pub struct Dropout\u0026lt;T\u0026gt; { mask: Array\u0026lt;T, Ix2\u0026gt;, keep_probability: T, seed: Option\u0026lt;u64\u0026gt; } Note that we also include the ability to specify a seed for the RNG to generate the mask. This is required for testing purposes as this is the only operation with a random element so we need to make sure we can handle a seed being passed.\nAs with the other operations, we will want to work with boxed versions of this type so as to be able to use it in the list of operations setup by our layer setup. Therefore we add a new_boxed method to produce a new Dropout instance. We will also add a new_boxed_with_seed function that specifies a seed for RNG also.\nimpl\u0026lt;T: Default\u0026gt; Dropout\u0026lt;T\u0026gt; { pub fn new_boxed(keep_probability: T) -\u0026gt; Box\u0026lt;Self\u0026gt; { Box::new(Self { mask: Default::default(), keep_probability, seed: None, }) } pub fn new_boxed_with_seed(keep_probability: T, seed: u64) -\u0026gt; Box\u0026lt;Self\u0026gt; { Box::new(Self { mask: Default::default(), keep_probability, seed: Some(seed), }) } } Note that we do require Default on elements of type T in order to use default on Array, even if we\u0026rsquo;re creating an array of size 0, just because there seems to be no \u0026ldquo;empty\u0026rdquo; constructor to construct a new, empty array and Default is only implemented for Array in the case that the element type is also Default.\nNow we just need to implement the BasicOperation trait for our Dropout type.\nAs far as trait bounds go, we will need the signature to look as follows:\nimpl\u0026lt; T: \u0026#39;static + Clone + Mul\u0026lt;Output = T\u0026gt; + One + PartialOrd + SampleUniform + ScalarOperand + Zero, \u0026gt; BasicOperation\u0026lt;T\u0026gt; for Dropout\u0026lt;T\u0026gt; where Standard: Distribution\u0026lt;T\u0026gt;, { // implementation follows } The explanations for these are below:\n \u0026lsquo;static - This is required because to be able to be clonable by the BasicOperationClone supertrait we need the elements to be of type T. The other operations so far have been empty, so the lifetime of T doesn\u0026rsquo;t matter as we never store any, and the operation type is \u0026lsquo;static. However for this one we store a mask of T\u0026rsquo;s and so we need to explicitly say T has to be \u0026lsquo;static for our type overall to be \u0026lsquo;static. Clone - This is required also for the cloning behaviour, we need T to be clonable so we can be clonable. Mul\u0026lt;Output=T\u0026gt; - This is required because we must multiply the elements of the input by our keep probability which is a T, or by our mask which is an Array of T. One - Required because in order to generate the probabilities for dropout, and to generate the mask of zeroes and ones, we will need to know what one of a T is. PartialOrd - Required because we generate the dropout probabilities randomly, and need to determine whether a given value is kept or dropped. We do this by doing a comparison of the random value against our keep_probability. SampleUniform - Required because we\u0026rsquo;re using a random number generator to generate random samples within the range 0 to 1, and in a generic context, to do this we need to only use types that can be sampled in this way. ScalarOperand - Required because we multiply the input by the keep probability if we\u0026rsquo;re in inference mode. This allows the operation to be broadcast to the elements of the array correctly. Zero - Required because in order to generate the probabilities for dropout, and to generate the mask of zeroes and ones, we will need to know what zero of a T is. Standard: Distribution - This one is a little \u0026ldquo;backward\u0026rdquo; in that we\u0026rsquo;re stating that the Standard trait from rand supports being a distribution over our type T.  For the calculate_output function, we firstly need to check whether we\u0026rsquo;re in inference mode or not. If we are then we will simply multiply the elements of the input array by our keep probability to simulate the magnitude dropoff, but without actually dropping any neurons out. We will make a mask of all 1\u0026rsquo;s just in case the backward pass is called (it shouldn\u0026rsquo;t be in inference mode). This looks like follows:\nfn calculate_output(\u0026amp;mut self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, mode: Mode) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { if mode == Mode::Inference { self.mask = Array::ones(input.raw_dim()); input * self.keep_probability.clone() } else { // training mode implementation goes here  } } Training mode is a little bit more complex!\nFirstly we need to create an RNG either truly random or with the seed if we have one. We can do this in the same way as we do in the DenseLayerSetup type:\nlet mut random = match self.seed { Some(seed) =\u0026gt; StdRng::seed_from_u64(seed), None =\u0026gt; StdRng::from_rng(thread_rng()).unwrap(), }; What we then need to do is calculate how many total elements we need to generate. This is easy enough - it\u0026rsquo;s the number of rows multiplied by the number of columns:\nlet number_required = input.ncols() * input.nrows(); To generate the mask, we know that we will be generating a sequence of T\u0026rsquo;s as a flat vector, and need to reshape it into a 2-D array of the correct size. This is pretty easy with the Array::from_shape_vec method we\u0026rsquo;ve been using:\nself.mask = Array::from_shape_vec( (input.nrows(), input.ncols()), ??? ).unwrap(); So we just have to fill in the ???\u0026rsquo;s which will produce the vector of randomly selected 1\u0026rsquo;s and 0\u0026rsquo;s.\nTo do this we can firstly create an iterator over the range (0..number_required) and then map that iterator onto one that produces T\u0026rsquo;s. The map function we will pass doesn\u0026rsquo;t actually use the index, so we replace that with an _.\nThe first thing we do is to generate a random T within the range 0 to 1. Since we\u0026rsquo;re in a generic context, we don\u0026rsquo;t know what 0 or 1 is in the context of a T, so we need to rely on the fact we\u0026rsquo;ve constrained T to implement the Zero and One traits so we can ask for the appropriate representations.\nThe actual generation can be done by the gen_range function provided by rand:\nlet elem: T = random.gen_range(T::zero()..=T::one()); Next we check if the generated element is less or equal to our keep probability. If it is, then we return a 1. If however the generated element is above the keep probability then it\u0026rsquo;s in the dropout range and should be emitted as a 0. This snippet looks as follows:\nif elem \u0026lt;= self.keep_probability { T::one() } else { T::zero() } Finally we can take that iterator that\u0026rsquo;s now emitting random T::zero()\u0026rsquo;s and T::one()\u0026rsquo;s, and collect it into a vector for reshaping.\nAfter we\u0026rsquo;ve got the mask, we just multiply the input by it to get the output:\ninput * self.mask.clone() In its entirety, the calculate_output function looks as follows:\nfn calculate_output(\u0026amp;mut self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, mode: Mode) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { if mode == Mode::Inference { self.mask = Array::ones(input.raw_dim()); input * self.keep_probability.clone() } else { let mut random = match self.seed { Some(seed) =\u0026gt; StdRng::seed_from_u64(seed), None =\u0026gt; StdRng::from_rng(thread_rng()).unwrap(), }; let number_required = input.ncols() * input.nrows(); self.mask = Array::from_shape_vec( (input.nrows(), input.ncols()), (0..number_required) .map(|_| { let elem: T = random.gen_range(T::zero()..=T::one()); if elem \u0026lt;= self.keep_probability { T::one() } else { T::zero() } }) .collect(), ) .unwrap(); input * self.mask.clone() } } By contrast however, the calculate_input_gradient function is incredibly simple!. The only job for that function is to drop gradients associated with the elements that were dropped in the forward pass. We stored the mask that was generated so this is easy, and in fact I\u0026rsquo;ll paste the code in its entirety here:\nfn calculate_input_gradient( \u0026amp;mut self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, _input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { output_gradient * self.mask.clone() } And with that, our Dropout operation is done!\nLayerSetupAdapter Now we need to provide a way to ensure that the Dropout operator can be opted into when creating the layers. The best way to do this is to write a wrapper around any implementation of a LayerSetupand ensure that we also implement the LayerSetup.\nWhen layer_setup is called we can call the wrapped one to get the initial list of operations for the layer, and then we can tack a Dropout onto the end. This is called the Composite pattern and is quite pervasive in Rust because we don\u0026rsquo;t have inheritance, instead the user can choose their functionality and combine it like this.\nThat way we don\u0026rsquo;t need to duplicate the dropout functionality if a different LayerSetup other than DenseLayerSetup is created later.\nThe struct will be generic over two types:\n T - The type of the underlying elements in the array U - The type of the concrete instance of LayerSetup that we\u0026rsquo;re actually decorating  It also needs to hold onto the keep_probability (also a T) for later during setup. The definition then looks as follows:\n#[derive(Clone)] pub struct DropoutLayerAdapter\u0026lt;T, U\u0026gt; { layer_setup: U, keep_probability: T, } We also need a new_boxed constructor function like we have for our other types, but this isn\u0026rsquo;t that noteworthy so will show it here in it\u0026rsquo;s entirety:\nimpl\u0026lt;T, U\u0026gt; DropoutLayerAdapter\u0026lt;T, U\u0026gt; { pub fn new_boxed(layer_setup: U, keep_probability: T) -\u0026gt; Box\u0026lt;Self\u0026gt; { Box::new(Self { layer_setup, keep_probability, }) } } Finally, we need to actually implement the LayerSetuptrait for our adapter so that it can stand in for the decorated type in the network. The trait bounds here seem funky:\nimpl\u0026lt; T: \u0026#39;static + Clone + Default + One + PartialOrd + SampleUniform + ScalarOperand + Zero, U: Clone + \u0026#39;static + LayerSetup\u0026lt;T\u0026gt;, \u0026gt; LayerSetup\u0026lt;T\u0026gt; for DropoutLayerAdapter\u0026lt;T, U\u0026gt; where Standard: Distribution\u0026lt;T\u0026gt; However, we don\u0026rsquo;t need any of this functionality to implement the trait, these are simply bounds \u0026ldquo;inherited\u0026rdquo; from the various types in use that we\u0026rsquo;re decorating (specifically LayerSetup, and Dropout).\nThe implementation is simple also, as we just need to:\n Call the wrapped LayerSetup to get a list of Operations Push our Dropout operation onto the end of the list Return the modified list  In code, this equates to:\nlet mut operations = self.layer_setup.layer_setup(num_neurons, input, seed); operations.push(Operation::new_basic(Dropout::new_boxed( self.keep_probability.clone(), ))); operations And that\u0026rsquo;s it!\nThe caller can now create a DenseLayerSetup (or any LayerSetup) either without dropout as before:\nDenseLayerSetup::new_boxed(Tanh::new_boxed()) Or with dropout, by wrapping it:\nDropoutLayerAdapter::new_boxed(DenseLayerSetup::new(Tanh::new_boxed()), KEEP_PROBABILITY) Testing Finally we can create an example to test this new dropout functionality. We\u0026rsquo;ll take the latest example which included:\n Softmax cross entropy loss function Momentum Exponential learning rate decay  And we\u0026rsquo;ll add dropout with a keep probability of 65% to the hidden layer (example is just called \u0026ldquo;dropout\u0026rdquo;)\u0026hellip;.\nAccuracy (training): 91.414%\rAccuracy (testing): 91.522%\rWhat happened!?\nWell, we\u0026rsquo;re dropping out neurons which will make it less likely to overfit, but will also slow down, or set back the training. Recall that dropout lets us increase the capacity of the network. Let\u0026rsquo;s go ahead and do that by adding another hidden layer with twice the number of neurons:\nAccuracy (training): 95.372%\rAccuracy (testing): 95.322%\rThis is almost the same, but a little lower than we had previously. However we are able to have multiple layers now to model more complex relationships which dropout will cope with. Tweaking hyperparameters some more will likely result in gains.\nAs a sanity test, let\u0026rsquo;s remove the dropout with this new layer:\nAccuracy (training): 87.46199999999999%\rAccuracy (testing): 87.434%\rSo we can see that indeed the dropout operator is letting us expand the layers and capacity of the network while preventing degraded accuracy.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/dropout/","summary":"Out of all the modifications for neural networks we\u0026rsquo;ve covered in order to try to get the network to train the problem \u0026ldquo;better\u0026rdquo;, we haven\u0026rsquo;t actually tried what people would think is the most obvious - adding more neurons to the network.\nIntroduction Adding more neurons to the network can take one of two forms:\n Adding more neurons to a hidden layer in the network (expanding wide) Adding more hidden layers to the network (expanding long)  More neurons means more parameters in the network that can be tweaked - if you recall, each weight is a contribution to the overall prediction and a set of neurons in the network can represent some \u0026ldquo;learned feature\u0026rdquo; that the network deems important.","title":"4.6 - Dropout"},{"content":"As we\u0026rsquo;ve covered a couple of different optimisation strategies (stochastic gradient descent with and without momentum), it\u0026rsquo;s become clear that the learning rate hyperparameter is one of, if not the most important hyperparameters in deep learning.\nA hyperparameter in deep learning is a tweakable value that needs to be tuned based on the problem at hand. These include the learning rate, momentum, epoch count, batch size, etc. and half of the success for training a network is finding the correct hyperparameter values.\nIf we refer back to the diagram seen previously, which can be seen below\nWe can see that with small values for the learning rate (the blue arrows), the steps we take are small but we run the risk of falling into a local minimum. Conversely, if the learning rate is too large (the red arrows), we can see that we \u0026ldquo;bounce around\u0026rdquo; and might skip over the true minimum.\nWhat we would like is a happy balance of the two approaches, in fact we\u0026rsquo;d like to start the training with a larger learning rate to broadly find the region of the minimum, but as we progress through training, we\u0026rsquo;d like to take smaller and smaller steps so as to hone in on the true minimum and be less likely to skip over it.\nWe can achieve this by decaying the learning rate hyperparameter as we progress through the epochs.\nRefactoring In order to allow for decaying the learning rate over the process of training the network, we will need to (slightly) refactor the existing code such that we can later drop in a decaying learning rate if needed.\nOptimiser trait The first step will be to add a couple of additional methods to the Optimiser trait, these will be:\n init(epochs) - Called at the very beginning of the training process with the number of epochs we plan to run through. This allows a chance to set up data at the beginning of the process that might rely on that total epoch count (such as a percentage progress, or something else). end_epoch() - Called at the end of each epoch EXCEPT for the very last one, and gives the Optimiser a chance to update state at the end of the epoch before it ends up getting stepped again (allowing for example learning rate to decay).  After we add these, the Optimisertrait looks as follows:\npub trait Optimiser\u0026lt;T\u0026gt; { fn init(\u0026amp;mut self, epochs: u32); fn step(\u0026amp;mut self, net: \u0026amp;mut Network\u0026lt;T\u0026gt;); fn end_epoch(\u0026amp;mut self); } train function I\u0026rsquo;ll show how we implement this into our Optimiser implementations in a moment, but let\u0026rsquo;s take a look at the changes required to the train function to invoke these new functions. Firstly at the beginning of the train function before entering the epoch loop, we call init:\nlet mut best_loss: Option\u0026lt;T\u0026gt; = None; optimiser.init(epochs); for e in 0..epochs { // rest of code follows  And at the end of the loop, we call the end_epoch function. However, we only do this if the epoch number is not the very last one. If it\u0026rsquo;s the very last one then there\u0026rsquo;s no point updating the optimiser as it\u0026rsquo;s not going to be invoked again. The code therefore looks as follows:\nif let Some(last_model) = last_model { let (_, test_loss) = network.forward_loss(batch_test.clone(), targets_test.clone()); if best_loss.is_none() || test_loss.abs() \u0026lt; *best_loss.as_ref().unwrap() { best_loss = Some(test_loss.abs()); } else { *network = last_model; break; } } if e \u0026lt; (epochs - 1) { optimiser.end_epoch(); } SGD For the SGD optimiser, we will need to implement the new functionality required by the trait, however we know that we will be implementing the new functionality in the SGDMomentum type also in the exact same way. In order to prevent duplicated code, and in order to support easily picking which strategy we\u0026rsquo;re using for handling the learning rate, we will delegate the handling of the learning rate to a new type.\nWe\u0026rsquo;ll talk about that soon, but for now just be aware that instead of directly storing a T inside our SGD type, we will be storing a different type that will implement a particular interface.\nFor the struct definition, we don\u0026rsquo;t actually need to change anything!. We only had one field (learning_rate), so we can simply rename this to indicate it should be a handler for our learning rate instead:\npub struct SGD\u0026lt;T\u0026gt; { learning_rate_handler: T, } We rename it also in the new and new_boxed functions:\nimpl\u0026lt;T\u0026gt; SGD\u0026lt;T\u0026gt; { pub fn new(learning_rate_handler: T) -\u0026gt; Self { Self { learning_rate_handler, } } pub fn new_boxed(learning_rate_handler: T) -\u0026gt; Box\u0026lt;Self\u0026gt; { Box::new(Self::new(learning_rate_handler)) } } For the trait implementation, this is where we will need to tell the compiler about this new trait type that we have that will represent our handler, or wrapper that is responsible for storing and updating the learning rate hyperparameter.\nTo do this, we need to introduce a second generic parameter. We place a trait bound on that generic parameter to indicate that it must implement LearningRateHandler, and the type that it\u0026rsquo;s handling/wrapping around must be the same type as the elements in the Network.\nAfter adding this, it looks like follows:\nimpl\u0026lt;T: LearningRateHandler\u0026lt;U\u0026gt;, U: LinalgScalar + ScalarOperand\u0026gt; Optimiser\u0026lt;U\u0026gt; for SGD\u0026lt;T\u0026gt; { // implementation here } In this, we\u0026rsquo;re saying:\n T is the type that\u0026rsquo;s stored inside the SGD and is handling the learning rate hyperparameter. Our trait bound indicates that this type is implementing LearningRateHandler and the type that it\u0026rsquo;s handling is our element type, U U is the type of the elements inside the neural network we\u0026rsquo;re optimising, and contains all the same bounds as it previously did  As for the implementations of the new function, we will delegate to the handler type. This will allow us to have a central handler type with the correct degregation logic, and have that shared between our SGD and SGDMomentum (and any other Optimiser implementations we might write).\nThe new function implementations then just look as follows:\nfn init(\u0026amp;mut self, epochs: u32) { self.learning_rate_handler.init(epochs); } fn end_epoch(\u0026amp;mut self) { self.learning_rate_handler.end_epoch(); } The only other change is that since we aren\u0026rsquo;t storing the learning rate directly now, we must access it instead through the handler object when we\u0026rsquo;re optimising the network parameters:\n*param = \u0026amp;*param - (gradient * *self.learning_rate_handler.learning_rate()); SGDMomentum The same changes are required here basically as we made to the SGD. That is:\n We need to change learning_rate inside the type to be learning_rate_handler This type needs to be independant of the element type (which we use for momentum, and as the element type for storing velocities) We need to implement the new functions from the Optimiser trait, and delegate their behaviour to our LearningRateHandler implementation We need to make sure we get the current learning rate by calling the learning_rate() method on our LearningRateHandler implementor  LearningRateHandler This is our new trait that we are using to define the behaviour and capabilities of whichever type is wrapping the actual learning rate. This trait needs to have the following functionalities:\n It needs to be able to provide the current learning rate to a caller It needs to be able to be initialised at the beginning of training, with the number of epochs that will be run It needs to be able to be updated at the end of an epoch  When coded up, this is pretty easy:\npub trait LearningRateHandler\u0026lt;T\u0026gt; { fn learning_rate(\u0026amp;self) -\u0026gt; \u0026amp;T; fn init(\u0026amp;mut self, epochs: u32); fn end_epoch(\u0026amp;mut self); } LearningRateFixed The final part of refactoring the existing stuff to support the potential for decaying learning rates, while making sure the existing tests and examples continue to work, is a handler for our learning rate that behaves the same as we had our learning rate behaving before.\nThat is, a LearningRateHandler that is initialised with a learning rate, and never changes it.\nFor this struct, it will just contain the actual fixed learning rate inside of itself, and the \u0026ldquo;new\u0026rdquo; function is similarly uneventful:\npub struct LearningRateFixed\u0026lt;T\u0026gt; { learning_rate: T, } impl\u0026lt;T\u0026gt; LearningRateFixed\u0026lt;T\u0026gt; { pub fn new(learning_rate: T) -\u0026gt; Self { Self { learning_rate } } } We\u0026rsquo;ll implement the LearningRateHandler trait for this type, but the only thing it does is to return a reference to the inner learning_rate when asked for it:\nimpl\u0026lt;T\u0026gt; LearningRateHandler\u0026lt;T\u0026gt; for LearningRateFixed\u0026lt;T\u0026gt; { fn learning_rate(\u0026amp;self) -\u0026gt; \u0026amp;T { \u0026amp;self.learning_rate } fn init(\u0026amp;mut self, _epochs: u32) {} fn end_epoch(\u0026amp;mut self) {} } Finally, we just need to make sure wherever we construct an SGD or an SGDMomentum instance, that instead of passing the learning rate, we\u0026rsquo;re now passing an instance of a fixed handler. We just go through and change all the tests/examples to look something like:\nlet mut optim = SGDMomentum::new(LearningRateFixed::new(0.1), 0.9); And with that, our refactor is done!\nThe next couple of sections of this post will be dedicated to the actually decaying handler types.\nLinear Decay The first of the two types of decay that we\u0026rsquo;ll cover is a decay that has a fixed size step between epochs, that is, it always decays the current learning rate by a fixed amount each epoch.\nThe formula for this is\n$$ \\alpha_t = \\alpha_{start} - (\\alpha_{start} - \\alpha_{end}) \\times \\frac t {epochs - 1} $$\nBut rather than computing quantities each frame, we will calculate the amount to be subtracted per epoch at the point where the handler is initialised. We then simply subtract that each time end_epoch is called.\nStruct For the structure itself, we will need to store several quantities which will be:\n The defined starting learning rate. This is the starting rate that the current rate will be set to whenever the init function is called. The defined ending learning rate. After the maximum number of epochs is called, the learning rate will end up being this value (or close to it due to rounding errors). The current learning rate The amount of learning rate to subtract each epoch  These are generic as with everything else, but the type is just defined as \u0026ldquo;T\u0026rdquo;:\npub struct LearningRateLinearDecay\u0026lt;T\u0026gt; { starting_rate: T, ending_rate: T, current_rate: T, decay_per_epoch: T, } Constructor In order to construct a new instance of this structure, the required fields are the starting and ending learning rates that must be passed in. The other two fields are used internally and calculated from these two.\nWe will however require the Clone trait bound on T, because we firstly need to initialise the current learning rate to the starting rate anyway, but also because we need to initialise our decay_per_epoch field to something. We don\u0026rsquo;t mind what we set it to because it\u0026rsquo;ll be recalculated when we initialise during training, but it must be set to something, so we\u0026rsquo;ll just use the starting rate as a valid value.\nThe whole constructor function then looks as follows:\nimpl\u0026lt;T: Clone\u0026gt; LearningRateLinearDecay\u0026lt;T\u0026gt; { pub fn new(starting_rate: T, ending_rate: T) -\u0026gt; Self { Self { starting_rate: starting_rate.clone(), ending_rate, current_rate: starting_rate.clone(), decay_per_epoch: starting_rate, // this will get recalculated during training  } } } Trait Now we just need to implement the LearningRateHandler trait so that we can use this type as a decaying learning rate with one of our optimisers.\nThe init function will simply calculate the amount of decay per epoch. In order to do this, it will use the following formula:\n$$ decay = (start - end) / (epochs - 1) $$\nThat is, it takes the difference between start and end (the distance to decay over), and the number of epochs it needs to decay over (the time it has to decay), and calculates the rate of decay (the speed).\nThe number of epochs is epochs - 1 rather than epochs because we don\u0026rsquo;t call the end_epoch function on the very last epoch so we need to make sure we reach the ending learning rate after we call end_epoch for the very last time (which is 1 iteration fewer than the total number of steps we\u0026rsquo;re doing).\nCoding that up, it will look as follows:\nfn init(\u0026amp;mut self, epochs: u32) { let epochs: T = epochs.into(); self.decay_per_epoch = (self.starting_rate.clone() - self.ending_rate.clone()) / (epochs - T::one()); } Note that we need to clone starting and ending rate as the subtraction operator consumes its inputs. We also need to convert the number of epochs from a u32 into a T before dividing. We do this rather than requiring a bound on Div(as in, dividing a T by u32), because it\u0026rsquo;s more common to have a Div implementation that takes a T on the right hand side.\nReturning the current learning_rate is a trivial operation:\nfn learning_rate(\u0026amp;self) -\u0026gt; \u0026amp;T { \u0026amp;self.current_rate } Finally, when the end_epoch is called, we modify the current learning rate by simply subtracting the decay amount. Again though, we must clone it as the SubAssign trait consumes the right hand side input:\nfn end_epoch(\u0026amp;mut self) { self.current_rate -= self.decay_per_epoch.clone(); } We put all these implementations inside an impl block for the trait, making sure to place the appropriate trait bounds on T based on the operations we required inside the implementations.\nThe block then looks as follows:\nimpl\u0026lt;T: Clone + Div\u0026lt;Output = T\u0026gt; + From\u0026lt;u32\u0026gt; + One + Sub\u0026lt;Output = T\u0026gt; + SubAssign\u0026gt; LearningRateHandler\u0026lt;T\u0026gt; for LearningRateLinearDecay\u0026lt;T\u0026gt; { // implementations here } Exponential Decay Whereas linear decay applies a fixed decay to the learning rate each step, the exponential decay instead applies a multiplier.\nThe definition of the struct, and the new function actually is identical to the linear version. This is because we need the exact same parameters, it\u0026rsquo;s just we\u0026rsquo;re calculating the decay multiplier differently, and applying it to the learning rate differently too.\nThe struct definition and new function then look as follows\npub struct LearningRateExponentialDecay\u0026lt;T\u0026gt; { starting_rate: T, ending_rate: T, current_rate: T, decay_per_epoch: T, } impl\u0026lt;T: Clone\u0026gt; LearningRateExponentialDecay\u0026lt;T\u0026gt; { pub fn new(starting_rate: T, ending_rate: T) -\u0026gt; Self { Self { starting_rate: starting_rate.clone(), ending_rate, current_rate: starting_rate.clone(), decay_per_epoch: starting_rate, } } } The changes come in the implementation of the trait itself. The formula we use to calculate the decay multiplier is as follows:\n$$ {\\frac {end} {start}}^{\\frac {1} {epochs - 1}} $$\nWhich is coded up as below in the init function\nfn init(\u0026amp;mut self, epochs: u32) { let epochs: T = epochs.into(); self.decay_per_epoch = (self.ending_rate / self.starting_rate).powf(T::one() / (epochs - T::one())); } Accessing the learning rate in the learning_rate method remains the same as the linear version. The only other change except from calculating the decay rate is actually applying it.\nIn the linear version we had used subtraction to subtract the decay amount from the learning rate at the end of an epoch, however in the exponential version we need to multiply it instead, so the end_epoch method looks as follows\nfn end_epoch(\u0026amp;mut self) { self.current_rate *= self.decay_per_epoch; } Testing In order to test these decay methods, we will create two new examples, based off the softmax_cross_entropy_loss_with_momentum example. These will be:\n softmax_cross_entropy_loss_with_linear_decay - A version that applies a linear decay to the learning rate. softmax_cross_entropy_loss_with_exponential_decay - A version that applies an exponential decay to the learning rate.  As a refresher, we\u0026rsquo;ll re-run the base version (with no decay) and we can see that we get a decent accuracy:\nAccuracy (training): 93.47999999999999%\rAccuracy (testing): 93.522%\rFor the linear decay, we\u0026rsquo;ll still start with a learning rate of 0.1, but we\u0026rsquo;ll decay down to 0.05 over our epochs, and the results we can see are slightly better:\nAccuracy (training): 94.582%\rAccuracy (testing): 94.624%\rFinally, we\u0026rsquo;ll test the exponential decay with the same start and end learning rates, however we see a sudden drop in accuracy!. Using exponential decay results in a drop to 90% accuracy.\nThis is because the decay rate with exponential is much steeper, and so we are taking too small a step too quickly.\nOne thing this steep decay lets us do though is take larger steps at the beginning. Let\u0026rsquo;s go ahead and change the starting learning rate to something slightly higher (0.2) and we see that the results are better again:\nAccuracy (training): 95.506%\rAccuracy (testing): 95.562%\r95% aint bad!\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/learningratedecay/","summary":"As we\u0026rsquo;ve covered a couple of different optimisation strategies (stochastic gradient descent with and without momentum), it\u0026rsquo;s become clear that the learning rate hyperparameter is one of, if not the most important hyperparameters in deep learning.\nA hyperparameter in deep learning is a tweakable value that needs to be tuned based on the problem at hand. These include the learning rate, momentum, epoch count, batch size, etc. and half of the success for training a network is finding the correct hyperparameter values.","title":"4.5 - Learning Rate Decay"},{"content":"So far, when we have updated the weights on our parameters with their gradients, it has been an instantaneous update without any temporal relationship with previous optimisation steps.\nThis means that between two successive epochs, the velocity change can be instantaneous. One epoch the velocity could be low, and the next it might be super high.\nIn terms of the updating of the weights with respect to the gradients of parameters, we can imagine this from the graph visualisation as being the object moving up and down the line of the graph, or bouncing around the troughs and valleys created by such a graph (there was a diagram in a previous post showing the arrows bouncing around like that).\nHowever, in real life, objects don\u0026rsquo;t suddenly change direction on an instant and instead if the velocity (gradient) stops or becomes 0, the object will slow down over time before coming to a stop.\nWe can simulate this momentum in our neural network optimisation also.\nThe rationale is that when we reach a trough in the graph, and the gradient is 0, the weights will still be updated for a time (although the rate will slow on successive iterations). This preservation of momentum can allow the weights to escape a false, or local minima.\nFormula What this means in practice is that the amount we use to update the weight of each parameter (the velocity) isn\u0026rsquo;t just a function of the gradient at that point, and the learning rate but instead is a weighted average of the velocities at previous timesteps with the weights decaying exponentially.\nThe formula to calculate this sum is pretty simple and is defined as:\n$$ update = \\nabla_t + (\\mu \\times \\nabla_{t-1}) + (\\mu^2 \\times \\nabla_{t-2}) + \u0026hellip; $$\nWhere \\(\\mu\\) is the momentum parameter, or, how much the velocity from the previous timestep is degraded.\nFor example if the momentum parameter is 0.9, then in a given time step, we multiply the velocity from the previous timestep by 0.9, or 90%, the one from two timesteps ago by \\(0.9^2\\) (0.81), the one from three time steps ago by \\(0.9^3\\) (0.729), and so on. We then add them all up to get the total velocity for the current time step.\nHowever in practice, we won\u0026rsquo;t compute the entire history each update, as we\u0026rsquo;ll soon see.\nAlgorithm In order to implement the momentum, we can\u0026rsquo;t be computing the entire historical sum every update, so we should find a better way to calculate what the velocity should be.\nAs it turns out, we can just do the following steps to update the velocity with momentum, and the current gradient:\n Multiply the velocity by the momentum parameter (0.9 in the above example) Add the current gradient in  As an example, this is what the stored velocity would represent at each timestep, starting at t=1:\n \\(\\nabla_1\\) \\(\\nabla_2 + (\\mu \\times \\nabla_1)\\) \\(\\nabla_3 + (\\mu \\times \\nabla_2) + (\\mu^2 \\times \\nabla_1)\\)  And so on, all with us only needing to store a single quantity and not a history of previous quantities.\nImplementation To implement this, we will be create a new implementor of the Optimiser trait, so far we have only had the basic Stochastic Gradient Descent (SGD) optimiser.\nWe will still be following the same basic algorithm of SGD and updating the weight with a proportion of the gradient, however we will want to incorporate momentum into the calculation of velocity.\nWe call this new optimiser SGDMomentum\nSDGMomentum The structure itself is a bit more than with the regular SGD optimiser. The regular one only had a learning rate, which was the proportion of the gradient to update the weights with.\nThis new optimiser needs two additional pieces of data stored to retain state between update steps:\n velocities - This will be a Vec of Arrays with the elements in the same order as they appear in the iterator returned by the parameters_and_gradients() function on Network. Thus, the velocity at index i will be the velocity for the parameter/weight at index i. momentum - This is a simple parameter like learning rate which represents the multiplier we will update the current velocity with before updating it with the current gradient. Represents the degredation of velocity when no forces are acting on it (slowing down).  One thing to note is that we can only calculate the velocities when there is a valid iterator of parameter weights and gradients from the network, and this is only valid after one forward and backward pass have been performed.\nThis means that when we create the optimiser the vector will be uninitialised and we will need to initialise it the first time we\u0026rsquo;re optimising. We will therefore use Option to represent the None state initially, and it will be replaced with Some once the velocities are initialised.\nThe whole definition of our structure looks as follows\npub struct SGDMomentum\u0026lt;T\u0026gt; { velocities: Option\u0026lt;Vec\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;\u0026gt;, learning_rate: T, momentum: T, } Construction In order to construct a new instance of the Optimiser we require both a learning rate, and a momentum parameter. The velocities as mentioned previously are initialised to None by default until the first step is performed.\nThe \u0026ldquo;new\u0026rdquo; function we add to handle the creation of a new instance looks as follows\nimpl\u0026lt;T\u0026gt; SGDMomentum\u0026lt;T\u0026gt; { pub fn new(learning_rate: T, momentum: T) -\u0026gt; Self { Self { velocities: None, learning_rate, momentum, } } } Optimiser Now we just need to implement the Optimisertrait for SGDMomentum, which requires only a single function, step. Step operates on a mutable reference to self, to allow us to retain state between calls (which is ideal because that\u0026rsquo;s precisely what we\u0026rsquo;re doing with velocities), and it takes a mutable reference to the Network to optimise.\nThe very first thing we will want to do is to check if the velocities list has been initialised (is some). If it\u0026rsquo;s still None then this is the first time we\u0026rsquo;ve had the step function called, and we need to initialise one velocities array for each parameter weight entry, and in the same order.\nEach velocity Array will have the same shape as the associated network parameter weight, since we will be performing an elementwise subtraction when we update the weights. The elements will start at all zeroes initially as the velocity is \u0026ldquo;at rest\u0026rdquo;.\nIn order to check if we need to initialise the velocities, we can use the following function provided by Option\nif self.velocities.is_none() { // initialisation code } What we want to do to create the Vec of velocities is take the iterator provided by the network that iterates over all the parameter weights in the network (provided by the parameters_and_gradients function), and for each element of that iterator, we want to produce an Array of the same shape that\u0026rsquo;s all zeroes.\nThere is a method provided to us by the Iterator trait for this which is the map function. Finally, we can take that new iterator and use the collect function to allocate a Vec and collect the iterator into it.\nThe code for that looks as follows:\nnet.parameters_and_gradients() .map(|(param, _)| Array::zeros(param.raw_dim())) .collect() Then we can do the actual update. For this we will once again want to iterate over the parameters and gradients in the network, but we want to do this in tandem with their current velocities.\nAnother provided method by the Iterator trait is zip which is called on an iterator, takes another iterator and produces a new iterator whose elements are a tuple of the elements from each.\nTherefore each element in the zipped iterator will be a tuple containing the parameter/gradient, and the associated velocity for that parameter.\nFinally we want to run \u0026ldquo;some code\u0026rdquo; for each element in the zipped iterator, which we can do with the for_each method.\nThis section of code then looks as follows:\nnet.parameters_and_gradients() .zip(self.velocities.as_mut().unwrap().iter_mut()) .for_each(|((parameter, gradient), velocity)| { // do stuff here  }); The code we put inside the callback to for_each will be the code to update one parameter, given its velocity.\nWe can do this elementwise by zipping the weight, gradient, and velocity arrays together which will let us iterate over their elements in an arbitrary order, but in lockstep with one another.\nndarray gives us a macro to perform this iteration, called azip (probably for Array Zip).\nThe way this macro is called is you provide a set of specifiers and arrays from which to draw the values for each specifier, in a syntax like a parameter list. The code to run for each set of elements goes in curly braces. For example, the following snippet:\nazip!((parameter in parameter, gradient in gradient, velocity in velocity) { // do stuff here }); will iterate over the elements of the parameter, gradient, and velocity arrays in lockstep with each other, and for each element will bind the appropriate element of parameter to the binding \u0026ldquo;parameter\u0026rdquo;, the appropriate element of gradient to the binding \u0026ldquo;gradient\u0026rdquo;, and velocity to \u0026ldquo;velocity\u0026rdquo;. This is a feature in Rust called shadowing and is useful because it means that inside the function, we can\u0026rsquo;t accidentally use them as arrays and are limited to only operating on the individual elements provided for that iteration.\nInside the function body, to update a single element we want to do the following sequence of steps:\n Multiply the current velocity by momentum Add the correct proportion of the current gradient to the velocity (using learning rate) Update the parameter weight by the new velocity  We can use standard arithmetic operations for these, however we need to explicitly clone the various parameters since we\u0026rsquo;re in a generic context and only require Clone, not Copy as a trait bound. The resulting function body is then:\n*velocity = (*velocity).clone() * momentum.clone() + (*gradient).clone() * learning_rate.clone(); *parameter -= (*velocity).clone(); Note the use of the dereference (*) operator, this is because we\u0026rsquo;re iterating over references to arrays, meaning the elements we\u0026rsquo;re iterating over are also references. We need to dereference them whenever we want to access the value (like to update the parameter, or velocity).\nIn order to support these operations, the following trait bounds are required on the element type T\n Clone - Required because as mentioned before, we need to reuse things like the learning rate and momentum, and also arithmetic operators take operands by value (which we can\u0026rsquo;t do in generics out of a reference, can\u0026rsquo;t move from a reference) Mul\u0026lt;Output=T\u0026gt; - This is required because we\u0026rsquo;re multiplying a T by a T to produce a new T when we\u0026rsquo;re updating the velocity SubAssign - Used when updating the parameter by velocity, we use the -= operator Zero - Required for initialising the velocities on the first update, initialised to all zeroes which requires we know what \u0026ldquo;zero\u0026rdquo; of a \u0026ldquo;T\u0026rdquo; means  The code then in its entirety looks as follows:\nimpl\u0026lt;T: Clone + Mul\u0026lt;Output = T\u0026gt; + SubAssign + Zero\u0026gt; Optimiser\u0026lt;T\u0026gt; for SGDMomentum\u0026lt;T\u0026gt; { fn step(\u0026amp;mut self, net: \u0026amp;mut Network\u0026lt;T\u0026gt;) { // if this is the first step/iteration, we should set up the velocities  // at zeroes, but matching the dimensions of the parameters.  if self.velocities.is_none() { self.velocities = Some( net.parameters_and_gradients() .map(|(param, _)| Array::zeros(param.raw_dim())) .collect(), ); } // weight updating can be done with the parameters and gradients iterator in lockstep  // with the velocities.  let momentum = self.momentum.clone(); let learning_rate = self.learning_rate.clone(); net.parameters_and_gradients() .zip(self.velocities.as_mut().unwrap().iter_mut()) .for_each(|((parameter, gradient), velocity)| { azip!((parameter in parameter, gradient in gradient, velocity in velocity) { *velocity = (*velocity).clone() * momentum.clone() + (*gradient).clone() * learning_rate.clone(); *parameter -= (*velocity).clone(); }); }); } } Testing As you may recall from the previous post, the accuracy for the basic neural network was around 16% which was the same as a simple linear regression. This seemingly is due to it falling into a local minima that it can\u0026rsquo;t escape.\nMomentum allows it to escape a local minima to get a higher accuracy, which is tested by replacing our standard SGD optimiser with the new SGDMomentum optimiser. For this test we will use a learning rate of 0.1, and a momentum of 0.9\nAfter changing the code and running, against the same test as we had before, we get the following output:\nAccuracy (training): 59.955999999999996%\rAccuracy (testing): 60.126000000000005%\rSo we see that it\u0026rsquo;s increased to \u0026ldquo;better than average\u0026rdquo;. This is a decent jump from 16%!\nFor softmax, we were already at a pretty high accuracy, with the standard SGD optimiser scoring about 91% accuracy.\nHowever, momentum gives us gains even here, and when we run the same problem with the same network, and change to using the momentum based optimiser we get slightly better results again:\nAccuracy (training): 93.47999999999999%\rAccuracy (testing): 93.522%\rSo from these we can see that using a momentum-based optimiser is better than a basic SGD optimiser.\nAs a side note, using momentum doesn\u0026rsquo;t improve linear regression at all. This is likely because linear regression can only simulate a straight line, so the best or most accurate it can get to it was already achieving, and using momentum to find that ideal state doesn\u0026rsquo;t help.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/momentum/","summary":"So far, when we have updated the weights on our parameters with their gradients, it has been an instantaneous update without any temporal relationship with previous optimisation steps.\nThis means that between two successive epochs, the velocity change can be instantaneous. One epoch the velocity could be low, and the next it might be super high.\nIn terms of the updating of the weights with respect to the gradients of parameters, we can imagine this from the graph visualisation as being the object moving up and down the line of the graph, or bouncing around the troughs and valleys created by such a graph (there was a diagram in a previous post showing the arrows bouncing around like that).","title":"4.4 - Momentum"},{"content":"This post will describe a new loss function called \u0026ldquo;cross entropy loss\u0026rdquo;. Due to often being coupled with the \u0026ldquo;softmax\u0026rdquo; function, this is often just called the \u0026ldquo;softmax cross entropy loss\u0026rdquo; function. There are a couple of components needed to support that.\nSoftmax Thus far we used the mean squared error as a loss function which has a nice property that the further the prediction is from the target, the steeper the gradient that\u0026rsquo;s sent in the backpropagation pass is which enables it to converge quite fast.\nHowever, if the problem is a classification problem where the output is identifying one of a set of classes which the observation evaluates to, then we know that the predictions that are output for those classes are the probabilities of the observation being in that category.\nDue to being a set of probabilities, we know:\n Each individual probability should be between 0 and 1 All probabilities summed up should total 1  The softmax function will exploit these properties in order to produce even steeper gradients to converge faster for classification problems.\nFor a problem with 3 classes, the network will output a vector of 3 elements per observation as predictions, however the network will provide these as raw floating point values which aren\u0026rsquo;t necessarily between 0 and 1.\nFor a vector such as \\(\\begin{bmatrix}5 \u0026amp; 3 \u0026amp; 2\\end{bmatrix}\\) we want to convert this into a vector of probabilities. The simplest way to do this would be to normalize the vector, that is dividing each element by the total sum. For this specific vector, it would look like follows\n$$ Normalize(\\begin{bmatrix}5 \\\\ 3 \\\\ 2\\end{bmatrix}) = \\begin{bmatrix}\\frac 5 {5 + 3 + 2} \\\\ \\frac 3 {5 + 3 + 2} \\\\ \\frac 2 {5 + 3 + 2}\\end{bmatrix} = \\begin{bmatrix}\\frac 5 {10} \\\\ \\frac 3 {10} \\\\ \\frac 2 {10}\\end{bmatrix} = \\begin{bmatrix}0.5 \\\\ 0.3 \\\\ 0.2\\end{bmatrix} $$\nHowever it turns out that the softmax function instead will provide steeper gradients which is to say that it will be able to bias the probabilities towards that which it thinks is correct.\nThe formula for softmax is much the same as regular normalization except that we\u0026rsquo;re using the values as powers of the mathematical constant, e.\nFor that same example then, the softmax function look like follows\n$$ Softmax(\\begin{bmatrix}5 \\\\ 3 \\\\ 2\\end{bmatrix}) = \\begin{bmatrix}\\frac {e^5} {{e^5} + {e^3} + {e^2}} \\\\ \\frac {e^3} {{e^5} + {e^3} + {e^2}} \\\\ \\frac {e^2} {{e^5} + {e^3} + {e^2}}\\end{bmatrix} = \\begin{bmatrix}\\frac {148.41315910257660342111558004055} {175.88775212469492138927453715571} \\\\ \\frac {20.085536923187667740928529654582} {175.88775212469492138927453715571} \\\\ \\frac {7.389056098930650227230427460575} {175.88775212469492138927453715571}\\end{bmatrix} = \\begin{bmatrix}0.84379473448133947005179288509305 \\\\ 0.11419519938459447893014170254169 \\\\ 0.04201006613406605101806541236525\\end{bmatrix} $$\nShortening these results to 2 decimal places gives us \\(\\begin{bmatrix}0.84 \u0026amp;\u0026amp; 0.11 \u0026amp;\u0026amp; 0.04\\end{bmatrix}\\)\nAs we can see the calculation is now less neutral to the answer already closest to 1 and so this function biases it more toward 1 by reducing the value of the less favourable results.\nIf we were to apply a max function which sets the probability of the highest prediciton to 1 and the others to 0, then we\u0026rsquo;d end up with \\(\\begin{bmatrix}1, 0, 0\\end{bmatrix}\\) instead.\nAs we can see, softmax generates results halfway between a standard normalization, and the max function, hence the name softmax.\nCross entropy loss The second portion is the actual loss calculation which is called cross entropy loss and again is specific to classification problems where the predictions and targets represent the probabilities of being in a particular class.\nThe cross entropy loss function which is represented by the following dual case formula\n$$ CE(p_i, y_i) = \\begin{cases} -log(1 - p_i), \u0026amp; \\text{if } y_i = 0 \\\\ -log(p_i), \u0026amp; \\text{if } y_i = 1 \\end{cases} $$\nWhere \\(p_i\\) is an entry in the predictions probabilities output by the softmax function at index \\(i\\), and \\(y_i\\) is the target at the same index which is guaranteed to be either a 0 or a 1 (since this is a classification problem).\nThis can actually be represented in a single formula due to the target being either a 0 or a 1\n$$ CE(p_i, y_i) = -y_i \\times log(p_i) - (1 - y_i) \\times log(1 - p_i) $$\nThis works because, if \\(y_i\\) is a 0 then the first term will be eliminated, and if it\u0026rsquo;s a 1 then the second term is eliminated (due to 1 - 1 being 0).\nIf we look at a graph comparing the loss values generated by the cross entropy loss, compared to those generated by the mean squared error function we can see that the penalties are not only higher from the beginning, but that they get higher at a much steeper rate\nIn fact, as can be seen from the graph, as the difference between our prediction and target approaches 1, the loss approaches infinity.\nThis produces greater penalties for loss, and the softmax function produces steeper gradients, so the two combined together will enable us to move weights by a larger amount and train the network much faster. These two are often used together, and so is usually seen as the softmax cross entropy loss\nThe formula for this can be written as follows (assuming 3 classes in the output)\n$$ SCE_1 = -y_1 \\times log(\\frac {e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3}}) - (1 - y_1) \\times log(1 - \\frac {e^{x_1}} {e^{x_1} + e^{x_2} + e^{x_3}}) $$\nThis more complex formula might seem to have an equally complex derivative calculation, however it\u0026rsquo;s a very simple formula in fact\n$$ \\frac {\\partial {SCE_1}} {\\partial x_1} = {\\frac {e^{x_1}} {e^{x_1} + e^{x_2} + e^{x_3}}} - y_1 $$\nWhich means that the derivative is simply\n$$ softmax(\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}) - \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} $$\nCoding Now we\u0026rsquo;re ready to implement this as a loss function which as it turns out will be fairly simple. Each row of the input data to the loss function will be a prediction with X columns (for X classes) that we pass through the softmax function to normalize them into a set of probabilities.\nAfter the softmax, we will do the cross entropy loss calculation, and this is where we must be careful. Cross entropy loss as mentioned before is asymptotic and will tend towards infinity as the difference approaches 1, so to avoid this we clip the output of the softmax function so that the prediction is neither 0 nor 1, but we\u0026rsquo;ll clamp to some small epsilon value.\nSoftmaxCrossEntropy struct For the struct, as with all of our other \u0026ldquo;vtable\u0026rdquo; style types, it will be an empty type - though we need to make sure it\u0026rsquo;s Clonable due to the cloning of the neural network required in the training process\n#[derive(Clone)] pub struct SoftmaxCrossEntropy; The only function we\u0026rsquo;ll have associated with this type is one that can create a new boxed instance for convenience. For consistency we call this new_boxed\npub fn new_boxed() -\u0026gt; Box\u0026lt;Self\u0026gt; { Box::new(Self) } LossImpl trait bounds For implementing the LossImpl trait, we require that we are implementing it only for those types that support the necessary functionality. These are mostly arithmetic, but in particular the num_traits::Float trait will provide us a few functions we\u0026rsquo;re going to make use of.\nWe need the following traits placed as bounds on the type T\n Float - This is from num_traits and provides us functions such as epsilon() and exp() ScalarOperand - This is from ndarray and is required because there are a few points where we are dividing or otherwise operating on the whole array with a value of type T  Therefore the declaration that we are implementing LossImplfor our SoftmaxCrossEntropy type will look as follows\nimpl\u0026lt;T: Float + ScalarOperand\u0026gt; LossImpl\u0026lt;T\u0026gt; for SoftmaxCrossEntropy LossImpl::calculate_output In order to calculate the output from our given set of predictions and targets, we firstly will need to apply the softmax function to turn the predictions into probabilities, and as mentioned previously need to also clamp the output to within +/- some epsilon value.\nThere is however something else we need to consider which is that if our predictions and targets have only a single class that a value either can be in (1) or not at all (0), then applying the softmax function would always produce a 1 since any number divided by itself is 1.\nIn order to ensure that the softmax can work on a single-class problem, we will need to modify it to be a dual class problem.\nWe can do this by treating the existing column as the probability that the observation is in the class, and therefore we can add a new column that is the probability of it not being in the class. We can calculate this by knowing that probabilities sum to 1, so the value of this column will be 1-x where x is the value in the existing one.\nTherefore the vector of predictions in a single class problem \\(\\begin{bmatrix} 0.4 \\\\ 0.1 \\\\ 0.9 \\end{bmatrix}\\) would be mapped onto the dual class problem matrix of \\(\\begin{bmatrix} 0.4 \u0026amp; 0.6 \\\\ 0.1 \u0026amp; 0.9 \\\\ 0.9 \u0026amp; 0.1 \\end{bmatrix}\\)\nHowever, since we\u0026rsquo;re turning the single class problem into a dual class one, we need to also do the same for the target vector so that the cross entropy can be applied correctly.\nWe will wrap all this up into a helper function that we\u0026rsquo;ll detail shortly called calculate_softmax_predictions_and_targets which we call as such in our calculate_output method\nlet (predictions, targets) = calculate_softmax_predictions_and_targets((*predictions).clone(), (*targets).clone()); Next we can apply the cross loss to the predictions and targets. Recall from the formula of the cross entropy loss that we can calculate it as a subtraction sum.\nThe left hand side of the subtraction (called the minuend) is \\(-y \\times log(p) \\) which means that we need to get a version of the targets array that is negated, and a version of the predictions array that is the natural log of the elements.\nIn code this looks as follows\nlet minuend = targets.mapv(|elem| -elem) * predictions.mapv(|elem| elem.ln()); The right hand side of the formula (called the subtrahend) is \\((1 - y) \\times log(1 - p)\\) so the subtrahend looks as follows\nlet subtrahend = targets.mapv(|elem| T::one() - elem) * predictions.mapv(|elem| (T::one() - elem).ln()); Then we can perform the subtraction to get the loss Array\nlet result = minuend - subtrahend; Finally, the loss function is expected to return a single value representing the total loss over all predictions, so we need to sum the results to get the final loss value\nresult.sum() Putting all of this together we get the entire calculate_output function\nfn calculate_output(\u0026amp;self, predictions: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, targets: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; T { let (predictions, targets) = calculate_softmax_predictions_and_targets((*predictions).clone(), (*targets).clone()); let minuend = targets.mapv(|elem| -elem) * predictions.mapv(|elem| elem.ln()); let subtrahend = targets.mapv(|elem| T::one() - elem) * predictions.mapv(|elem| (T::one() - elem).ln()); let result = minuend - subtrahend; result.sum() } LossImpl::calculate_input_gradient The second required function is to be able to calculate the input gradients during the backward pass. For this we firstly need to know whether this is a single class problem or not, this is because when we calculate the softmax predictions on a single class problem, we are converting it to a dual class problem.\nBecause of this, the gradients produced will also be of a dual class nature, so before propagating back through the network, this extra dummy column we added needs to be stripped off.\nWe can cache whether it\u0026rsquo;s a single class Array by simply checking the number of columns in it is 1\nlet is_single_class = predictions.ncols() == 1; We need to go ahead and recalculate the predictions because we aren\u0026rsquo;t retaining any state in our vtable trait objects, but won\u0026rsquo;t show that here.\nRecalling that calculating the gradient for the cross entropy loss function is simply the result of subtracting the targets Array from the calculated predictions, we can implement it as such\nlet gradient = predictions - targets; The last step is to strip away that dummy column we added if the problem is a single class problem, which we can do through a helper function\nif is_single_class { dual_class_to_single(gradient) } else { gradient } Putting all of this together gives us the complete implementation for our gradient calculation function\nfn calculate_input_gradient( \u0026amp;self, predictions: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, targets: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { let is_single_class = predictions.ncols() == 1; let (predictions, targets) = calculate_softmax_predictions_and_targets((*predictions).clone(), (*targets).clone()); let gradient = predictions - targets; if is_single_class { dual_class_to_single(gradient) } else { gradient } } calculate_softmax_predictions_and_targets (helper) As a reminder, we want this helper function to prepare the predictions and targets for our cross entropy loss calculation, by doing the following:\n Adding a dummy column if it\u0026rsquo;s a single class Array so that softmax can work with a dual class Array Run the predictions through the softmax function to turn them into probabilities Clamp the probabilities so that they don\u0026rsquo;t ever hit 0 or 1 due to asymptotic calculations when we use log during the cross entropy loss calculation  The first step to this is easy enough, but we will delegate this to a helper function that we will cover later. For calling it, we will simply check if the number of columns in the predictions is 1, and if it is then we will update both predictions and targets to have the additional column\nif predictions.ncols() == 1 { predictions = single_class_to_dual(predictions); targets = single_class_to_dual(targets); } Then we apply the softmax function on the predictions which is done with a simple call, again we\u0026rsquo;ll defer covering the softmax function until later\nlet mut predictions = softmax(predictions) For the third point, clamping the probabilities, we can do this by using a method called \u0026ldquo;clamp\u0026rdquo; from the num_traits library. This function works with any T where T implements the Float trait. Conveniently, Float also gives us an associated method \u0026ldquo;epsilon\u0026rdquo; which gives us a small epsilon value to clamp with.\nWe can map the predictions in place, and for each element of the predictions array, perform the clamp. This looks as follows\npredictions.mapv_inplace(|elem| num_traits::clamp(elem, T::epsilon(), T::one() - T::epsilon())); Finally we\u0026rsquo;re able to return the predictions and targets. As an entire function it looks as follows\nfn calculate_softmax_predictions_and_targets\u0026lt;T: Float\u0026gt;( mut predictions: Array\u0026lt;T, Ix2\u0026gt;, mut targets: Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; (Array\u0026lt;T, Ix2\u0026gt;, Array\u0026lt;T, Ix2\u0026gt;) { if predictions.ncols() == 1 { predictions = single_class_to_dual(predictions); targets = single_class_to_dual(targets); } let mut predictions = softmax(predictions); predictions.mapv_inplace(|elem| num_traits::clamp(elem, T::epsilon(), T::one() - T::epsilon())); (predictions, targets) } softmax This function will be responsible for applying softmax to the Array of predictions. We actually will need to apply this independently on each row\nWe know that in softmax calculation, we are working with values of \\(e^x\\) and don\u0026rsquo;t need to keep the original values around. Therefore the first step should be to prepare the input array by calculating each element to be e raised to that power as such\narr.map_inplace(|elem| *elem = elem.exp()); ndarray::Array contains a convenient function called map_axis which takes an axis to map over, and a function. The function will take an entire slice of that axis at a particular index, and should return a single value. This results in an Array that is one dimension less.\nFor our purposes we will iterate in the direction of the columns which is Axis(1), and the function will therefore accept one row at a time. We just sum the row to get the total of all features in that row.\nThis will give us a 1 dimensional array, but for the division we will be doing, to ensure that the broadcasting works correctly, we need to reshape this into a 2 dimensional array, with a single column. Therefore each entry in the column vector will be a total for the associated row in the input matrix.\nWe can use the into_shape function to reshape the totals, but we need to unwrap the Result as if the number of elements in the input don\u0026rsquo;t match the requested dimensions it will give an error. In this case we know it has to be correct so we can unwrap it safely.\nThis looks as follows\nlet totals = arr .map_axis(Axis(1), |row| row.sum()) .into_shape((arr.nrows(), 1)) .unwrap(); Finally we can divide the array by the totals. Due to the way operator broadcasting works, this will broadcast the division of a multi-column vector by a single column one such that each element in a given row is divided by the associated element in the divisor.\nFor clarity, this means that\n$$ \\begin{bmatrix} x_1 \u0026amp; x_2 \u0026amp; x_3 \\\\ y_1 \u0026amp; y_2 \u0026amp; y_3 \\end{bmatrix} / \\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\end{bmatrix} = \\begin{bmatrix} x_1/z_1 \u0026amp; x_2/z_1 \u0026amp; x_3/z_1 \\\\ y_1/z_2 \u0026amp; y_2/z_2 \u0026amp; y_3 /z_2 \\end{bmatrix} $$\nThe total function is then as follows (note the Float trait bound so we have access to the exp function and also to arithmetical operators)\nfn softmax\u0026lt;T: Float\u0026gt;(mut arr: Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { arr.map_inplace(|elem| *elem = elem.exp()); let totals = arr .map_axis(Axis(1), |row| row.sum()) .into_shape((arr.nrows(), 1)) .unwrap(); arr / totals } single_class_to_dual This is the first of the two \u0026ldquo;mapping\u0026rdquo; functions we use to convert a single class problem into a dual class one for the purposes of being able to apply softmax and cross entropy appropriately.\nThis one takes the initial predictions array and adds an additional feature/column onto it representing the inverse probabilities, or the probabilities of not being in the single class.\nTo calculate this, we will iterate over the elements of the input array (which we validate is a single column so each element is a prediction for a row), we will then use a flat_map to map this single element onto an iterator that produces both the element, and then the inverse of the element (1 - element). We can then take this iterator and use Array::from_iter to convert it into a 1-D array.\nHowever, all of the code expects the array to be a 2-D array, so we must reshape it to be as such.\nThe function must also have appropriate bounds on \u0026ldquo;T\u0026rdquo; to be able to use the various functions. These bounds will be:\n Clone - We need to be able to clone elements in the predictions array so as to be able to emit it, and also use it to calculate the value in the additional column One - We need this trait to be able to get \u0026ldquo;one\u0026rdquo; of a \u0026ldquo;T\u0026rdquo; as we\u0026rsquo;ll need it to calculate the value in the second column Sub\u0026lt;Output=T\u0026gt; - This is used also in calculating the value for the new column as we need to be able to subtract the prediction (a \u0026ldquo;T\u0026rdquo;) from our \u0026ldquo;one\u0026rdquo; (also a \u0026ldquo;T\u0026rdquo;)  Overall the function looks as follows\nfn single_class_to_dual\u0026lt;T: Clone + One + Sub\u0026lt;Output = T\u0026gt;\u0026gt;(input: Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { assert_eq!(input.ncols(), 1); let rows = input.nrows(); Array::from_iter(input.iter().flat_map(|elem| { std::iter::once((*elem).clone()).chain(std::iter::once(T::one() - (*elem).clone())) })) .into_shape((rows, 2)) .unwrap() } dual_class_to_single The final function will go the other way which is to take a 2-D array with 2 columns and strip the last column off, retaining only the first one.\nWe can do this with the select function provided by ndarray::Array which takes the Axis to select from, and the indices to retain.\nIn our case, we are selecting from the columns of the Array which is Axis(1), and we only need select index 0.\nThe total function is then pretty simple\nfn dual_class_to_single\u0026lt;T: Clone\u0026gt;(input: Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { assert_eq!(input.ncols(), 2); input.select(Axis(1), \u0026amp;[0]) } Performance Now that we\u0026rsquo;ve implemented the Tanh activation function previously (in a previous post), and now the softmax cross entropy loss function, we can test on the sample data set/problem and see how accurate the various approaches are.\nFirstly we have a linear regression, which is simply a single dense layer with 1 neuron, and a linear activation function. For the loss function we use MeanSquaredError, so the code of the network setup looks as follows\nlet mut network = Network::new( vec![Layer::new_with_seed( 1, DenseLayerSetup::new_boxed(Linear::new_boxed()), SEED, )], Loss::new(MeanSquaredError::new_boxed()), ); And running the example shows that the linear regression in fact only has a small percentage chance of correctly guessing the right answer\nAccuracy (training): 16.652%\rAccuracy (testing): 16.796%\rSecondly we try out a basic neural network. For this, we will have two dense layers. The first dense layer will use the Tanh activation function and will have multiple neurons (we choose 54 here). The second layer will have a single neuron with a linear activation function and will serve to bring together the outputs from the hidden layer into a single output for use with the loss function.\nAgain, we use a basic MeanSquaredError loss function so the setup code looks as follows\nlet mut network = Network::new( vec![ Layer::new_with_seed(54, DenseLayerSetup::new_boxed(Tanh::new_boxed()), SEED), Layer::new_with_seed(1, DenseLayerSetup::new_boxed(Linear::new_boxed()), SEED), ], Loss::new(MeanSquaredError::new_boxed()), ); Surprisingly, this function results in exactly the same predictions as the linear regression, which is to say using a basic neural network with a MeanSquaredError loss function is resulting in a very low accuracy. It\u0026rsquo;s the same figure as shown by the following output.\nAccuracy (training): 16.652%\rAccuracy (testing): 16.796%\rThis at least indicates that our problem can at least be somewhat solved by a basic line in linear regression, however it\u0026rsquo;s surprising that a more complex network is not able to get any more accurate. This could be an issue with the MeanSquaredError.\nFinally, we make only one small change to the basic neural network, which is to swap the MeanSquaredError loss function for our new SoftmaxCrossEntropy loss function. This is the only change to the setup as shown by the following code\nlet mut network = Network::new( vec![ Layer::new_with_seed(54, DenseLayerSetup::new_boxed(Tanh::new_boxed()), SEED), Layer::new_with_seed(1, DenseLayerSetup::new_boxed(Linear::new_boxed()), SEED), ], Loss::new(SoftmaxCrossEntropy::new_boxed()), ); However, the accuracy on our problem data has shot right up!\nAccuracy (training): 91.786%\rAccuracy (testing): 91.742%\rThis indicates that the steeper gradients and harsher loss penalties given by softmax and cross entropy loss respectively are resulting in a much better fit after training the network. Thus we can conclude that this loss function is much better for classification type problems.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/softmax/","summary":"This post will describe a new loss function called \u0026ldquo;cross entropy loss\u0026rdquo;. Due to often being coupled with the \u0026ldquo;softmax\u0026rdquo; function, this is often just called the \u0026ldquo;softmax cross entropy loss\u0026rdquo; function. There are a couple of components needed to support that.\nSoftmax Thus far we used the mean squared error as a loss function which has a nice property that the further the prediction is from the target, the steeper the gradient that\u0026rsquo;s sent in the backpropagation pass is which enables it to converge quite fast.","title":"4.3 - Softmax"},{"content":"There are a couple of things that were required to be tweaked as we go on with development of the extensions to the network API. This post will briefly explain the changes that were required before we move onto the extensions themselves.\nChanging the examples Eidetic at this point has examples that are showing the performance of a linear regression vs a basic neural network, both using the \u0026ldquo;mean squared error\u0026rdquo; loss function.\nWe are planning to add more examples to demonstrate the various techniques as we learn how we can potentially optimise the training of a network.\nThere are a couple of things we want to change at this point.\nChanging the data set The previous data set we were using for training was a \u0026ldquo;real world\u0026rdquo; data set of credit scores mapped to a category of whether those scores are \u0026ldquo;good\u0026rdquo; or \u0026ldquo;bad\u0026rdquo;.\nThis works well, except that as we\u0026rsquo;re learning we\u0026rsquo;d ideally like to avoid such a large data set (that data set had around 20 features) that will also bring in questions of data preparation and scaling that we\u0026rsquo;re not prepared to answer yet.\nThe main issue being complexity, but also the fact that the individual features aren\u0026rsquo;t necessarily related scale-wise.\nWe\u0026rsquo;d ideally like a simpler data set, still complex enough to allow us to demonstrate the training accuracy, but not so complex as to risk not being able to find a good fit when training.\nIdeally we\u0026rsquo;d like a simple mathematical function. For this change, we will change all examples over to using a data set that is generated that has only 3 features and a target value.\nThe function we will be training to fit will be\n$$ f(x, y, z) = (x \u0026lt; y) \\\u0026amp;\\\u0026amp; (x \u0026gt; z) $$\nThat is, for a given observation, 3 floating point numbers will be generated within some decent range of values and the target output value is either a 0 or a 1.\nThe target will be a 1 only if the first number generated is strictly less than the second AND is strictly greater than the third.\nThe code for this can be found HERE, but will be posted here in case of link breakage\nuse { ndarray::{Array, Axis, Ix2}, rand::{ Rng, SeedableRng, rngs::StdRng } }; const TOTAL_OBSERVATIONS: usize = 100_000; const MIN_VALUE: f64 = -100_000.0; const MAX_VALUE: f64 = 100_000.0; /// Gets data for testing with the neural network examples. /// Data represents the following artiy-3 function we\u0026#39;re trying to train /// f(v1, v2, v3) = (v1 \u0026lt; v2) \u0026amp;\u0026amp; (v1 \u0026gt; v3) fn get_data( training_proportion: f64, seed: u64 ) -\u0026gt; Data { // Generate the entire array of samples (including the column for the targets too). \tlet mut random = StdRng::seed_from_u64(seed); let all_data = Array::from_shape_vec((TOTAL_OBSERVATIONS, 4), (0..TOTAL_OBSERVATIONS).flat_map(|_| { let v1 = random.gen_range(MIN_VALUE..=MAX_VALUE); let v2 = random.gen_range(MIN_VALUE..=MAX_VALUE); let v3 = random.gen_range(MIN_VALUE..=MAX_VALUE); let result = ((v1 \u0026lt; v2) \u0026amp;\u0026amp; (v1 \u0026gt; v3)) as u8 as f64; std::iter::once(v1).chain(std::iter::once(v2)).chain(std::iter::once(v3)).chain(std::iter::once(result)) }).collect::\u0026lt;Vec\u0026lt;_\u0026gt;\u0026gt;()).unwrap(); // Split off the targets from the input data. \tlet (input, targets) = all_data.view().split_at(Axis(1), 3); // determine the split point for the training/testing split. \tlet split_index = (input.nrows() as f64 * training_proportion) as usize; let (training_input, testing_input) = input.split_at(Axis(0), split_index); let (training_targets, testing_targets) = targets.split_at(Axis(0), split_index); // we have the data to return now. \tData { training_input: training_input.to_owned(), training_targets: training_targets.to_owned(), testing_input: testing_input.to_owned(), testing_targets: testing_targets.to_owned() } } struct Data { training_input: Array\u0026lt;f64, Ix2\u0026gt;, training_targets: Array\u0026lt;f64, Ix2\u0026gt;, testing_input: Array\u0026lt;f64, Ix2\u0026gt;, testing_targets: Array\u0026lt;f64, Ix2\u0026gt; } Essentially it iterates for TOTAL_OBSERVATIONS iterations, generating 3 random floats for each iteration, between MIN_VALUE and MAX_VALUE.\nIt then calculates the result, which as mentioned will either be a 0 or a 1.\nThis occurs inside of a flat_map call so it\u0026rsquo;s expecting an iterator as a return value, so we use a series of \u0026ldquo;chain\u0026rdquo; calls to chain the 4 numbers together into an iterator.\nThe whole thing is collected into a Vec, which can then be turned into an Array for us to use.\nThe rest of the code is just slicing the array into training and testing, and separating the input data from the targets.\nChanging the example output The examples were previously printing out a message to show the final error/loss value after training for both of the data sets.\nThe problem here is that this loss value kind of doesn\u0026rsquo;t tell us what scale we\u0026rsquo;re using, and it\u0026rsquo;s certainly possible to have a high error/loss value but still have an accurate trained network (as is the case when we look at the softmax cross entropy loss function).\nWhat we actually care about is the percentage of predictions that the network makes that match the targets correctly.\nTherefore we\u0026rsquo;ll change the examples to output as follows\n// Accuracy let training_predictions = network .forward_loss(data.training_input, data.training_targets.clone()) .0; let training_correct_count = training_predictions .iter() .zip(data.training_targets.iter()) .filter(|(prediction, target)| { let prediction = if **prediction \u0026lt; 0.5 { 0.0 } else { 1.0 }; prediction == **target }) .count(); let training_accuracy = (training_correct_count as f64 / data.training_targets.nrows() as f64) * 100.0; println!(\u0026#34;Accuracy (training): {training_accuracy}%\u0026#34;); let testing_predictions = network .forward_loss(data.testing_input, data.testing_targets.clone()) .0; let testing_correct_count = testing_predictions .iter() .zip(data.testing_targets.iter()) .filter(|(prediction, target)| { let prediction = if **prediction \u0026lt; 0.5 { 0.0 } else { 1.0 }; prediction == **target }) .count(); let testing_accuracy = (testing_correct_count as f64 / data.testing_targets.nrows() as f64) * 100.0; println!(\u0026#34;Accuracy (testing): {testing_accuracy}%\u0026#34;); For both training and testing data sets, we run them through the trained network and take the predictions array.\nThen we zip the predictions array with the target array (we know they both only have a single column/feature) and if the prediction is less than 0.5 consider it 0, else it\u0026rsquo;s a 1 if above 0.5.\nThen we just compare with the target value and filter those that match.\nThe count() function will give us the count of matching predictions and to work out the percentage we can divide it by the total number of targets and multiply by 100 to get the percentage to print out.\nConclusion With those two changes to the data set used by the examples, we have a nice simple but predictable function that we know should be trainable, and we can therefore see how well the various training strategies work.\nTanh activation function When we\u0026rsquo;re training a neural network, the biggest factor in how successful we would be is how much we can use the gradient to update the weights on the neurons in the layers during the backpropagation pass.\nHowever, we firstly are only updating the weights based on a proportion of the gradient (the learning rate), and secondly are limited by the possible gradients of the activation function we chose.\nWith the sigmoid activation function, the maximum gradient we can have is 0.25, however with the tanh function we\u0026rsquo;ll implement, we are able to get gradients of 1.\nThese steeper gradients allow us to train the network much faster, as is evident by the following graph comparing the derivatives/gradients of the sigmoid function we were using with the tanh function we want to use:\nThe tanh function is a non-linear function like sigmoid except it maps to a range of -1 to 1, rather than to a range of 0 to 1.\nThe additional advantage is that we can map to negative numbers and not just positive values. This is important as discovered since negative weights on neurons allow some neurons to \u0026ldquo;inhibit\u0026rdquo; the output and can subtract from the result.\nWith positive weights only, we can only create additive weights which results in a more narrow range of output values.\nThe graph of tanh is as follows, for visualisation\nThe formulas The formula for tanh is actually pretty simple, it\u0026rsquo;s related to the sigmoid function from before (also known as the logistic function) by being a moved and scaled version.\nThe formula for tanh is just as follows\n$$ tanh(x) = \\frac {e^x - e^{-x}} {e^x + e^{-x}} $$\nAs with the sigmoid function, the tanh function also has a very nice and easy derivative calculation, defined in terms of itself\n$$ tanh'(x) = 1 - tanh(x)^2 $$\nThe code Now we need to code this up to be able to use it in a network as an activation function. As with all of our other operations in the library, it is just an empty struct which will behave as a vtable for our function calls.\n#[derive(Clone)] pub struct Tanh; Nextly we need to be able to construct a boxed version for use in a layer. We will follow the convention set out by the API and call this function \u0026ldquo;new_boxed\u0026rdquo; and it will look as follows\nimpl Tanh { pub fn new_boxed() -\u0026gt; Box\u0026lt;Self\u0026gt; { Box::new(Self) } } Finally we will need to implement the BasicOperation trait. For this Operation, we need access to functions provided by the num_traits::Float trait, so we constrain it as such:\nimpl\u0026lt;T: Float\u0026gt; BasicOperation\u0026lt;T\u0026gt; for Tanh { // code } For the two required functions, these are easy enough to implement. For applying the Tanh function, we actually have this as a pre-made function on the Float trait.\nTherefore when we implement calculate_output, we are just mapping the input array into a new one, where tanh is called on each element\nfn calculate_output(\u0026amp;self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { input.mapv(|elem| elem.tanh()) } Calculating the gradient as mentioned before, is defined in terms of the square of the tan (subtracted from 1). We must remember that that only gives us our partial derivative, and we still need to multiply our partial gradient with the output gradient passed back from the next layer.\nTherefore the full code of this function looks thusly\nfn calculate_input_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { output_gradient * input.mapv(|elem| T::one() - elem.tanh() * elem.tanh()) } Weight initialization Our current weight initialization is initializing to random weights between 0 and 1, that is they are always positive weights and as mentioned before don\u0026rsquo;t have the ability to \u0026ldquo;inhibit\u0026rdquo; the result.\nIn order to get a decent range for generating our random weights at initialisation time, we can use a formula called Xavier weight initialisation.\nThe formula is as follows\n$$ \\pm \\frac {\\sqrt 6} {\\sqrt {n_i + n_{i+1}}} $$\nIn this formula, \\(n_i\\) is the number of input neurons (number of features/columns in the input data), also called fan-in, and \\(n_{i+1}\\) is the number of neurons output by this layer, also called fan-out\nFor coding this up, we simply need to go into the DenseLayerSetup layer which is responsible for initializing weights, and ensure that this formula is used to determine the minimum and maximum values for weight generation.\nThe relevant code can be found in dense_layer_setup.rs and looks as follows\nlet xavier_delta = T::from(6_u8).unwrap().sqrt() / (T::from(input.ncols()).unwrap() + T::from(num_neurons).unwrap()).sqrt(); // see Xavier initialization let weights = repeat_with(|| r.gen_range::\u0026lt;T, _\u0026gt;(-xavier_delta..=xavier_delta)) .take(weight_count) .collect(); Note that because we\u0026rsquo;re inside of a generic context, in order to get the square root of 6, we first need to turn 6 into a \u0026ldquo;T\u0026rdquo;. We can do this with the \u0026ldquo;from\u0026rdquo; function provided by the num_traits::Float trait.\nTo get the number of input neurons, we can just get the number of columns in the input data. The number of output neurons is provided to us in the function parameters.\nTo generate the weights, we can just use the gen_range method on the random number generator and specify +/- this calculated Xavier value.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/refactorings/","summary":"There are a couple of things that were required to be tweaked as we go on with development of the extensions to the network API. This post will briefly explain the changes that were required before we move onto the extensions themselves.\nChanging the examples Eidetic at this point has examples that are showing the performance of a linear regression vs a basic neural network, both using the \u0026ldquo;mean squared error\u0026rdquo; loss function.","title":"4.2 - Refactorings"},{"content":"Although we have managed to put together a working neural network library that allows us to build various topologies of networks and to train them to fit a non-specified mathematical function that is represented by the input data and targets, the initial implementation is only somewhat successful. We can do better in a few ways, we will explore 5 such extensions to the basic approach in the following notes.\nHigh level review We can imagine a neural network at a high level as a black box which consists of a bunch of weights internally (grouped into layers, but we can consider this as an implementation detail), taking a bunch of input data records and producing a bunch of predictions for those records. These predictions are then compared with the known targets during the training process to see how close they are (using the loss function).\nTherefore this high level intuition can be represented by the following diagram\nA more accurate representation The above diagram is good for seeing at a high level that neural networks are treated as black boxes to produce predictions and can be trained to produce better predictions, however what is really happening under the hood is that we\u0026rsquo;re tweaking the weights in such a way as to represent the complex mathematical function between the set of weights and the output loss that those weights produce (given a static set of inputs and outputs).\nThis graph between the weights and the loss can be shown as follows\nTraining With the above graph representation we can see that as the set of weights vary along the horizontal axis, the loss, or error, of the network can be greater or lower (actually the set of weights isn\u0026rsquo;t a single value to be plotted on an axis, but it helps intuition to think of it as a single dimension on which a single point is a set of values for all weights in the network).\nThe training process is then represented as starting from one point, or set of weights and using the learning rate, determining a new set of weights and continuing until we settle on the set of weights producing the lowest loss.\nWe can view this process as the following diagram\nIn the above diagram, the blue arrows show the process with a small learning rate, that is we move only slightly to the next set of weights. The red arrows represent a large learning rate which causes the set of weights and resulting loss to \u0026ldquo;bounce around\u0026rdquo; more than it might want to.\nThe problem As we can see from the diagram, we would like to find the set of weights that gives the lowest global loss across all sets of weights, however with a very small learning rate we risk the chance of falling into a local minima and accepting a sub-optimal solution.\nIf the steps we take are too large (the red arrows), then we might be near the global minimum but we might end up \u0026ldquo;skipping over\u0026rdquo; it repeatedly and again accepting a sub-optimal solution.\nThe tricks and tweaks described and implemented in the coming posts serve as to allow us to efficiently locate this global minimum without falling into a false, local minimum.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/overview/","summary":"Although we have managed to put together a working neural network library that allows us to build various topologies of networks and to train them to fit a non-specified mathematical function that is represented by the input data and targets, the initial implementation is only somewhat successful. We can do better in a few ways, we will explore 5 such extensions to the basic approach in the following notes.\nHigh level review We can imagine a neural network at a high level as a black box which consists of a bunch of weights internally (grouped into layers, but we can consider this as an implementation detail), taking a bunch of input data records and producing a bunch of predictions for those records.","title":"4.1 - Overview"},{"content":"This is the first in hopefully a long series of posts in Arduino development as I learn about embedded development, microcontrollers, and electrical engineering.\nThe hardware The target hardware for the experimentation will be this very Arduino Uno\nAnd, a quick look at the back will tell us that this is the R3 model\nHowever I believe any Arduino Uno model, as well as a few other Arduinos will work with the methods and APIs we\u0026rsquo;re using\nThe second component we will need is the cable to connect the Arduino to the computer, this is a male USB-A (plugs into the computer) to USB-B (plugs into Arduino) cable\nThis will be all that is needed to set up the environment and a \u0026ldquo;Hello, World!\u0026rdquo; application, but obviously as we go through different projects, etc. we\u0026rsquo;ll require more peripherals.\nThe coding environment We will be using Rust for all Arduino development which has the ability to compile to various embedded target platforms.\nMore generically, Rust has the ability to compile for any target platform for which LLVM supports, and a linker is available for.\nRust is a systems programming language rivalling C and due to the language itself being more expressive (such as expressing ownership), has a very good optimising compiler.\nThe Rust environment is easy to install, simply follow the directions provided on the Rustup website which will install Cargo and the Rust compiler.\nThe repository All of the code for this Arduino experimentation will be put in the Arduino Uno repository.\nThis repository is setup with Github workflows to check compilation and formatting on code submission, similar to my other repositories, except I\u0026rsquo;ve removed code coverage and testing workflows.\nDependencies There are a few dependencies that need to be set up on the development machine to support Arduino development. These are detailed below.\nAVR As mentioned previously, the Rust compiler is able to compile to any platform that LLVM supports, however requires an appropriate linker to be present on the system.\nIn the case of AVR development, we will need to make sure we install the tools for an AVR development environment.\nOn linux, these can be installed with the following command\nsudo apt install avr-libc gcc-avr pkg-config avrdude\rOn windows, the process is a little more involved, but not exactly difficult.\n Install the WinAVR package, which sets up an AVR development environment. The WinAVR package can be found HERE WinAVR is a little out of date, in particular the version of avrdude is too old for us to use with the library we\u0026rsquo;re going to be using. Download the latest version of avrdude from HERE and replace the avrdude.exe binary with it  Ravedude Ravedude is a program which handles flashing the compiled program onto the device. With Ravedude installed, running\ncargo run\rin a project that is setup to compile AVR executables will result in the executable being built, and then pushed to a connected device.\nTo set this up on either Windows or Linux (or any other OS), it can be installed through Cargo with the command\ncargo +stable install ravedude\rCreating the project The API we will be using is called avr-hal which stands for AVR Hardware Abstraction Layer.\nA HAL will handle mapping coding structures (such as an enum of pins) to the actual pins on a device. It\u0026rsquo;s intended that a HAL will abstract the hardware from the coder, leaving them to focus only on what they want it to do.\nThe author provides a neat template that can be used with cargo-generate to create the project.\nTo create a project with this method, simply install the cargo-generate program, conveniently through Cargo itself:\ncargo install cargo-generate\rSecondly invoke it as such:\ncargo generate --git https://github.com/Rahix/avr-hal-template.git\rIt will ask for the project name, and the target Arduino device (I chose Arduino Uno).\nRunning the project Thanks to the HAL, and Ravedude, it\u0026rsquo;s easy to build and run the generated project.\nFirstly, plug the Arduino device into the computer.\nSecondly, run the following command\ncargo run --release\rThis will compile the sample program, and push the executable to the Arduino, resulting in a lovely little flashing light as shown in the video below\nThere should have been a video here but your browser does not seem\rto support it.\r ","permalink":"https://forgottenmaster.github.io/posts/arduino/helloworld/","summary":"This is the first in hopefully a long series of posts in Arduino development as I learn about embedded development, microcontrollers, and electrical engineering.\nThe hardware The target hardware for the experimentation will be this very Arduino Uno\nAnd, a quick look at the back will tell us that this is the R3 model\nHowever I believe any Arduino Uno model, as well as a few other Arduinos will work with the methods and APIs we\u0026rsquo;re using","title":"Hello World"},{"content":"Now that we have a basic framework for creating neural networks, we will re-visit the code examples for Linear Regression and Neural Networks that were quickly implemented in chapter 2 and reimplement them as examples for the Eidetic library.\nDoing this will ensure that users of our API have some reference examples of how to create and train their networks, as well as ensuring that the API fulfils the required functionality from a users perspective.\nCreating a prelude Most Rust library crates will contain a module named by convention as the prelude. This is located at the root of the crate, and is intended to contain the most important items, re-exported from the library.\nThe usage of the prelude will look as follows\nuse eidetic::prelude::*; This common practice allows us to avoid each user having to import items individually or from multiple sub-modules for basic usage, although allows for optional items to be not included in the prelude and be imported individually still.\nCrates can have multiple preludes if needed, since they\u0026rsquo;re just regular modules that are named by convention, however for Eidetic we only require one.\nThe prelude for Eidetic can be located HERE\nData wrangling The first thing we\u0026rsquo;ll need to do is to provide common functionality for the examples that will do the following:\n Open the data file Read the data Parse features as floating point values for training Split the data into training and testing data Separate out the targets from the input data  The data wrangling code can be found HERE for anyone that\u0026rsquo;s interested but it\u0026rsquo;s just reading the credit score information found HERE and preparing it into a format that we can use for network training.\nUsing the data wrangler simply requires that the source file be included (with the include! macro) into the example source file.\nThe include! macro in Rust will dump the source contents into the file during compilation (so that the compiler sees it as if that data wrangling source code were written directly into the example source file).\ninclude!(\u0026#34;credit_score_data_wrangler.rs\u0026#34;); After including the data wrangling source code, we then just need to call the get_batches and get_targets functions which will each return a tuple containing firstly the training Array, and secondly the testing array.\nThese functions both take a float (should be between 0 and 1) indicating the proportion of the input data rows that are going to be in the training set (for example 0.8 indicates that 80% of the rows in the input data will make up the training data array, and 20% makes up the testing data array).\nThese are used for example as:\nlet (training_batch, testing_batch) = get_batches(TRAINING_BATCH_PROPORTION); let (training_targets, testing_targets) = get_targets(TRAINING_BATCH_PROPORTION); Linear regression example In order to perform a linear regression on the data, we can set up a network with a single dense layer and use a linear activation function to combine the results at the end into a single neuron. The loss function we use is the mean squared error as usual, and for the optimisation strategy we use the stochastic gradient descent as mentioned previously.\nThe code to setup the network then looks like the following:\nlet mut network = Network::new( vec![Layer::new( 1, DenseLayerSetup::new_boxed(Linear::new_boxed()), )], Loss::new(MeanSquaredError::new_boxed()), ); And to setup the SGD optimiser is simple too, we need to provide a learning rate value though:\nlet mut optimiser = SGD::new(LEARNING_RATE); We can train the network with the training function which we need to pass the network and optimiser into, along with training and testing input data and targets. The final few parameters we need to pass are indicating the number of epochs we are training for, how often we evaluate the training process for early termination, the size of the batches we feed through the network in each epoch, and finally, the random seed (if we\u0026rsquo;re using one - here it\u0026rsquo;s None) we\u0026rsquo;re using. After plugging all of this in, we get the following:\ntrain( \u0026amp;mut network, \u0026amp;mut optimiser, training_batch.clone(), training_targets.clone(), testing_batch.clone(), testing_targets.clone(), EPOCHS, EVAL_EVERY, BATCH_SIZE, SEED, ); After we finish training the network, we can print out the error/loss of the trained network on both the training and testing data to see how close they are to zero (0 is perfect training with no error), and more importantly how close they are to each other (to show the network generalises well and doesn\u0026rsquo;t overfit the training data too much).\nIn the Eidetic API we can use the forward_loss function which returns the predictions (that we don\u0026rsquo;t care about) and the error loss (that we do care about). We can thus just print out the item at index 1 of the tuple (the loss) as such:\nprintln!( \u0026#34;Final error (training): {}\u0026#34;, network.forward_loss(training_batch, training_targets).1 ); println!( \u0026#34;Final error (testing): {}\u0026#34;, network.forward_loss(testing_batch, testing_targets).1 ); When we run this, we see that the error is around what we had without the API when the algorithm was hacked together, and that they are very close to one another:\nFinal error (training): 0.20482454235971267\rFinal error (testing): 0.20557973252746742\rNeural network example Now that we have an example that uses Eidetic to train a network using linear regression, we literally only need to modify the setup of the network to change the linear regression into a deeper neural network.\nThe difference between a neural network and a linear regression is the addition of one or more dense \u0026ldquo;hidden\u0026rdquo; layers with multiple neurons and using the sigmoid nonlinear activation function. In the example program, we have the following setup (with 25 hidden neurons on one layer):\nlet mut network = Network::new( vec![ Layer::new(25, DenseLayerSetup::new_boxed(Sigmoid::new_boxed())), Layer::new(1, DenseLayerSetup::new_boxed(Linear::new_boxed())), ], Loss::new(MeanSquaredError::new_boxed()), ); When we run this, it takes a little longer, but we see similar values as with the linear regression:\nFinal error (training): 0.20514753984418146\rFinal error (testing): 0.20600618973999624\rAs mentioned in chapter 2 with the original implementation, the neural network approach is slightly worse than the linear regression approach, but both are around 0.2 indicating that the API seems to be working as intended.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/examples/","summary":"Now that we have a basic framework for creating neural networks, we will re-visit the code examples for Linear Regression and Neural Networks that were quickly implemented in chapter 2 and reimplement them as examples for the Eidetic library.\nDoing this will ensure that users of our API have some reference examples of how to create and train their networks, as well as ensuring that the API fulfils the required functionality from a users perspective.","title":"3.9 - Examples"},{"content":"The last part to the puzzle, the last thing we need to do is to be able to encapsulate the algorithm for training a given network. In the book, this is represented as the \u0026ldquo;Trainer\u0026rdquo; type which binds together a network with an Optimiser instance. However there is only one implementation for this training process, and customisation is instead via the various trait objects in the Network setup, along with the selected Optimiser.\nTherefore making it a type for no reason (a good reason would be to use it as a trait object for example) just results in more code. For now, training is just done via a single function which will take the Network and Optimiser to use.\nAlgorithm As a reminder of the process that we will go through when we train, here is an overview of the sequence of steps that we will run each epoch\n Clone the network if needed. We provide a tweakable value for the frequency to evaluate the network at (i.e. every X epochs). In the case that after evaluation the network is performing worse than before, then we terminate the training early and return to the previous network state. To restore the network state, we need to clone the network if it\u0026rsquo;s one of these evaluation epochs. Permute the data. When we feed the training data into the network, we feed it in a different order each epoch. Feeding data in the same batches each time can result in overfitting. permuting the samples each time helps the network to generalise. Generate batches. We don\u0026rsquo;t necessarily feed all the data into the network at once (we might do though, or might feed them one by one, or somewhere in between). We provide a parameter to the function to determine how large each batch of data is when we feed it through the network. For each batch\u0026hellip;  Tell the network to perform a forward, and then a backward pass to calculate/update the parameter gradients. Tell the optmiser to apply its optimisation algorithm to update the parameters in the network for next epoch   If this is an evaluation epoch, check the loss value of the network after training\u0026hellip;  If it\u0026rsquo;s worse then abort training and return the previous snapshot of the network If it\u0026rsquo;s better, continue training and update the best loss value encountered for next evaluation    In order to achieve these we will need the following three functions, which we will cover in the following sections:\n A function to permute the rows of the training data A function to batch the rows of the training data up A function to orchestrate the training algorithm  Permuting the data As mentioned in the overview, to avoid overfitting the network to a specific setup or order of data, we want to shuffle the rows of the training data set around. Note that we want to always keep each row with its corresponding entry in the training target array.\nThis means that as we\u0026rsquo;re shuffling, we need to apply the same ordering to both the training input data, and the training targets arrays.\nIn order to permute the data, we will essentially be shuffling the rows of the data in tandem with their target counterparts in the targets array. This means we must zip each row of the input data array with row in the targets array, to get a single list of tuples that we can then shuffle.\nAfter shuffling, we need to re-split the data again into arrays to pass back.\nNOTE: There\u0026rsquo;s definitely a better way of building the output/permuted arrays than what I\u0026rsquo;m doing here, and there\u0026rsquo;ll probably be an optimisation stage coming up before moving to the next chapter of the book which may also include refactoring the API to remove unnecessary traits, etc.\nSignature The signature of the permute_data method looks as follows:\nfn permute_data\u0026lt;T: Clone + Default\u0026gt;( batch: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, targets: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, seed: Option\u0026lt;u64\u0026gt;, ) -\u0026gt; (Array\u0026lt;T, Ix2\u0026gt;, Array\u0026lt;T, Ix2\u0026gt;) We require Cloning on the elements of the array, due to us using the append method to add rows to the arrays we\u0026rsquo;re building up, this requires the elements to be cloned into the storage location. We require Default due to us wanting to initialise the output arrays with a set number of columns. Despite initialising with 0 rows, the Array::default method requires the type T to be default constructible.\nFor the other parameters, we take the batch of data to permute, and the targets array. We also take an optional random seed which is used to determine the random permutation order for the rows. We need to be able to specify this seed for unit testing, but in practice this will be given as None for \u0026ldquo;true\u0026rdquo; randomness.\nThe return values from this function are of course the permuted batch and targets arrays. These are returned by value as they are brand new arrays produced using the inputs as a basis.\nShape checking The first step in the permutation is to cache the number of rows and columns in the batch and targets arrays (we expect the targets to only be a single column but it\u0026rsquo;s indeed possible to produce more). We do require that the number of rows in the batch and targets arrays is the same though:\nlet (batch_row_count, batch_col_count) = (batch.nrows(), batch.ncols()); let (targets_row_count, targets_col_count) = (targets.nrows(), targets.ncols()); assert_eq!(batch_row_count, targets_row_count); Zipping the arrays In order to shuffle the batches and targets together we will need to zip them into a single vector. First step is to get an iterator that produces one row at a time from the Array. We can then zip those iterators together to get an iterator that produces a tuple with each step. The tuple has a row from each of the two arrays in it.\nIn order to produce an iterator over the rows of the array we can firstly get a flat slice of the data in the batch. This slice will have a row every NUM_COLS elements. We can use the chunks method on slices to get an iterator over chunks of size NUM_COLS which gives us the rows of the array.\nThis all looks as follows once coded up:\nlet batch_rows = batch.as_slice().unwrap().chunks(batch_col_count); let targets_rows = targets.as_slice().unwrap().chunks(targets_col_count); let mut zipped_rows = batch_rows.zip(targets_rows).collect::\u0026lt;Vec\u0026lt;_\u0026gt;\u0026gt;(); Creating the RNG Next we need the random number generator. This construction is basically the same as we have for initialising the weights in the network and constructs either from the given seed if there is one, or constructs a random generator:\nlet mut random_generator = match seed { Some(seed) =\u0026gt; StdRng::seed_from_u64(seed), None =\u0026gt; StdRng::from_rng(thread_rng()).unwrap(), }; Shuffling the data Shuffling a vector with a random generator is an extension method provided to us by the rand crate. We can simply call it and pass a mutable reference to the generator. The reference needs to be mutable because the generator internally contains state it has to update upon generating a random number, in order to affect the next one produced.\nThis is a single line:\nzipped_rows.shuffle(\u0026amp;mut random_generator); Building the permuted arrays In order to build the permuted arrays for returning, we will start with empty arrays, and then upon traversing the zipped vector, will add each row to the end of the appropriate array. This is analagous to building a Vecby iterating and pushing elements on one at a time. This is likely one of the things that isn\u0026rsquo;t very efficient, but as a starting point it works.\nThere are a few ways that we can construct the initial Array instances, however we want to make sure that we are constructing two dimensional arrays, and that the number of columns we\u0026rsquo;re specifying is correct. The choice I picked for initialising the arrays is using the default constructor. This takes a shape description. Initially we\u0026rsquo;ll fix the number of columns, but will have 0 rows that will be extended as we append.\nThis default code looks as follows:\nlet mut batch = Array::default((0, batch_col_count)); let mut targets = Array::default((0, targets_col_count)); For the appending of the rows, we iterate over the entries in the zipped array. Remember that each entry in the zipped array is a tuple, where the first element is a row from the batch array, and the second is the corresponding row from the targets array.\nThe structure of the iteration then is as follows:\nzipped_rows .into_iter() .for_each(|(batch_row, targets_row)| { // append code follows  }); Only one thing remains, which is to actually append batch_row to the batch array we\u0026rsquo;re building, and targets_row to the targets array we\u0026rsquo;re building. As mentioned before, we use the append method on the array. This method takes two parameters:\n An axis along which we\u0026rsquo;re appending the new data - in our case this will be the 0th axis (rows) as we\u0026rsquo;re adding each row as a row in the Array. An array or array view to append - in this case, we just shape the row data into an ArrayView with 1 row, and the requisite number of columns.  The code for this when updating the batch looks like follows:\nbatch .append( Axis(0), ArrayView::from_shape((1, batch_col_count), batch_row).unwrap(), ) .unwrap(); And the targets follows the same format:\ntargets .append( Axis(0), ArrayView::from_shape((1, targets_col_count), targets_row).unwrap(), ) .unwrap(); Generating the batches In comparison to the function to permute the arrays, generating the batches to feed into the network for training is shorter. Essentially what we would like is to get an iterator over two arrays (the batch and targets arrays).\nEach element in this iterator will be a pair of new arrays. One array will be a batch of rows from the \u0026ldquo;batches\u0026rdquo; input array, and the next will be a batch of rows from the \u0026ldquo;targets\u0026rdquo; input array.\nSince we are yielding new arrays from the iterator, we need elements to be cloneable, which is why we require the element type in the arrays to be cloneable. Since we don\u0026rsquo;t require ownership of the input arrays and are simply producing new arrays from them, we take references to them. However, since we\u0026rsquo;re yielding an iterator over those arrays, we must make sure the lifetime of the produced iterator is tied to them, so we can\u0026rsquo;t drop them before being finished with the iterator.\nThis results in the following signature:\nfn generate_batches\u0026lt;\u0026#39;a, T: Clone\u0026gt;( batch: \u0026amp;\u0026#39;a Array\u0026lt;T, Ix2\u0026gt;, targets: \u0026amp;\u0026#39;a Array\u0026lt;T, Ix2\u0026gt;, size: usize, ) -\u0026gt; impl Iterator\u0026lt;Item = (Array\u0026lt;T, Ix2\u0026gt;, Array\u0026lt;T, Ix2\u0026gt;)\u0026gt; + \u0026#39;a Firstly, we need to make sure that both input arrays have the same number of rows. Each yielded tuple is a row from each, so they must match:\nassert_eq!(batch.nrows(), targets.nrows()); In order to produce the desired iterator we need to do the following:\n Get an iterator over chunks of size rows from the batch array Get an iterator over chunks of size rows from the targets array Zip the two together to get an iterator over chunks from both arrays Map this tuple of array views into a tuple of owned arrays  For steps 1 and 2 we can use the built in method on arrays axis_chunks_iter which takes an axis along which to iterate (again we will iterate over the 0th axis - the rows), and the size we would like each cunk to be.\nFor step 3 we use zip to zip the iterators together.\nFor step 4 we can use the map adapter, and use the to_owned method to turn the ArrayView chunks into owned Array instances.\nThe full code for this will look as follows:\nbatch .axis_chunks_iter(Axis(0), size) .zip(targets.axis_chunks_iter(Axis(0), size)) .map(|(view1, view2)| (view1.to_owned(), view2.to_owned())) The training function Finally we can implement the main attraction, the function which will take the network to train, an optimisation strategy, and the training+testing data, along with other parameters and actually run the training process on the network.\nThe signature for it isn\u0026rsquo;t exactly pretty:\npub fn train\u0026lt;T: Clone + Default + PartialOrd\u0026gt;( network: \u0026amp;mut Network\u0026lt;T\u0026gt;, optimiser: \u0026amp;mut impl Optimiser\u0026lt;T\u0026gt;, batch_train: Array\u0026lt;T, Ix2\u0026gt;, targets_train: Array\u0026lt;T, Ix2\u0026gt;, batch_test: Array\u0026lt;T, Ix2\u0026gt;, targets_test: Array\u0026lt;T, Ix2\u0026gt;, epochs: u32, eval_every: u32, batch_size: usize, seed: Option\u0026lt;u64\u0026gt;, ) Firstly an explanation of the traits in use:\n Clone: We need to clone the Networkin order to create a snapshot/undo point for early termination. We can only do this if T itself is Clone Default: This is required because as explained before, permute_data requires T to be Default PartialOrd: We need to evaluate the calculated loss from the network against the previous best loss for early termination. In order to do this, we need the loss value (of type T) to support comparison  Next, we\u0026rsquo;ll explain the parameters of the train function (although they\u0026rsquo;ll be fairly self explanatory):\n network: The neural network to train, taken my mutable reference as we need to update the parameters within optimiser: The optimisation strategy to use, also taken by mutable reference, in case the optimiser stores state between epochs batch_train: The input data from the training batch targets_train: The output data/targets for the training batch batch_test: The input data from the testing batch targets_test: The output data/targets for the testing batch epochs: The maximum number of iterations of training that we will go through eval_every: The frequency at which we evaluate the loss, and revert to the previous state/abort if it regresses batch_size: The size of each batch we feed through the network during a given epoch seed: An optional seed for the RNG when training. During testing this will be provided, during normal use it should be None  Note that epochs is the maximum number of epochs we go through, since we\u0026rsquo;re implementing early stopping if the loss regresses and starts to diverge from the desired loss.\nWe make a variable to track the \u0026ldquo;best\u0026rdquo; loss value during evaluation, which needs to be mutable as we upate it as we train. The actual iteration over the epochs is as expected:\nlet mut best_loss = None; for e in 0..epochs { // training code here } Note that we do need to use a traditional for loop, rather than a for_each call on the iterator because we can\u0026rsquo;t early abort the for_each style where we can break out of a traditional loop.\nThe first step in the epoch is to store off a snapshot of the Network if necessary. We don\u0026rsquo;t need to clone the network every epoch, just at the frequency we are evaluating. We therefore use an Option\u0026lt;Network\u0026gt; to store the snapshot:\nlet last_model = if (e + 1) % eval_every == 0 { Some((*network).clone()) } else { None }; Secondly comes the actual training for the epoch. The sequence of steps for this will be:\n Permute the training data Generate batches from the training data For each batch, train the network, and step the optimiser  In code this looks like the following:\nlet (batch_train, targets_train) = permute_data(\u0026amp;batch_train, \u0026amp;targets_train, seed.clone()); generate_batches(\u0026amp;batch_train, \u0026amp;targets_train, batch_size).for_each(|(batch, targets)| { network.train(batch, targets); optimiser.step(network); }); Finally, after training the network for one epoch, if we need to evaluate the loss, we do that. We can use the forward_loss method on the testing data in order to get the predictions (which we won\u0026rsquo;t use), and the loss value (which we will). If the best loss so far is None, or if this one is better, then we update the best loss value and continue. In the case that the loss is now worse than we had, we revert to the snapshot and break the training process.\nCode for all of this is shown below:\nif let Some(last_model) = last_model { let (_, test_loss) = network.forward_loss(batch_test.clone(), targets_test.clone()); if best_loss.is_none() || test_loss \u0026lt; *best_loss.as_ref().unwrap() { best_loss = Some(test_loss); } else { *network = last_model; break; } } ","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/training/","summary":"The last part to the puzzle, the last thing we need to do is to be able to encapsulate the algorithm for training a given network. In the book, this is represented as the \u0026ldquo;Trainer\u0026rdquo; type which binds together a network with an Optimiser instance. However there is only one implementation for this training process, and customisation is instead via the various trait objects in the Network setup, along with the selected Optimiser.","title":"3.8 - Training"},{"content":"As a small detour before we talk about the final structures required for a basic and usable deep learning API, we will talk about making the Networktype cloneable. As we will find out when we talk about the trainer, we need to be able to take a snapshot of the neural network for us to restore to at a later date. This requires us to be able to call the clone() method to obtain a duplicate. However as we\u0026rsquo;ll soon see, this is problematic and requires a clever trick.\nThe Problem Okay, so we want to add the ability to clone a Networkfor taking a snapshot when training. This seems simple enough and, at least for the Network itself we can use the derive macro in order to automatically implement Clone in the case that all members implement Clone. The definition is simple enough:\n#[derive(Clone)] pub struct Network\u0026lt;T\u0026gt; { layers: Vec\u0026lt;Layer\u0026lt;T\u0026gt;\u0026gt;, loss: Loss\u0026lt;T\u0026gt;, } Okay\u0026hellip;\nAs mentioned, Network will be clonable if all its component parts are clonable. In this case we have a Vec\u0026lt;Layer\u0026gt; and a Lossthat are required to be clonable. Vec\u0026lt;Layer\u0026gt; will be clonable if Layeritself is clonable.\nThis means that we need to make sure that both Layerand Lossimplement Clone (when T itself implements Clone of course).\nLet\u0026rsquo;s take a look at making Lossclonable as it\u0026rsquo;s simpler. Firstly we\u0026rsquo;ll hop over to the definition of Loss and add the derive attribute there too:\n#[derive(Clone)] pub struct Loss\u0026lt;T\u0026gt; { implementation: Box\u0026lt;dyn LossImpl\u0026lt;T\u0026gt;\u0026gt;, prediction: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, target: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, } Now we take a look again at the members.\n prediction and target are Options which will implement Clone if the underlying type implements Clone. In this case the underlying type is an Array which will implement Clone in the case that T does. These two members are fine and not an issue. implementation is a boxed trait object. Boximplements Clone if T implements clone. T in this case is \u0026ldquo;dyn LossImpl\u0026rdquo;. However, this is where we have an issue. trait objects don\u0026rsquo;t implement Clone.  This is one such location, however there are a few locations where we have a boxed trait object that we would like to clone. The complete list of such types in the library is:\n Loss LayerSetup BasicOperation ParameterOperation  All of these types are held in boxes as trait objects, and all need to support the ability to be cloned.\nWhy doesn\u0026rsquo;t clone work? Let\u0026rsquo;s take a little look at why trait objects can\u0026rsquo;t implement Clone. The reason is pretty simple really. Let\u0026rsquo;s first look at the definition of the clone method as required by the Clone trait:\nfn clone(\u0026amp;self) -\u0026gt; Self; That is, it is called on an immutable reference to an instance, and must return a new instance by value of the same concrete type (Self). However, in the case of a trait object which is dynamically sized - the compiler cannot determine the concrete size at compile time of, for example dyn LossImpl.\nThe compiler however, needs to be able to calculate how many bytes are required in order to allocate the correct memory on the stack for the clone.\nTherefore we hit one of the rules of trait objects in Rust:\nTraits that have methods returning \u0026ldquo;Self\u0026rdquo; cannot be made into trait objects\nWorkaround So, how can we make sure that we are able to clone our trait objects?.\nWell, we don\u0026rsquo;t need to be able to implement Clone for the trait objects themselves (e.g. dyn LossImpl), but instead we should realise that we only ever store them inside a Box. This means that what we actually need to do is to implement Clone for Box\u0026lt;dyn LossImpl\u0026gt; (or Box\u0026lt;dyn LayerSetup\u0026gt;, etc.)\nSo how can we do this?, well there\u0026rsquo;s nothing actually stopping us from implementing Clone on the boxed trait object. A Boxis always a known size, even when T is dynamically sized because a Box is just a pointer. Therefore we start to write this out:\nimpl\u0026lt;T\u0026gt; Clone for Box\u0026lt;dyn LossImpl\u0026lt;T\u0026gt;\u0026gt; { fn clone(\u0026amp;self) -\u0026gt; Self { Box::new(???) } } Of course, we run into an issue here, since we know how to create a new Box with Box::new. However, we need to provide a new and concrete instance of the underlying type to the constructor. However our underlying type is \u0026ldquo;dyn LossImpl\u0026rdquo; so we run into the same issue we had before.\nHow can we solve this? Well, we can use another trait to get a newly constructed Box with a newly cloned object inside and use that as such:\nimpl\u0026lt;T\u0026gt; Clone for Box\u0026lt;dyn LossImpl\u0026lt;T\u0026gt;\u0026gt; { fn clone(\u0026amp;self) -\u0026gt; Self { self.clone_box() } } So, what does this trait look like?. It\u0026rsquo;s simple enough and almost the same as Clone, except it returns the Box\u0026lt;dyn LossImpl\u0026gt; directly. This allows us to do any cloning internally where it would know about concrete sizes, etc. The trait looks as follows:\npub trait LossImplClone\u0026lt;T\u0026gt; { fn clone_box(\u0026amp;self) -\u0026gt; Box\u0026lt;dyn LossImpl\u0026lt;T\u0026gt;\u0026gt;; } Slight problem The eagle-eyed will notice that this trait is LossImplCloneand not LossImpl, however in the implementation for Box\u0026lt;dyn LossImpl\u0026gt; we are calling clone_box directly on the Box. This will pass through to calling it on the dyn LossImpltrait object internally.\nHow can we possibly call a method from our new trait when we have an instance of a different trait?, well, we need to make LossImpla subtrait of LossImplClone:\npub trait LossImpl\u0026lt;T\u0026gt;: LossImplClone\u0026lt;T\u0026gt; { ... } The compiler will then know that, if we have a trait object of type LossImplthen it will also be implementing LossImplClonewhich means that we can call functions from that trait when we have an instance of the subtrait.\nImplementing LossImplClone The only thing remaining is to actually implement the new LossImplClonewhich is required by all LossImplimplementors.\nRather than requiring every implementation of LossImplto manually implement this trait which is very boilerplatey, we can instead use Rust\u0026rsquo;s ability to do a blanket implementation and implement this trait for all valid types as such:\nimpl\u0026lt;T, U\u0026gt; LossImplClone\u0026lt;T\u0026gt; for U where U: \u0026#39;static + LossImpl\u0026lt;T\u0026gt; + Clone, { fn clone_box(\u0026amp;self) -\u0026gt; Box\u0026lt;dyn LossImpl\u0026lt;T\u0026gt;\u0026gt; { Box::new(self.clone()) } } For all concrete types, U, such that they already implement LossImpland Clone themselves, and have \u0026lsquo;static lifetime we can provide the implementation of LossImplClonefor them. This implementation just constructs a new box and clones self to put inside it.\nThe bound on LossImpland Clone is fairly self-explanatory however a brief comment could help to understand why the \u0026lsquo;static bound is required for the type we\u0026rsquo;re implementing this clone trait for.\nThe reason is because we are able to implement traits on any type in Rust, and references are first-class citizens in the type system. Therefore, we could if we wanted, implement LossImplClonefor a reference. These references obviously have a lifetime that\u0026rsquo;s tied to the stack generally.\nThis means that Rust won\u0026rsquo;t let us create a Box with that data. If it did, we could create a Box with a short-lived reference in it. Since we have no lifetime information in the trait, we can\u0026rsquo;t guarantee that the type U lives for at least as long as the returned Box will live.\nTherefore Rust won\u0026rsquo;t compile this program without the \u0026lsquo;static bound.\nThe \u0026lsquo;static bound ensures that we\u0026rsquo;re only implementing this for concrete types, and not references, and allows us to actually put the data inside a Box as we require.\nAll together And that\u0026rsquo;s the workaround, but it would help to look at it all in one place:\npub trait LossImplClone\u0026lt;T\u0026gt; { fn clone_box(\u0026amp;self) -\u0026gt; Box\u0026lt;dyn LossImpl\u0026lt;T\u0026gt;\u0026gt;; } impl\u0026lt;T, U\u0026gt; LossImplClone\u0026lt;T\u0026gt; for U where U: \u0026#39;static + LossImpl\u0026lt;T\u0026gt; + Clone, { fn clone_box(\u0026amp;self) -\u0026gt; Box\u0026lt;dyn LossImpl\u0026lt;T\u0026gt;\u0026gt; { Box::new(self.clone()) } } impl\u0026lt;T\u0026gt; Clone for Box\u0026lt;dyn LossImpl\u0026lt;T\u0026gt;\u0026gt; { fn clone(\u0026amp;self) -\u0026gt; Self { self.clone_box() } } Conclusion We were able to make the various boxed trait objects we\u0026rsquo;re using implement Clone through a little workaround using the trait system, and ultimately was able to make Networkitself implement Clone. We\u0026rsquo;ll see in the next post how we make use of that to take a \u0026ldquo;snapshot\u0026rdquo; of the Network for rolling back/early termination of the training process.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/networkcloning/","summary":"As a small detour before we talk about the final structures required for a basic and usable deep learning API, we will talk about making the Networktype cloneable. As we will find out when we talk about the trainer, we need to be able to take a snapshot of the neural network for us to restore to at a later date. This requires us to be able to call the clone() method to obtain a duplicate.","title":"3.7 - Network Cloning"},{"content":"Now that the layers of abstraction are done, we need to work on actually running the training of the network. We will be encapsulating the optimisation process to allow for the API user to select the strategy they would like to use to apply the gradients to parameters when training the network. This will be a fairly short post as the actual optimisation process is pretty simple.\nThe Trait An Optimiser has a single job in the network training process. To apply the calculated partial derivatives for the parameter operations in the network to the parameters themselves. The basic form as we\u0026rsquo;ll see shortly is just applying some proportion of the gradients to the parameters.\nHowever, it\u0026rsquo;s entirely possible the API user may wish to update them in a different way, such as updating using a historical average to smooth out the modifications over time.\nAt the end of the day though, the Optimiser trait only needs a single method, which is called to update the provided Network. The trait definition is as follows:\npub trait Optimiser\u0026lt;T\u0026gt; { fn step(\u0026amp;mut self, net: \u0026amp;mut Network\u0026lt;T\u0026gt;); } Again it needs a generic type, T, which is the type of the underlying elements in the arrays within the network.\nThe step function operates on a mutable reference to the Optimiser object itself, because it may want to for example store a history of gradients, or other information. Updating that information stored in the members of the implementor requires mutable access.\nIt takes the Network in by mutable reference also. We don\u0026rsquo;t want the implementor to own the Network, but merely borrow it for the duration of the step function. We need mutable access because we need access to and to update the parameters within the network.\nStochastic Gradient Descent This is the official terminology for the optimisation algorithm used previously. This simply runs through the parameters and gradients of the network and applies some proportion of the gradient to the parameter. This multiplier is called the learning rate.\nAs far as the structure of the SGD struct itself, we need to store the learning rate that is to be applied. This is an instance of the generic type T since we don\u0026rsquo;t know what concrete type is being used to store data within the network.\nThis definition then looks fairly trivial:\npub struct SGD\u0026lt;T\u0026gt; { learning_rate: T, } Secondly, we will need a way to construct a new instance of the struct. We will do this by adding a new_boxed method, since we need to store the optimiser as a trait object, thus it will need to be placed behind a box.\nThe implementation of new_boxed then shouldn\u0026rsquo;t be much of a surprise:\nimpl\u0026lt;T\u0026gt; SGD\u0026lt;T\u0026gt; { pub fn new_boxed(learning_rate: T) -\u0026gt; Box\u0026lt;Self\u0026gt; { Box::new(Self { learning_rate }) } } For the implementation of the Optimiser trait on SGD, it will look as follows:\nimpl\u0026lt;T: LinalgScalar + ScalarOperand\u0026gt; Optimiser\u0026lt;T\u0026gt; for SGD\u0026lt;T\u0026gt; { ... } These bounds are required due to the implementation of the function requiring arithmetic operations on the elements of the parameters and gradients arrays.\nAs for the implementation of the step function, it\u0026rsquo;s simply running over each parameter \u0026amp; gradient pair, and subtracting a small portion of the gradient (dictated by learning_rate) from the parameter:\nnet.parameters_and_gradients() .for_each(|(param, gradient)| { *param = \u0026amp;*param - (gradient * self.learning_rate); }); This runs the following sequence of operations:\n Calls parameters_and_gradients on the network which, as you may recall, returns an impl Iterator\u0026lt;Item=(\u0026amp;mut Array, \u0026amp;Array)\u0026gt; object. That is we don\u0026rsquo;t know what the concrete type is, but we do know that it implements the Iterator trait. We run code for each parameter and gradient pair. We use the for_each method here rather than a regular for loop in order to communicate the semantics that we are indeed updating every entry. We can multiply an Arrayby a single value T thanks to the ScalarOperand bound. We need to do a reborrow in this code since subtraction isn\u0026rsquo;t implemented for a left hand side of a mutable reference to an Array. We also can\u0026rsquo;t just dereference param since that would result in calling the implementation of the operator that takes an owned value on the left hand side. Therefore we need to reborrow the mutable reference as an immutable one and the compiler will find the function correctly.  ","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/optimiser/","summary":"Now that the layers of abstraction are done, we need to work on actually running the training of the network. We will be encapsulating the optimisation process to allow for the API user to select the strategy they would like to use to apply the gradients to parameters when training the network. This will be a fairly short post as the actual optimisation process is pretty simple.\nThe Trait An Optimiser has a single job in the network training process.","title":"3.6 - Optimiser"},{"content":"In this post we\u0026rsquo;ll take a look at implementing the next (and final) level of abstraction for a neural network, the Network type itself.\nJust as how we encapsulated multiple operations in a Layer type, the Network is nothing more than a wrapper around multiple layers. However this isn\u0026rsquo;t entirely true, since the Network will also require a loss function in order to calculate the final single value measure of error for the predictions. We will then use the loss function also on the backward pass.\nThe way we train the network is the same as how we train the layers/operations and will consist of the following steps for a certain number of epochs:\n Feed the data batch into each layer, taking the output of that layer and passing it to the next, until the output of the final layer is captured. Feed the final output into the loss function, along with the expected targets to get the loss value. Perform the backward pass on the loss function to get the output gradient to feed to the last layer, and continue passing the input gradient of a layer to the output gradient of the previous one.  Additionally, we\u0026rsquo;ll need a way to get an iterator over all the parameters and parameter gradients of the network for training purposes, but as we\u0026rsquo;ll see, this is nothing more than a wrapper over the layers as well.\nThe Definition The type definition itself is very straightforward:\npub struct Network\u0026lt;T\u0026gt; { layers: Vec\u0026lt;Layer\u0026lt;T\u0026gt;\u0026gt;, loss: Loss\u0026lt;T\u0026gt;, } T here is, as in the layers/operations, the underlying element type of the Array type we\u0026rsquo;re using to store the data.\nFor the Network type itself, it\u0026rsquo;s just a Vec of layers, and a loss function as mentioned above.\nThe \u0026ldquo;Constructor\u0026rdquo; For abstraction to allow for future private implementation changes, we will provide the user with a \u0026ldquo;new\u0026rdquo; method to create the Network as we do with the other types.\nThe type of T for creating a new Network doesn\u0026rsquo;t need to be constrained, so the implementation is straightforward:\nimpl\u0026lt;T\u0026gt; Network\u0026lt;T\u0026gt; { pub fn new(layers: Vec\u0026lt;Layer\u0026lt;T\u0026gt;\u0026gt;, loss: Loss\u0026lt;T\u0026gt;) -\u0026gt; Self { Self { layers, loss } } } The rest of the methods we will implement require T to be clonable (due to the layers/operations needing to store off copies of their input for use on the backpropagation), these will therefore be in an impl block that looks like:\nimpl\u0026lt;T: Clone\u0026gt; Network\u0026lt;T\u0026gt; { ... } Forward Function We want a public function that represents the forward pass of the network, but doesn\u0026rsquo;t require a secondary testing array, since once the network is trained, we would like to use it to get predictions for new data such as from a testing data set.\nThis function does need to take a mutable reference to the Network since the forward pass causes copies of the input data to be stashed off in the layers/operations of the network. The signature for this function is pretty straightforward, just taking the input data and returning a batch of predictions:\npub fn forward(\u0026amp;mut self, batch: Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { ... } As for the implementation, this is little more than running through each of the layers of the network, and calling its \u0026ldquo;forward\u0026rdquo; method to obtain output from that layer, then passing it through to the next and so on. In Rust with the iterator API we can easily model this as a fold expression:\nself.layers .iter_mut() .fold(batch, |state, layer| layer.forward(\u0026amp;state)) Forward Loss Function While we\u0026rsquo;re training the network, we will want to run the forward pass, but also obtain the loss value at the end. This will never be called externally, and only is part of training the network, so it doesn\u0026rsquo;t need to be public.\nFor the signature, we will take the input batch as before, but also because we\u0026rsquo;re calculating the loss value will need to accept the array of targets as well.\nThe return value will be a tuple containing the predictions, and loss value.\nSince the function is very simple, I\u0026rsquo;ll just paste it in its entirety here:\nfn forward_loss(\u0026amp;mut self, batch: Array\u0026lt;T, Ix2\u0026gt;, targets: Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; (Array\u0026lt;T, Ix2\u0026gt;, T) { let predictions = self.forward(batch); let loss = self.loss.forward(predictions.clone(), targets); (predictions, loss) } Backward Pass Now that we have the forward pass covered, we need to look at the backward pass. Again, this is a private helper function that\u0026rsquo;s only called as part of the training process and not independently. For the backward pass, we don\u0026rsquo;t actually need to take any arguments.\nWe do however still need mutable access to the Network since the backward pass will calculate and update parameter gradients as it goes through.\nAs with the forward pass, this is modeled with a fold expression, calling the \u0026ldquo;backward\u0026rdquo; function on each layer with the result of the previous one. The code looks as follows:\nfn backward(\u0026amp;mut self) { self.layers .iter_mut() .rev() .fold(self.loss.backward(), |gradient, layer| { layer.backward(\u0026amp;gradient) }); } A couple of points to talk about:\n We need to run through the layers in reverse order, because this is the backward pass. This is achieved by the rev() iterator adapter call. In order to get the initial gradient, we call backward on the loss function. For the rest, we pass it through the layer.  Training When we train the network, we are always performing a forward pass, followed by a backward pass. Therefore the public API for training the network doesn\u0026rsquo;t expose the backward, or forward loss functions since the caller should only use the train function.\nThis function will take the input data, and target array. It will run the forward pass, followed by a backward pass. Finally it will return the loss that was determined from the forward pass because the training code may use this.\nThe function itself is just a wrapper around the forward_loss and backward function calls and looks as follows:\npub fn train(\u0026amp;mut self, batch: Array\u0026lt;T, Ix2\u0026gt;, targets: Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; T { let (_, loss) = self.forward_loss(batch, targets); self.backward(); loss } Parameter \u0026amp; Gradient Access This is the last part that\u0026rsquo;s missing, which is the ability after training for an epoch, to obtain the parameters and parameter gradients from the network so that they can be optimized (e.g. via gradient descent).\nAgain, the function itself is very simple since all the complexity is now buried in the lower levels. Network is a wrapper, or composite over the layers and loss function, and so we\u0026rsquo;re just composing the parameters and gradients from the layers.\nWhat we want, is a parameters_and_gradients method that will return an iterator over each parameter and gradient in the first layer, then in the second, and so on.\nWe can achieve this in Rust by using the iterator adapter flat_map.\nflat_map takes each element of the iterator it\u0026rsquo;s adapting (in our case an iterator over the layers), and calls a closure on it. This closure returns an iterator, which Rust will chain together.\nFor our case, calling parameters_and_gradients on a Layer also returns an iterator, so it fulfils the contract for flat_map.\nThe full function then looks as follows:\npub fn parameters_and_gradients( \u0026amp;mut self, ) -\u0026gt; impl Iterator\u0026lt;Item = (\u0026amp;mut Array\u0026lt;T, Ix2\u0026gt;, \u0026amp;Array\u0026lt;T, Ix2\u0026gt;)\u0026gt; { self.layers .iter_mut() .flat_map(|layer| layer.parameters_and_gradients()) } Conclusion And that\u0026rsquo;s all there is to the Network abstraction, just a bunch of composing and encapsulation!.\nThe final part of the implementation is to create types responsible for running the training of the Network, and for optimizing the parameters given the calculated gradients.\nWe\u0026rsquo;ll look at implementing these in the next post.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/network/","summary":"In this post we\u0026rsquo;ll take a look at implementing the next (and final) level of abstraction for a neural network, the Network type itself.\nJust as how we encapsulated multiple operations in a Layer type, the Network is nothing more than a wrapper around multiple layers. However this isn\u0026rsquo;t entirely true, since the Network will also require a loss function in order to calculate the final single value measure of error for the predictions.","title":"3.5 - The Network Type"},{"content":"After trying to make a start on the next level of abstraction with the NeuralNetwork class, it\u0026rsquo;s becoming apparent that dealing with lifetimes at the lower levels of abstraction (Operation and Layer trait objects) is causing it more difficult to make a nice abstraction at the upper level. We will take a brief interlude to look at what the problem is, and why we need to do a refactor at this point.\nThe Problem The problem we run into with lifetimes is that we are required to keep the items for which we are borrowing around at least as long as the struct we\u0026rsquo;re instantiating and when we use the literal reference extension \u0026ldquo;feature\u0026rdquo; of Rust, it\u0026rsquo;s simply syntactic sugar.\nFor example, taking an example from one of the previous tests if we have a line such as:\nlet net = \u0026amp;mut [ Operation::new_parameter(\u0026amp;WeightMultiply, weights), Operation::new_parameter(\u0026amp;BiasAdd, bias), Operation::new_basic(\u0026amp;Sigmoid), Operation::new_basic(\u0026amp;Linear), ]; Rust compiler will see that this is using temporary literals, and everything is all in the same scope, it can see that net here is a mutable reference, which means that nothing else can modify it, and more importantly it knows that net can\u0026rsquo;t be moved due to the reference. The compiler is fine with this because it can desugar it by putting all these temporaries inside let bindings:\nlet _1 = WeightMultiply; let _2 = BiasAdd; let _3 = Sigmoid; let _4 = Linear; let mut _5 = [ Operation::new_parameter(\u0026amp;_1, weights), Operation::new_parameter(\u0026amp;_2, bias), Operation::new_basic(\u0026amp;_3), Operation::new_basic(\u0026amp;_4) ]; let net = \u0026amp;mut _5; And everything is happy here because the temporaries are literals and can be evaluated/verified to exist at compile time. It can be desugared into local variables, and doesn\u0026rsquo;t have to worry about the network moving without the data inside because it can\u0026rsquo;t be moved due to us only having access to a borrow.\nHowever if we take a look at the following snippet, which is a very stripped down version what we would like to have for the next abstraction (the neural network):\nstruct Network\u0026lt;\u0026#39;a, T\u0026gt; { layers: Vec\u0026lt;Layer\u0026lt;\u0026#39;a, T\u0026gt;\u0026gt; } #[test] fn test_throwaway() { let n: Network\u0026lt;\u0026#39;_, f64\u0026gt; = Network { layers: vec![Layer::new(1, \u0026amp;DenseLayerSetup::new(\u0026amp;Sigmoid))] }; println!(\u0026#34;{:?}\u0026#34;, n.layers.len()); } As we can see, we simply would like a Vec of Layers in our Network. However the important thing here is the following:\n\u0026amp;DenseLayerSetup::new(\u0026amp;Sigmoid) As mentioned in the previous test, Sigmoid is a literal and so the Rust compiler will extend it with the reference. However, in our case we aren\u0026rsquo;t creating a DenseLayerSetup literal. To do so, we would need access, and specify all the fields inside a DenseLayerSetup, but we don\u0026rsquo;t want to do this as it\u0026rsquo;s exposing implementation details and can cause code to break if the structure of the\u0026hellip;..structure changes.\nInstead, we use the \u0026ldquo;new\u0026rdquo; associated method to create a new instance. The problem here then, is this is no longer a literal, it returns an instance of DenseLayerSetup created at runtime and not (as in the case of the above test with literal extension) at compile time.\nSince it\u0026rsquo;s a runtime expression, Rust can no longer use its lifetime extension desugaring to store off the temporary.\nIf we try to compile this code, we get the following error message indicating that the temporary returned from the new function doesn\u0026rsquo;t live long enough:\nSolution #1 - Get the API user to store intermediate values The first of the two solutions we can use to get around this is simply to not care, continue using lifetimes all the way up the abstraction tree, and instead of having a Vec of layer instances, we instead have a slice of layer instances. In this solution we would change the definition of the Network structure to be thus:\nstruct Network\u0026lt;\u0026#39;a, T\u0026gt; { layers: \u0026amp;\u0026#39;a mut [Layer\u0026lt;\u0026#39;a, T\u0026gt;] } This will work, for sure, but it requires that the caller ensure that the storage for the slice is as long as the Network itself. This leads to code that isn\u0026rsquo;t as succinct, such as:\nlet dense_layer_setup = DenseLayerSetup::new(\u0026amp;Sigmoid); let mut layers = [Layer::new(1, \u0026amp;dense_layer_setup)]; let n: Network\u0026lt;\u0026#39;_, f64\u0026gt; = Network { layers: \u0026amp;mut layers }; Which has a couple of issues:\n It\u0026rsquo;s not exactly nice to write. We would like to be able to write the whole network construct as a single expression. We can\u0026rsquo;t move the network around. Since it doesn\u0026rsquo;t own the data, it\u0026rsquo;s kind of pinned in place by virtue of the fact that the data it\u0026rsquo;s borrowing is not movable (can\u0026rsquo;t be moved while borrowed).  Note that neither is necessarily a problem, and in environments where we have no heap allocation it might even be necessarily to give stack control to the caller. However, we are trying to write a nice, usable API and as such, these limitations are not really acceptable for us.\nSolution #2 - Use boxing instead of borrows The other option is for the structures to own their component parts instead of borrowing them. This will also remove the requirement to have an explicit lifetime annotation since owning the data means it is guaranteed to live as long as the object itself.\nIn order to do this with Rust, we simply use the Box type. We can use the Box::new constructor to get a boxed/owned trait object rather than taking a reference to it. We can also use Vec rather than slice to get an owned collection of things. Thus, instead of doing the following:\nlet dense_layer_setup = DenseLayerSetup::new(\u0026amp;Sigmoid); let mut layers = [Layer::new(1, \u0026amp;dense_layer_setup)]; let n: Network\u0026lt;\u0026#39;_, f64\u0026gt; = Network { layers: \u0026amp;mut layers }; We would simply like to be able to do:\nlet n: Network\u0026lt;f64\u0026gt; = Network { layers: vec![Layer::new( 1, Box::new(DenseLayerSetup::new(Box::new(Sigmoid))), )], }; Replacing the references with boxed and owned instances. However, we can do one better by having the new function return the new instance already boxed. However, to make it explicit, we\u0026rsquo;ll call it new_boxed as it\u0026rsquo;s good practice for a function named \u0026ldquo;new\u0026rdquo; to return an instance of the exact type it\u0026rsquo;s called on. The proposed API would then be something like:\nlet n: Network\u0026lt;f64\u0026gt; = Network { layers: vec![Layer::new( 1, DenseLayerSetup::new_boxed(Sigmoid::new_boxed()), )], }; We could go one step further and have the DenseLayerSetup internally construct the Sigmoid activation function trait object here for storage, and rely on generics. This would result in code looking like the following:\nlet n: Network\u0026lt;f64\u0026gt; = Network { layers: vec![Layer::new( 1, DenseLayerSetup::new_boxed::\u0026lt;Sigmoid\u0026gt;(), )], }; However there are a couple of issues with this:\n It doesn\u0026rsquo;t support operations/activation functions with additional setup. It relies entirely on the presence of the Default trait/method. It requires all types implementing BasicOperation to then implement Default, which might not be wanted (they might want to ensure that data is provided to construct an instance).  Therefore we won\u0026rsquo;t take this step, and stick with having it passed in.\nSolution #3 - Insanity There is a third solution which allows the network to be constructed with the nice syntax, and which can be moved around and such, however the code becomes much less readable, especially when looking at the types of layers/operations.\nThis solution is of course to use generics such that the type of the neural network overall statically encodes the concrete setup of the network.\nI will be ignoring this as a viable solution, at least until finishing going through the book with a more traditional runtime solution, however I may revisit it in the future as a fun exercise.\nSolution Choice All things considered the best choice for this API for the time being is solution number 2. We are assuming we\u0026rsquo;re running on hardware that has an allocator setup in some fashion, and we\u0026rsquo;re assuming that the allocations will be very small.\nAdditionally, we only allocate once up front when constructing the neural network and after that don\u0026rsquo;t need any further allocations. This solution also allows for easy API usage, especially if we add the new_boxed methods to remove the visual clutter of Box::new calls.\nFollowing is a brief overview of the changes that need to be made.\nOperationData/Operation We change the OperationData structure so as to not accept a reference trait object but a boxed one. These BasicOperation and ParameterOperation instances are usually (if not always) going to be zero sized types, since they don\u0026rsquo;t store any state themselves and only exist to act as a vtable for the actual calculations of the operation.\nIn Rust, boxing a ZST is guaranteed to not cause an allocation, therefore we introduce no allocations at this level (in general, but someone could still theoretically provide a BasicOperation or ParameterOperation that does have data).\nWe are able to remove the lifetime annotations since the trait object will now be owned rather than borrowed. Operation data will then change from:\nenum OperationData\u0026lt;\u0026#39;a, T\u0026gt; { Basic(\u0026amp;\u0026#39;a dyn BasicOperation\u0026lt;T\u0026gt;), Parameter { operation: \u0026amp;\u0026#39;a dyn ParameterOperation\u0026lt;T\u0026gt;, parameter: Array\u0026lt;T, Ix2\u0026gt;, parameter_gradient: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, }, } To:\nenum OperationData\u0026lt;T\u0026gt; { Basic(Box\u0026lt;dyn BasicOperation\u0026lt;T\u0026gt;\u0026gt;), Parameter { operation: Box\u0026lt;dyn ParameterOperation\u0026lt;T\u0026gt;\u0026gt;, parameter: Array\u0026lt;T, Ix2\u0026gt;, parameter_gradient: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, }, } Which then also lets us remove the explicit lifetime annotation from Operation as well, since it only existed to relay that information into the OperationData type being used. Therefore the new Operation will become:\npub struct Operation\u0026lt;T\u0026gt; { input: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, input_gradient: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, output: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, data: OperationData\u0026lt;T\u0026gt;, } We also replace the references with boxed trait objects in the new_basic and new_parameter functions, but we don\u0026rsquo;t need to show this here.\nLayerSetup The LayerSetup trait previously had an explicit lifetime parameter so as to be able to return a Vec of Operation with the correct lifetime. However we wish to remove this lifetime annotation which we now can do since Operation no longer takes one.\nThe new definition of the LayerSetup trait then becomes:\npub trait LayerSetup\u0026lt;T\u0026gt; { fn layer_setup( \u0026amp;self, num_neurons: usize, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, seed: \u0026amp;Option\u0026lt;u64\u0026gt;, ) -\u0026gt; Vec\u0026lt;Operation\u0026lt;T\u0026gt;\u0026gt;; } DenseLayerSetup Previously this took a borrow to a BasicOperation to be used at the end of the layer (mimicking how OperationData::Basic had a borrow to one). As part of this lifetime annotation removal, we wish to hold a boxed instance (which doesn\u0026rsquo;t cause an allocation generally if BasicOperation is a ZST).\nThe new definition of DenseLayerSetup then is simply:\npub struct DenseLayerSetup\u0026lt;T\u0026gt;(Box\u0026lt;dyn BasicOperation\u0026lt;T\u0026gt;\u0026gt;); Layer Previously the Layer struct held a borrow to the LayerSetup trait object like in the other cases, but we will replace this with a boxed trait object and can then remove the lifetime annotation as it\u0026rsquo;s no longer needed.\nThe new definition of Layer then becomes:\npub struct Layer\u0026lt;T\u0026gt; { num_neurons: usize, operations: Option\u0026lt;Vec\u0026lt;Operation\u0026lt;T\u0026gt;\u0026gt;\u0026gt;, setup: Box\u0026lt;dyn LayerSetup\u0026lt;T\u0026gt;\u0026gt;, output: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, seed: Option\u0026lt;u64\u0026gt;, } Loss Finally, the Loss struct is also storing its vtable/trait object by borrow. We can do the same kind of replacement as elsewhere there too and end up with:\npub struct Loss\u0026lt;T\u0026gt; { implementation: Box\u0026lt;dyn LossImpl\u0026lt;T\u0026gt;\u0026gt;, prediction: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, target: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, } Conclusion After changing these types, we then need to go through the compiler errors that we now get as a result of the change from reference to owned box. While we\u0026rsquo;re doing this, we will make it so that all the appropriate types have a \u0026ldquo;new_boxed\u0026rdquo; associated method for convenience.\nThis means that instead of writing Box::new(Foo::new()) people can then just write Foo::new_boxed() which is a little leaner.\nAfter these changes we can now use the API in a way we expect, for example the following code now compiles where it wouldn\u0026rsquo;t in the previous version:\nstruct Network\u0026lt;T\u0026gt; { layers: Vec\u0026lt;Layer\u0026lt;T\u0026gt;\u0026gt;, } #[test] fn test_throwaway() { let n: Network\u0026lt;f64\u0026gt; = Network { layers: vec![Layer::new( 1, DenseLayerSetup::new_boxed(Sigmoid::new_boxed()), )], }; println!(\u0026#34;{:?}\u0026#34;, n.layers.len()); } ","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/lifetimerefactor/","summary":"After trying to make a start on the next level of abstraction with the NeuralNetwork class, it\u0026rsquo;s becoming apparent that dealing with lifetimes at the lower levels of abstraction (Operation and Layer trait objects) is causing it more difficult to make a nice abstraction at the upper level. We will take a brief interlude to look at what the problem is, and why we need to do a refactor at this point.","title":"3.4 - Lifetime Refactor"},{"content":"Overview Once we\u0026rsquo;ve passed the data through the hidden layers of our neural network, and passed it through a final output layer (which should have only 1 neuron to bring the dimensionality of the data back to 1 feature/column) we will end up with a vector of predictions as mentioned previously (1 for each observation in the data). We already have a vector of targets which are the actual measured answers. We know that we will need to calculated derivatives on the backpropagation pass, but how do we know what to use as an initial value?\nThis is where the \u0026ldquo;loss function\u0026rdquo; (or error function) comes in, which will take the vector of predictions, and a vector of targets, and will produce a single number which identifies just how far away our predictions are from the target values.\nThis loss function will be implemented in a similar fashion to how we have done Operations and Layers so far, in that it will provide the framework and public API to call for use in the forward and backward passes, as well as asserting the shapes of arrays along the way.\nIt will delegate the actual calculations to another vtable like trait object though for customisation.\nLossImpl First of all we define a trait for us to use to wrap a trait object. This allows the customisation point for choosing exactly what kind of calculation we would like. This trait is parameterised over the type T (type of the Array elements in use) as the others have been, but otherwise looks fairly uneventful:\npub trait LossImpl\u0026lt;T\u0026gt; { ... } This trait will define two functions. The first is a function that will calculate a single value (of type T of course) when given the Array of predictions and targets. The signature looks as expected from such a function:\nfn calculate_output(\u0026amp;self, prediction: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, target: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; T; That function will be used on the forward pass in order to calculate the single value, total \u0026ldquo;error\u0026rdquo;. The other function we will need is a way of calculating what the gradients should be of the predictions (since these are what are varying and calculated) to kick off the backpropagation step. This function also takes the predictions and targets arrays, except that it will produce an Array of gradients, of the same shape as predictions:\nfn calculate_input_gradient( \u0026amp;self, prediction: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, target: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; MeanSquaredError This is an implementation of a loss function, and is one of the most commonly used ones, the average squared error. This is popular also because it\u0026rsquo;s easy to calculate the gradient for as we\u0026rsquo;ll see soon. Like the customisation points for Operation and Layer, this type doesn\u0026rsquo;t need to store any state, so is defined as a simple unit struct:\npub struct MeanSquaredError; Next we will need to implement the LossImpl trait for it so we can use it as a trait object. We need to place some generic bounds on this though for T, so I\u0026rsquo;ll show the impl block first, and then explain why we need the bounds:\nimpl\u0026lt;T: LinalgScalar + From\u0026lt;u32\u0026gt; + ScalarOperand\u0026gt; LossImpl\u0026lt;T\u0026gt; for MeanSquaredError { }  LinalgScalar - This trait is required for the same reason as we needed it in our Operation implementations. It allows for us to use arithmetic operations on Array\u0026rsquo;s of this type, which will be required to calculate the mean squared error. From - We need to be able to convert from numbers into our type T at a couple of points. One is when calculating the gradient, we need to be able to multiply the error by 2. However most types implement multiplication with both operands as the same type, therefore we need to be able to convert the integer 2 into an instance of type T. Additionally, we want to divide by the number of entries, which again should be an instance of type T. The downside here is that array.nrows() will give a usize, but we use Frominstead because floating point types implement it not conversion from usize. The program will crash if there are more than u32 observations in the data. ScalarOperand - This is required because we multiply an array by a scalar in calculating the input gradient.  With that out of the way, we can implement the two functions!\nFirst up is the calculate_output function. In the case of the mean squared error, this is just finding the difference between prediction and target arrays, then squaring that difference (we can use the elementwise multiplication operator * on the error array). Then we take the squared error Array and find its sum. Finally we divide by the number of rows. The code is fine to follow so I\u0026rsquo;ll just post it here:\nfn calculate_output(\u0026amp;self, prediction: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, target: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; T { let error = prediction - target; let squared_error = \u0026amp;error * \u0026amp;error; let squared_error_sum = squared_error.sum(); squared_error_sum / (prediction.nrows() as u32).into() } As mentioned above, in the last line you can see that in order to divide by the number of rows (count), we need to turn that number into an instance of type T. We can do this because we put the trait bound Fromwhich allows us to use the \u0026ldquo;into\u0026rdquo; function on a u32 to get an instance of T to divide by.\nThe second function is also kind of simple. The formula for calculating the gradient is 2 * (prediction - target) / count. There\u0026rsquo;s another instance of converting the count into an instance of type T, however because we\u0026rsquo;re also multiplying by 2, we need to get an instance of type T that represents the number \u0026ldquo;2\u0026rdquo;:\nfn calculate_input_gradient( \u0026amp;self, prediction: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, target: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { let two_of: T = 2.into(); let error = prediction - target; let count: T = (prediction.nrows() as u32).into(); let average_error = error / count; average_error * two_of } Loss For the actual Loss type, we would like to allow customization via specifying a LossImpl trait object as a \u0026ldquo;vtable\u0026rdquo; as previously described. Additionally, on the forward pass we will need to cache the predictions and targets in order to calculate the gradient with the last used pair on the backward pass. As with Operation and Layer, these are initially None and then on the first forward pass will be set to Some. Therefore they are of type Option. As our LossImpl trait object is a reference that can be on the stack, we will require a lifetime annotation (\u0026lsquo;a in the code). The full definition then looks as follows:\npub struct Loss\u0026lt;\u0026#39;a, T\u0026gt; { implementation: \u0026amp;\u0026#39;a dyn LossImpl\u0026lt;T\u0026gt;, prediction: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, target: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, } The first function we will need is a way to construct a new instance. We will do this with the idiomatic \u0026ldquo;new\u0026rdquo; function. The only thing we are required to provide when constructing one is the LossImpl trait object as the other two fields will be initialized to None like so:\npub fn new(implementation: \u0026amp;\u0026#39;a dyn LossImpl\u0026lt;T\u0026gt;) -\u0026gt; Self { Self { implementation, prediction: None, target: None, } } The second function we need is a function to call on the forward pass to calculate the total loss value. For this we will pass it the predictions and targets arrays, which are required to be the same size. We delegate the actual output calculation to the LossImpl provided. We will also stash the provided predictions and targets for use in the backward pass. The full function is fairly simple so I\u0026rsquo;ll list it here:\npub fn forward(\u0026amp;mut self, prediction: Array\u0026lt;T, Ix2\u0026gt;, target: Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; T { assert_eq!(prediction.raw_dim(), target.raw_dim()); let loss_value = self.implementation.calculate_output(\u0026amp;prediction, \u0026amp;target); self.prediction = Some(prediction); self.target = Some(target); loss_value } The final method we need is to perform the backwards pass. This is also simple and just passes the previously stashed predictions and targets from the forward pass into the input gradient calculation function of LossImpl. We can use the standard \u0026ldquo;as_ref().unwrap()\u0026rdquo; pattern on Optionto get a \u0026amp;T from it. After we get the input gradient back from the implementation, we need to check that the shape matches that of the predictions since we\u0026rsquo;ll be starting the backpropagation with it. The function implementation is then as follows:\npub fn backward(\u0026amp;mut self) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { let input_gradient = self.implementation.calculate_input_gradient( self.prediction.as_ref().unwrap(), self.target.as_ref().unwrap(), ); assert_eq!( self.prediction.as_ref().unwrap().raw_dim(), input_gradient.raw_dim() ); input_gradient } ","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/lossfunction/","summary":"Overview Once we\u0026rsquo;ve passed the data through the hidden layers of our neural network, and passed it through a final output layer (which should have only 1 neuron to bring the dimensionality of the data back to 1 feature/column) we will end up with a vector of predictions as mentioned previously (1 for each observation in the data). We already have a vector of targets which are the actual measured answers.","title":"3.3 - Loss Function"},{"content":"Overview The lowest level of abstraction as mentioned in the previous post is the operation. These were the lowest level building block that is able to perform a single unit of calculation on the forward pass, and single unit of partial derivative calculation on the backward pass.\nDuring that chapter, we could see how multiple of these operations are able to be chained together, though we don\u0026rsquo;t want to do this by hand. Instead, we would like an abstraction that has the same interface as the operations (a forward and backward method) but which internally handles routing the data through the operations that are part of the layer.\nWe define a layer as a series of linear operations, followed by a final (optional) non-linear operation known as the \u0026ldquo;activation function\u0026rdquo;.\nIn the context of the simple neural network implemented in the last chapter, we had 5 operations that were:\n A WeightMultiply operation which takes the initial input and selection of weights and multiplies them together, ending with a matrix that represents the weighted sum for each of the \u0026ldquo;learned features\u0026rdquo; that we wanted in this \u0026ldquo;hidden layer\u0026rdquo;. A BiasAdd operation which added in the bias term to the weighted multiplication. A Sigmoid function that took the results and \u0026ldquo;squashed\u0026rdquo; them to a non-linear range between 0 and 1. A WeightMultiply operation which took the inputs from the learned features, and multiplied them with a second set of weights. A BiasAdd operation which added in the bias term to the calculated weighted sum (which was a single value at this point).  We can see that there are 2 layers here:\n Operations 1 to 3 comprise the first layer which takes the initial input, operates on it, and produces values for a set of learned features (or as we\u0026rsquo;ll use here, \u0026ldquo;neurons\u0026rdquo;) Operations 4 and 5 will comprise the final layer which takes the learned hidden features to the final output  A diagram below shows this grouping of operations (with the loss function tacked onto the end):\nNeurons Rather than looking at the individual operations within the layers, we instead look at neural networks at the abstraction of these layers. A layer is, as mentioned previously a series of non-linear operations followed by a linear one, but another important facet of a layer is the number of neurons it contains.\nIn the brain, neurons will take electrical impulses from other connected neurons and will fire if that energy exceeds some certain threshold (which then causes other neurons to possibly fire, etc.). In the case of artificial neural networks, this activation is modeled by transformation of the input to an output through a non-linear function (the Sigmoid function for example), which we also can call the activation function for a layer. The values coming out of this are the activations for that layer.\nThe number of neurons in an artificial neural network will correspond to the number of features in the input. Therefore in the simple neural network seen in the last chapter, and whose operations are briefed above we can see that:\n The number of neurons in the input layer is X where X is the number of features in the data set. The number of neurons in the hidden layer is N where N is some number that we define. The output from this layer will be a matrix with the number of rows as in the input, and the number of columns equal to the neuron count for that layer. The number of neurons from the output layer is 1. This is because we\u0026rsquo;re collapsing the values down to a single prediction for each observation (to allow us to use the loss function to detect error).  Deep learning is simply using neural networks with more than one hidden layer.\nA more useful diagram below doesn\u0026rsquo;t show the operations inside the layers, but abstracts the layers only to show their inputs and outputs:\nA small detour Before delving into the implementation for the Layer abstraction, we first address a need for a function that will be required to be added to the \u0026ldquo;Operation\u0026rdquo; type that we implemented in the previous section.\nFor ParameterOperation instances we stored the parameter being used, along with the last computed gradient for the parameter in the backward pass. One thing that we will need and will lift to the Layer abstraction is the ability to access temporarily the parameter and parameter gradients in use.\nIn order to achieve this, we will add the following function to the Operation type:\npub fn parameter_and_gradient(\u0026amp;mut self) -\u0026gt; Option\u0026lt;(\u0026amp;mut Array\u0026lt;T, Ix2\u0026gt;, \u0026amp;Array\u0026lt;T, Ix2\u0026gt;)\u0026gt; { ... } This function will return the parameter as a mutable reference, since we will eventually need to be able to update the parameter, given the gradient. However the gradient doesn\u0026rsquo;t need to be mutable so we return an immutable reference.\nOne thing to note here is that not all operations have a secondary parameter (those that are BasicOperations) - therefore we return an Option to allow for this possibility.\nThe implementation of this function then look as follows:\nmatch \u0026amp;mut self.data { OperationData::Parameter { parameter, parameter_gradient, .. } =\u0026gt; Some((parameter, parameter_gradient.as_ref().unwrap())), _ =\u0026gt; None, } That is, we must match on the OperationData enum value we have. In the case of it being a parameter operation, we can get the references we want, and return these in a \u0026ldquo;Some\u0026rdquo; variant. If not, then we return None as there\u0026rsquo;s no parameters to get.\nLayerSetup Just as we have the BasicOperation and ParameterOperation traits to allow for customization of the calculations for specific operations, we would also like to define a static structure for a Layer while allowing construction of that layer (determining which operations, etc.) to be customized.\nThis is the purpose of the LayerSetup trait. We will shortly see that the structure of Layer is static while delegating the actual setup of the layer to a trait object implementing this trait.\nThe trait header looks as follows:\npub trait LayerSetup\u0026lt;\u0026#39;a, T\u0026gt; { ... } This again requires the same generic parameters that are required by Operation. That is:\n \u0026lsquo;a - This is the lifetime of the borrow for the BasicOperation or ParameterOperation vtables in use T - This is the type of the elements inside the ndarray::Array  Looking at the trait itself, it only requires implementors to provide the following function:\nfn layer_setup( \u0026amp;self, num_neurons: usize, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, seed: \u0026amp;Option\u0026lt;u64\u0026gt;, ) -\u0026gt; Vec\u0026lt;Operation\u0026lt;\u0026#39;a, T\u0026gt;\u0026gt;; This is a function that, given the number of neurons/features that the layer should have, along with the input (used to determine the number of features in the input) can return the list of Operations required in that layer.\nThe additional parameter is an optional seed used for random number generation. If this is None then it\u0026rsquo;s expected that the numbers should be truly randomly generated, if it\u0026rsquo;s Some then the seed shouild be used to construct the PRNG stream.\nDenseLayerSetup Previously we saw that the layers (the hidden and output layers) in our neural network fully connects the neurons of the previous layers and consists of a sequence of WeightMultiply -\u0026gt; BiasAdd -\u0026gt; ActivationFunction where ActivationFunction is some non-linear function. For the hidden layer we used Sigmoid and for the output layer we used a Linear activation function which is just a passthrough function.\nHowever the basic concept of fully connecting the neurons of the previous layer, and using a weighted sum and bias addition is consistent between layers. The activation function needs to be tweakable.\nThe terminology for these fully connected layers is a dense layer (due to it being densely connected).\nThis, therefore will be known as the DenseLayerSetup.\nAs mentioned above, we will need to modify the activation function in use, so we will keep this as a field. As before, we will need to make it generic over the lifetime, \u0026lsquo;a, and the type T. We know that the activation function should be one that doesn\u0026rsquo;t take an additional parameter (if this proves an incorrect assumption we can change later), so we can take just the BasicOperation trait object rather than a wrapped Operation.\nThe structure definition therefore is as follows:\npub struct DenseLayerSetup\u0026lt;\u0026#39;a, T\u0026gt;(\u0026amp;\u0026#39;a dyn BasicOperation\u0026lt;T\u0026gt;); Next, we want a simple constructor function for this. We don\u0026rsquo;t want to make the fields public since they could be changed after construction, so instead we keep the field private and provide a constructor function for external use.\nimpl\u0026lt;\u0026#39;a, T\u0026gt; DenseLayerSetup\u0026lt;\u0026#39;a, T\u0026gt; { pub fn new(activation: \u0026amp;\u0026#39;a dyn BasicOperation\u0026lt;T\u0026gt;) -\u0026gt; Self { Self(activation) } } Finally, we have the actual trait implementation. First of all, we have the header for the impl block:\nimpl\u0026lt;\u0026#39;a, T: LinalgScalar\u0026gt; LayerSetup\u0026lt;\u0026#39;a, T\u0026gt; for DenseLayerSetup\u0026lt;\u0026#39;a, T\u0026gt; where Standard: Distribution\u0026lt;T\u0026gt;, { ... } A couple of things to note here:\n We require a trait bound of LinalgScalar on the generic parameter, T. This is because the layer needs to use the WeightMultiply and BiasAdd operations that both put a restriction of LinalgScalar on their generic parameter (due to requiring the arithmetic operations, etc.) We are using random generation for the initial parameters on the weight and bias operations. In order to use this random number generation, we require that our type T is such that the structure \u0026ldquo;Standard\u0026rdquo; implements Distribution.  Next is the actual function implementation. I won\u0026rsquo;t repeat the function signature here as it was specified in the LayerSetup section.\nThe first step in the implementation is to get the random number generator instance. This is going to depend on if there\u0026rsquo;s a random seed specified or not. If there is, we will use it to construct the generator. If not, we get a truly random generator. This code looks as followws:\nlet mut r = match seed { Some(seed) =\u0026gt; StdRng::seed_from_u64(*seed), None =\u0026gt; StdRng::from_rng(thread_rng()).unwrap(), }; Next, we need to generate the random weights and bias values to use as parameters to the appropriate operations. The steps for this are as follows:\n We figure out what dimensions we require for the weights matrix. The number of rows in the weights matrix will match the number of columns/features in the input data - this allows us to perform the matrix multiplication. The number of columns in the weights matrix will then be the number of output neurons we require in the layer. Calculate the number of elements in the matrix which we use for creating the buffer in the next step. This is just the number of rows multiplied by number of columns. Create a vector with a number of elements calculated in step #2. Each element is a randomly generated value using the generator we made previously. In order to make these, we can use the repeat_with function from std::iter which takes a function and calls it repeatedly. We provide a function that just produces random values. Then we use the take adapter to take the appropriate number of elements, and finally collect them into a vector. Finally, we use the Array::from_shape_vec constructor to turn the Vec into an Array.  The code for the above looks as follows:\nlet weight_dim = (input.ncols(), num_neurons); let weight_count = weight_dim.0 * weight_dim.1; let weights = repeat_with(|| r.gen::\u0026lt;T\u0026gt;()).take(weight_count).collect(); let weights = Array::from_shape_vec(weight_dim, weights).unwrap(); Creating the bias matrix is the same, except the dimensions are different. The bias matrix always consists of a single row, and the number of columns is the number of neurons we have. This looks as follows:\nlet bias_dim = (1, num_neurons); let bias_count = bias_dim.0 * bias_dim.1; let bias = repeat_with(|| r.gen::\u0026lt;T\u0026gt;()).take(bias_count).collect(); let bias = Array::from_shape_vec(bias_dim, bias).unwrap(); Finally we have the parameters we need to create the layer. We make sure to include the activation function that we stored as a field as the last operation in the sequence:\nvec![ Operation::new_parameter(\u0026amp;WeightMultiply, weights), Operation::new_parameter(\u0026amp;BiasAdd, bias), Operation::new_basic(self.0), ] Layer Finally we can implement the abstraction for a Layer. As with Operation, I\u0026rsquo;ll split this up into sub-headers to break it up a little.\nDefinition The structure definition looks as follows:\npub struct Layer\u0026lt;\u0026#39;a, T\u0026gt; { num_neurons: usize, operations: Option\u0026lt;Vec\u0026lt;Operation\u0026lt;\u0026#39;a, T\u0026gt;\u0026gt;\u0026gt;, setup: \u0026amp;\u0026#39;a dyn LayerSetup\u0026lt;\u0026#39;a, T\u0026gt;, output: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, seed: Option\u0026lt;u64\u0026gt;, } As always, we need to take the type of elements in the arrays (T) and lifetime of the OperationData vtables (\u0026lsquo;a).Following is a description of each parameter within the definition however:\n num_neurons - This is the number of columns/features in the output of the layer. It\u0026rsquo;s used to generate weights and bias of a correct size to produce the desired dimension operations - This is an owned vector of the Operations in the layer. It\u0026rsquo;s an Option here because we can default to None until the first forward pass is performed. We could use an empty Vector but that would not be as explicit. This way if someone tries to use the list of operations without doing a forward pass first then they\u0026rsquo;ll see it\u0026rsquo;s invalid by the program panicking setup - A reference to the LayerSetup to use to produce the list of Operations on the first forward pass output - Store the calculated output from the forward pass which can be used then on a backward pass seed - The optional random seed to use for generating weights/bias terms in the layer  Factory Functions Secondly we have 2 factory functions to create a Layer. One of which takes the random seed to use, and the other which doesn\u0026rsquo;t (so the RNG will be different every time). The basic version will take the number of neurons we want in the layer, along with a required LayerSetup implementation.\nEverything else is set to None by default:\npub fn new(num_neurons: usize, setup: \u0026amp;\u0026#39;a dyn LayerSetup\u0026lt;\u0026#39;a, T\u0026gt;) -\u0026gt; Self { Self { num_neurons, operations: None, setup, output: None, seed: None, } } The fixed seed version is more or less the same, except it sets the seed to the provided value:\npub fn new_with_seed(num_neurons: usize, setup: \u0026amp;\u0026#39;a dyn LayerSetup\u0026lt;\u0026#39;a, T\u0026gt;, seed: u64) -\u0026gt; Self { Self { num_neurons, operations: None, setup, output: None, seed: Some(seed), } } Forward Pass Both the forward and backward functions are inside an impl block that looks as follows:\nimpl\u0026lt;\u0026#39;a, T: Clone\u0026gt; Layer\u0026lt;\u0026#39;a, T\u0026gt; { ... } That is, we place the bound of Clone on the type T. The reason for this is simply that the forward method takes \u0026ldquo;input\u0026rdquo; by reference, however we will need it by value for folding as we\u0026rsquo;ll see shortly, so we need to clone it.\nThe signature of the forward method itself is identical to the forward method on Operation. This is not a coincidence as Layer is wrapping multiple Operations while keeping the API the same. This is the Composite design pattern where one larger thing has the same API as the smaller unit, but which encapsulates multiple of those smaller units.\npub fn forward(\u0026amp;mut self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { ... } The first step on the forward pass is to make sure that we generate (using the LayerSetup) the list of Operations on the first forward pass. We can actually do this nicely in Rust by using a method on Optioncalled get_or_insert_with. This function takes a callback, and calls it if the Option is currently None. The return value from the function is then set as the value of the Option for further runs. Finally a reference to the got or inserted entry is returned.\nlet operations = self .operations .get_or_insert_with(|| self.setup.layer_setup(self.num_neurons, input, \u0026amp;self.seed)); In order to perform the forward pass, we basically just want to start off with the input we were provided, and then pass it through the chain of operations. We can easily do this with the fold method on iterators. This method takes an initial state (starts as the provided input), and a function that is called on every item in the iterator that can update and return new state. Once we\u0026rsquo;ve gone through all elements, the final value is returned, which we\u0026rsquo;ll stash in self.output:\nself.output = Some( operations .into_iter() .fold((*input).clone(), |state, operation| { operation.forward(\u0026amp;state) }), ); Finally we can return a clone of the output (we can\u0026rsquo;t return the output itself as we need a copy for the backward pass):\n(self.output.as_ref().unwrap()).clone() Backward Pass For the backward pass, just like with Operation, we will first need to validate the shape of the provided output gradient is correct before passing it backwards through the operations list.\nThe signature also is very familiar as it mimics the same for Operation:\npub fn backward(\u0026amp;mut self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { ... } We firstly verify that the shape of the provided output gradient matches that of the output of the layer, as calculated from the forward pass:\nassert_eq!( self.output.as_ref().unwrap().raw_dim(), output_gradient.raw_dim() ); Passing the output gradient through the series of Operations to get an input gradient to return is the same as the forward pass almost. We do need to remember to run backwards through the vector of operations. We can get a reversed iterator with the rev() method:\nself.operations .as_mut() .unwrap() .into_iter() .rev() .fold((*output_gradient).clone(), |state, operation| { operation.backward(\u0026amp;state) }) Parameters And Gradients Finally we need a way to process all the parameters and gradients from ParameterOperations on the layer. This is why we added the parameter_and_gradient method to Operation, so that we can access those references from the layer.\nThis function is a bit tricky to write however, because the reference to the parameter is mutable.\nRust doesn\u0026rsquo;t allow multiple mutable references to the same item (or even a mutable and immutable reference active at the same time) and so we can\u0026rsquo;t just return references as returned from parameter_and_gradient. This is because those are tied to the lifetime of the Operation itself, rather than the lifetime of the iterator.\nTo convince Rust that the returned references have the same lifetime as the iterator, we can specify explicitly that the references have the same lifetime as the \u0026amp;mut self parameter to the method. The iterator owns the \u0026amp;mut self borrow until it goes out of scope, so we can tie the lifetime of the references returned as items to the lifetime as the borrow of Self fairly easy:\npub fn parameters_and_gradients\u0026lt;\u0026#39;b\u0026gt;(\u0026amp;\u0026#39;b mut self) -\u0026gt; impl Iterator\u0026lt;Item = (\u0026amp;\u0026#39;b mut Array\u0026lt;T, Ix2\u0026gt;, \u0026amp;\u0026#39;b Array\u0026lt;T, Ix2\u0026gt;)\u0026gt; { ... } Here we are introducing the lifetime parameter \u0026lsquo;b, and saying that this borrow \u0026amp;mut self lasts for that length of time. Then we\u0026rsquo;re saying that the items in the returned iterator also live for this time too.\nThe next step is that we require a little unsafe\u0026hellip;.\nThe problem we have now is that the vector of Operation instances we have is of the following type:\nVec\u0026lt;Operation\u0026lt;\u0026lsquo;a, T\u0026raquo;\nNow \u0026lsquo;a is the lifetime of the Operation itself (or more specifically the lifetime of the \u0026amp; dyn BasicOperation/\u0026amp; dyn ParameterOperation references inside). So if we were to iterate over that as it is now, then it would produce references with lifetime \u0026lsquo;a.\nThis means the produced references from the iterator would live for \u0026lsquo;a, but the iterator itself only for \u0026lsquo;b. Thus, there\u0026rsquo;s a conflict and the compiler will complain.\nIn order to solve this, we can use (unsafely) std::mem::transmute to very carefully change Operation\u0026lt;\u0026lsquo;a, T\u0026gt; into Operation\u0026lt;\u0026lsquo;b, T\u0026gt;.\nNow this is perfectly safe in fact because we know that the references actually live for \u0026lsquo;a, which is guaranteed to live at least as long as \u0026lsquo;b. Therefore there\u0026rsquo;s no harm in shortening the lifetime to \u0026lsquo;b on those Operation instances.\nThe code for that is as follows (note that I\u0026rsquo;m coercing the Vec into a slice and transmuting that, but either works):\nlet operations = unsafe { transmute::\u0026lt;\u0026amp;\u0026#39;b mut [Operation\u0026lt;\u0026#39;a, T\u0026gt;], \u0026amp;\u0026#39;b mut [Operation\u0026lt;\u0026#39;b, T\u0026gt;]\u0026gt;( self.operations.as_mut().unwrap(), ) }; Once we\u0026rsquo;ve done this transmutation to shorten the invariant lifetime on Operation, the rest of the function is simple enough and is just a case of doing a filter_map and calling parameter_and_gradient on each operation.\nparameter_and_gradient returns None if it\u0026rsquo;s a BasicOperation, or Some((param, grad)) if it\u0026rsquo;s a ParameterOperation so we can return this from filter_map directly.\nfilter_map will only pass forward any Some values and will unwrap them as it runs through them:\noperations .into_iter() .filter_map(|op| op.parameter_and_gradient()) ","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/layers/","summary":"Overview The lowest level of abstraction as mentioned in the previous post is the operation. These were the lowest level building block that is able to perform a single unit of calculation on the forward pass, and single unit of partial derivative calculation on the backward pass.\nDuring that chapter, we could see how multiple of these operations are able to be chained together, though we don\u0026rsquo;t want to do this by hand.","title":"3.2 - Layers"},{"content":"This chapter is about creating the abstractions in code that enable us to think of neural networks in a modular fashion, and build up complex deep networks with simple structures.\nI will be implementing the abstractions in an accompanying library called Eidetic which is free for use and distribution, since I\u0026rsquo;m only implementing this as a learning opportunity.\nOverview The Operation construct will represent and encapsulate a single operation in a neural network, such as the sigmoid function, matrix multiplication, etc.\nAn operation should be able to take inputs in and produce an output, as well as take a computed partial derivative from the output side and add its own derivative into the mix (i.e. the chain rule).\nAn Operation structure will have forward and backward methods which achieve this, and we will deal exclusively with 2-dimensional ndarray::Array instances as this is the library of choice to do the lower level matrix representation and operations we\u0026rsquo;ve picked to use in Rust.\nThere are two types of Operations - ones which take only a single array and produces an output array, and ones that take two arrays.\nAn example of the arity-1 operations would be the sigmoid function, and an example of the arity-2 operations would be the matrix multiplication, or bias addition functions.\nAdditionally there are some operations which will change the shape of the input array as output, and some that will retain the shape.\nAn example of shape preserving operation is the sigmoid function. And an example of a shape changing operation is matrix multiplication.\nFor computing the gradients, an operation will have an output gradient that represents the total gradient calculated by the network to the right/output side of an operation, and will have an input gradient which is what is propagated to the left to other operations on the input side.\nThe general rule is an Operation must make sure that the shape of the input gradient matches the input, and shape of the output gradient matches the output to allow us to correctly reason about these derivatives.\nDiagrams An example of an Operation which takes only one input and produces an output is shown in the diagram below:\nAnd an example of an Operation which takes two inputs and produces an output is shown in the following diagram:\nImplementation Approach In order to ensure that the correct checks are done during use, with respect to the shapes of the input/input gradient, and output/output gradient arrays (additionally parameter/parameter gradient arrays if this is an operation that takes a secondary parameter), we will make the Operation type a concrete type, letting us fix forward/backward functions and perform the appropriate invariant checks.\nSince there are two types of operation, and one taking a secondary parameter needs to do additional work/checks on the parameter and parameter gradient, we will make this type an enumeration type.\nWithin these two broad categories of Operation, we can implement anything that\u0026rsquo;s able to take an input and produce an output, as well as take an output gradient and produce a gradient of its input.\nIn order to achieve this in Rust, we can use a trait object that we pass into the Operation. This will act as a sort of vtable.\nThis approach lets us hard-code the parts that are important, while allowing customisation on the parts that should change. The following sections will detail each type and explain why the code is that way.\nBasicOperation This is a trait for the \u0026ldquo;vtable\u0026rdquo; I mentioned above. It represents the inner calculation for an operation that does not take a secondary parameter.\nThe signature is as follows and is generic over some type T\npub trait BasicOperation\u0026lt;T\u0026gt; The generic parameter here as we\u0026rsquo;ll see shortly is the underlying data type that\u0026rsquo;s stored inside of the ndarray::Array instances that we\u0026rsquo;ll be using.\nThe first function defined in the trait is the one that will be used to calculate an output array from a given input array on the forward pass through this operation.\nfn calculate_output(\u0026amp;self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; A few things to note are:\n We don\u0026rsquo;t need mutable access to self. It\u0026rsquo;s intended that these vtables for the inner calculations of operations are stateless, thus it should be sufficient to use the provided input parameter to calculate an output. We don\u0026rsquo;t put a bound on the type T at this point as it\u0026rsquo;s good practice in Rust to only put bounds when absolutely necessary so as to not cause bound propagation whenever the type is used. We know that we\u0026rsquo;re working with 2-dimensional data (the rows representing the observations in the data, and the columns representing the features), so we hardcode the dimensionality of these arrays to ndarray::Ix2. We don\u0026rsquo;t need to have ownership of the input so to avoid unnecessary cloning, we take by reference. This means that the previous Operation can stash the input, and provide a reference to it, rather than needing to clone and have it discarded anyway.  The second function that is defined by this trait is the function that will be called on the backward pass to calculate the gradient of its inputs given the output gradient coming in from the rest of the network.\nfn calculate_input_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; The points above are valid here too, with a couple of additional things to note:\n The implementor MUST combine their partial derivative with the output gradient to produce the final gradient. We can\u0026rsquo;t do this externally in the Operation type where we do shape validation, because the way in which they\u0026rsquo;re combined varies based on the operation (could be elementwise multiplication or matrix multiplication, etc.). We take the input again here, because calculating the input gradient necessarily needs to know what the input was. Since these implementations are stateless (so they can be reused for multiple operations if needed), we get given the previous input.  ParameterOperation The second broad category of operation are those that take a secondary parameter array as well as the input.\nAs with the basic operation we need the ability to calculate the output on the forward pass and the input gradient on the backward pass. However, unlike the BasicOperation, we need to also be able to calculate the gradient of the parameter, as well as route the parameter through the other calculation functions.\nThe signature looks the same as BasicOperation, again here we\u0026rsquo;re taking a T to define the underlying data type of the elements in the ndarray::Array instances:\npub trait ParameterOperation\u0026lt;T\u0026gt; The next element required by implementations is again, a calculate_output function. This signature is almost identical to that defined by BasicOperation, however since we know there\u0026rsquo;s a secondary parameter, this signature takes that into account:\nfn calculate_output(\u0026amp;self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; Likewise, the calculate_input_gradient is almost identical to the BasicOperation version, except this too takes an additional parameter (since the gradient of the input can be defined in terms of the secondary parameter):\nfn calculate_input_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; This trait however defines a third item. Since this is an operation that takes a secondary parameter, we need to be able to calculate the gradient for it to be able to update (for example the weights). This third item is identical to calculate_input_gradient except that it instead calculates the gradient of the parameter:\nfn calculate_parameter_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; WeightMultiply Now that we\u0026rsquo;ve defined the traits for the two broad categories of operation, we can define the concrete implementations of these operations.\nThe first up is the WeightMultiply operation which we use to multiply our matrix of observations together with the current vector of weights (not including the bias term).\nAs mentioned before, these \u0026ldquo;vtables\u0026rdquo; will be stateless, and as such the definition of the WeightMultiply structure is suitably underwhelming:\npub struct WeightMultiply; That is just a unit struct that doesn\u0026rsquo;t have any fields, but allows us to implement the calculations on it.\nThe definition of the implementation block for WeightMultiply is as expected, just implementing ParameterOperation on WeightMultiply. We need to implement ParameterOperation rather than BasicOperation because the weights are taken in as the secondary parameter. The impl block then is:\nimpl\u0026lt;T: LinalgScalar\u0026gt; ParameterOperation\u0026lt;T\u0026gt; for WeightMultiply { ... } The only thing to note is that we are doing a blanket implementation for all types T, however, this would not allow us to perform any arithmetic operations on the arrays with this element type. We require the ability to calculate the dot product and for the dot product to be available on ndarray::Array then we need to place a bound on the generic type T such that we only accept T\u0026rsquo;s which conform to the LinalgScalar trait.\nLooking at the definition for this trait we can see that it\u0026rsquo;s just requiring a bunch of mathematical traits so we can multiply and add values (required for dot product):\npub trait LinalgScalar: \u0026#39;static + Copy + Zero + One + Add\u0026lt;Output = Self\u0026gt; + Sub\u0026lt;Output = Self\u0026gt; + Mul\u0026lt;Output = Self\u0026gt; + Div\u0026lt;Output = Self\u0026gt; { } Getting into the actual implementations of the ParameterOperation trait now, and starting with the easiest one\u0026hellip;.calculating the output on the forward pass. This is just a matrix multiplication between the input matrix of data, and the secondary parameter, our weights:\nfn calculate_output(\u0026amp;self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { input.dot(parameter) } Now we can calculate the gradients on the backward pass. Thinking back to the previous posts, we know that when a dot product is involved, the partial derivative is just the transpose of the other array to the one we care about. We can see this in the definition of the calculate_input_gradient function:\nfn calculate_input_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, _input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { output_gradient.dot(\u0026amp;(*parameter).clone().reversed_axes()) } Here we are doing the following sequence of events when calculating the input gradient:\n We dereference the input parameter array reference to get a value (we can\u0026rsquo;t move out of this value though as we got it from dereferencing a borrow that we don\u0026rsquo;t own) We clone the parameter matrix, which gives us a brand new copy that we do own and have permission to consume We call reversed_axes to get the transpose matrix. This function requires self by value and consumes it, re-using the memory. This is why we had to clone the parameter array, because we can\u0026rsquo;t consume out of a borrow We then take a borrow to the newly transposed matrix. This is required because we\u0026rsquo;re calling the dot product function on a reference to an array, and so requires the right hand side operand to also be a reference  The partial gradient, and contribution from this operation is the transposed parameter matrix. However the dot product is required since we are returning the total gradient thus far, incorporating our partial gradient into what we were told from the rest of the network.\nThe calculation of the parameter gradient is similar, however notice that we have the output_gradient on the right hand side this time, not the left:\nfn calculate_parameter_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, _parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { (*input).clone().reversed_axes().dot(output_gradient) } BiasAdd The second operation we need to implement is the ability to add a bias to the newly minted weighted product from the previous operation.\nThis too, is a ParameterOperation as we need to set the bias term as the additional parameter.\nThe struct definition is a unit struct just like WeightMultiply as we just use it as a trait object to get the behaviour from:\npub struct BiasAdd; As before too, for implementation we can only implement the trait for elements that conform to the LinalgScalar trait. In this case, it\u0026rsquo;s because we need the elementwise multiplication, and addition:\nimpl\u0026lt;T: LinalgScalar\u0026gt; ParameterOperation\u0026lt;T\u0026gt; for BiasAdd { ... } The calculate_output method implementation here is very simple, it\u0026rsquo;s just adding the parameter to the input. However, we must check that the bias term consists of only 1 row (with n elements, where n is the number of features).\nThe bias term is smaller in size than the input, but ndarray Arrays will broadcast the addition across the whole matrix.\nfn calculate_output(\u0026amp;self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { assert_eq!(parameter.nrows(), 1); input + parameter } For calculating the input gradient, we know from the reasoning in chapters 1 and 2 that the partial derivative for addition will just be a matrix with the same shape as the input but full of all 1\u0026rsquo;s. As before we need to multiply this by the output gradient to take into account the chain rule.\nIn ndarray, we can get an array of 1\u0026rsquo;s of a certain shape by using the associated function \u0026ldquo;ones\u0026rdquo;. This will take a tuple defining the shape we want. We can use the method \u0026ldquo;raw_dim\u0026rdquo; to get the shape of another array (in this case our input matrix):\nfn calculate_input_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, _parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { Array::ones(input.raw_dim()) * output_gradient } Finally, the calculation of the parameter gradient. This will start off being calculated the same way as we calculated the input gradient above, however we need to sum the resulting matrix along the axis of the rows.\nThis, as mentioned a few times previously, is to account for the bias term being added multiple times to the input data.\nWe will get a column vector from this, but to conform to the shape expected from the input gradient we convert to a row vector.\nIn order to get a view along a specific axis of an array, we can use the map_axis method which takes the axis we\u0026rsquo;re traversing along, and a callback function which is called for each subview. In our case, for each subview, we want to get the sum.\nThe method into_shape lets us reshape the same data, in our case we ensure we are producing a row vector. This can fail if the provided shape isn\u0026rsquo;t compatible with the provided data, but we just unwrap it here as I don\u0026rsquo;t care too much about error handling at the moment.\nThe full code for this method then is:\nfn calculate_parameter_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, _input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { let parameter_gradient = Array::ones(parameter.raw_dim()) * output_gradient; parameter_gradient .map_axis(Axis(0), |view| view.sum()) .into_shape((1, parameter_gradient.ncols())) .unwrap() } Sigmoid The sigmoid activation function is our only BasicOperation so far. It doesn\u0026rsquo;t require a secondary parameter as it just modifies each element of the array by applying the sigmoid function.\nThis function as explained previously is defined as:\n$$ f(x) = \\frac 1 {1 + e^{-x}} $$\nAnd the derivative is defined in terms of itself as:\n$$ \\frac {\\partial f} {\\partial x}(x) = f(x) \\times (1 - f(x)) $$\nFirst we have the struct definition which, you guessed it!, is a unit struct as we don\u0026rsquo;t have a need for state:\npub struct Sigmoid; The trait bounds for Sigmoid here are a little different however than previously. The previous trait bound of LinalgScalar doesn\u0026rsquo;t work for us here as that only provides basic arithmetic operations. However what we need is:\n The ability to calculate the exponential function with the type T The ability to produce a quantity of \u0026ldquo;one T\u0026rdquo; The basic mathematical operators of +, -, *, / The ability to use unary negation  Looking in the num_traits crate we can see that two traits provide the \u0026ldquo;exp\u0026rdquo; method. These are Real and Float\u0026hellip;.however Real is just a subtrait of Float with additional stuff. Since we\u0026rsquo;re trying to be least restrictive we pick Float.\nThis trait requires a bunch of others that ultimately end up satisfying all the criteria we want.\nimpl\u0026lt;T: Float\u0026gt; BasicOperation\u0026lt;T\u0026gt; for Sigmoid { ... } To calculate the input, we simply apply the sigmoid function to each element in the input array to produce the output array. We can use mapv to call a function on each element of an Array to produce a new array of the same shape:\nfn calculate_output(\u0026amp;self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { input.mapv(|elem| T::one() / (T::one() + (-elem).exp())) } The only other function we need to implement (with this being a BasicOperation) is the calculate_input_gradient function.\nSince the partial derivative is defined in terms of itself, we first recalculate the output from the input. We can then use mapv again on this to apply the partial derivative formula elementwise before finally multiplying it together with the current output gradient as per the chain rule:\nfn calculate_input_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { let output = self.calculate_output(input); // recalculate the output at the input.  let partial_derivative = output.mapv(|elem| elem * (T::one() - elem)); partial_derivative * output_gradient } Linear We implement a passthrough operation for those situations where we need to provide an operation but don\u0026rsquo;t actually require it to do anything. Since the code is so simple, and the functions have been defined above, I\u0026rsquo;ll just list the code in its entirety here:\npub struct Linear; impl\u0026lt;T: Clone\u0026gt; BasicOperation\u0026lt;T\u0026gt; for Linear { fn calculate_output(\u0026amp;self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { (*input).clone() } fn calculate_input_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, _input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { (*output_gradient).clone() } } Operation Finally the big bad Operation struct. This is the type that wraps around these vtables we\u0026rsquo;ve been making and provides the invariant/shape checking functionality, as well as providing a single type to let us store different operations together.\nNote that we could have made Operation a trait, and stored a list of trait objects. However doing this would allow a user to bypass the array shape checks, and caching, etc. which we definitely do not want.\nThe way we have it set up now is that the user only is able to customise the calculations but not able to opt-out of checks, and the general flow.\nSince this is the largest type, I\u0026rsquo;ll split this section up into a sub section per item.\nOperationData This is the smallest unit so we\u0026rsquo;ll start here. This wraps over either a BasicOperation or a ParameterOperation and provides a homogenous type to allow us to store in the Operation wrapper.\nThis is defined as an enumeration because we need to store different data depending on whether we\u0026rsquo;re wrapping a BasicOperation or ParameterOperation.\nIn the case of the basic operation, we need a BasicOperation trait object only to provide the calculations.\nIn the case of parameter operations, we need to additionally provide storage for the parameter array and the parameter gradient.\nThe definition of this enum is as follows:\nenum OperationData\u0026lt;\u0026#39;a, T\u0026gt; { Basic(\u0026amp;\u0026#39;a dyn BasicOperation\u0026lt;T\u0026gt;), Parameter { operation: \u0026amp;\u0026#39;a dyn ParameterOperation\u0026lt;T\u0026gt;, parameter: Array\u0026lt;T, Ix2\u0026gt;, parameter_gradient: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, }, } Notes on this are:\n We are storing a reference to the BasicOperation/ParameterOperation implementation. We could use a Box but this would involve putting it on the heap. As we are going to be constructing the network up front, there\u0026rsquo;s no need for it to live on the heap so we store by reference instead. The downside here of course, is that we now need to add a lifetime annotation for the borrow. The parameter field is an Array because as we\u0026rsquo;ll see in a minute, the \u0026ldquo;constructor\u0026rdquo; takes a parameter and so we\u0026rsquo;re guaranteed to have it from construction time. However the parameter gradient is only guaranteed to be calculated after the first backward pass. We store this as an Option so that we get a panic if we try to unwrap before a backward pass is performed.  Operation This is the actual type that users will be dealing with and store in a list of operations within the layer later on.\nIt\u0026rsquo;s a simple struct that stores the input and input gradient (both Option as they\u0026rsquo;re invalid until the first forward/backward pass is performed), and the calculated output also invalid by default.\nThe last thing it stores is the OperationData so it can access the vtable (and parameter, parameter_gradient fields if appropriate).\nAgain, we need to plumb the lifetime annotation through. This time though, we\u0026rsquo;re not storing a reference directly in Operation, but we do need to plumb it through to OperationData so it knows how long the reference it has lives for.\nFinished definition of the Operation struct is therefore:\npub struct Operation\u0026lt;\u0026#39;a, T\u0026gt; { input: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, // last provided input on a forward pass  input_gradient: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, // last calculated input_gradient on the backward pass  output: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, // last calculated output on the forward pass  data: OperationData\u0026lt;\u0026#39;a, T\u0026gt;, // stores additional data that isn\u0026#39;t common between the two kinds of operation } Constructors We have two \u0026ldquo;constructors\u0026rdquo; for the two categories of operation (basic and parameterised). These are super simple and as expected just create the instance.\nIn the case of a basic operation, the only thing that\u0026rsquo;s required is a BasicOperation \u0026ldquo;vtable\u0026rdquo; trait object that we can use for calculations. Again we need to ensure that the lifetimes match up correctly (lifetime \u0026lsquo;a is defined in the impl block):\npub fn new_basic(operation: \u0026amp;\u0026#39;a dyn BasicOperation\u0026lt;T\u0026gt;) -\u0026gt; Self { Self { input: None, input_gradient: None, output: None, data: OperationData::Basic(operation), } } The second is for parameterised operations. This takes a ParameterOperation trait object along with an initial value for the parameter:\npub fn new_parameter( operation: \u0026amp;\u0026#39;a dyn ParameterOperation\u0026lt;T\u0026gt;, parameter: Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Self { Self { input: None, input_gradient: None, output: None, data: OperationData::Parameter { operation, parameter, parameter_gradient: None, }, } } Forward Pass Now we can implement the final two functions. These two (forward and backward) are the meat of the neural network and performs the necessary validation during the backward pass, as well as ensuring data is cached on the forward pass for use in gradient calculation.\nThe forward function will cache the input array. However, because we\u0026rsquo;re taking the input array by reference will cache a clone of it.\nThe signature and this input stashing line are:\npub fn forward(\u0026amp;mut self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { self.input = Some((*input).clone()); The next section of code is to calculate the output array, which we also stash. We do this by delegating to the OperationData trait object we\u0026rsquo;re wrapping over.\nDue to this being an enumeration, we need to do a match on the data and in the case of a parameter operation, remember to pass along the parameter with the input:\nself.output = Some(match \u0026amp;self.data { OperationData::Basic(operation) =\u0026gt; operation.calculate_output(input), OperationData::Parameter { operation, parameter, .. } =\u0026gt; operation.calculate_output(input, \u0026amp;parameter), }); Finally we need to return the output. But we can\u0026rsquo;t just return the stashed array as we need that for shape checking on the backward pass. We therefore need to clone this array to get a new one to return. Due to us holding the array in an Option, we need to do this with the as_ref method:\n(*self.output.as_ref().unwrap()).clone() Backward Pass The backward pass looks more complex than forward because of the asserts checking the shapes of the arrays that we have. However at its core, we\u0026rsquo;re still delegating the actual calculation to the internal OperationData.\nWe begin by checking that the shape of the output gradient we got passed in matches the shape of the last processed output that was generated on the forward pass:\nassert_eq!( self.output.as_ref().unwrap().raw_dim(), output_gradient.raw_dim() ); Again, because we\u0026rsquo;re storing the output in an Option, we need to use the as_ref().unwrap() sequence.\nThe next step is to get the input gradient. As with the output gradient we can just do a match on our vtable type and ask it to calculate for us. However the match arm for parameterised operations is larger because we also need to calculate the parameter gradient (and verify its shape) in the process of calculating the input gradient:\nself.input_gradient = Some(match \u0026amp;mut self.data { OperationData::Basic(operation) =\u0026gt; { operation.calculate_input_gradient(output_gradient, self.input.as_ref().unwrap()) } OperationData::Parameter { operation, parameter, parameter_gradient, } =\u0026gt; { *parameter_gradient = Some(operation.calculate_parameter_gradient( output_gradient, self.input.as_ref().unwrap(), \u0026amp;parameter, )); assert_eq!( parameter_gradient.as_ref().unwrap().raw_dim(), parameter.raw_dim() ); operation.calculate_input_gradient( output_gradient, self.input.as_ref().unwrap(), \u0026amp;parameter, ) } }); We then need to assert the shape of the input gradient we just calculated matches the shape of the input we processed in the forward pass:\nassert_eq!( self.input.as_ref().unwrap().raw_dim(), self.input_gradient.as_ref().unwrap().raw_dim() ); Finally we need to return the calculated input gradient, but since we\u0026rsquo;re keeping it for later querying by the optimizer, we need to return a clone:\n(*self.input_gradient.as_ref().unwrap()).clone() Usage The next step in defining our abstractions will be to bundle multiple operations into a single layer, but already see an example of the usage of these Operation instances we have.\nFor example, below is a chain of a multiply, bias add, and sigmoid. We first build the network as a vector of Operations and running through the network is just a case of running through each of the operations.\nWe can create the neural network in a separate function to which we pass the initial parameters. In this code we take the weights and bias terms and set up a very simple sequence of operations:\nfn create_network\u0026lt;\u0026#39;a\u0026gt;(weights: Array\u0026lt;f64, Ix2\u0026gt;, bias: Array\u0026lt;f64, Ix2\u0026gt;) -\u0026gt; Vec\u0026lt;Operation\u0026lt;\u0026#39;a, f64\u0026gt;\u0026gt; { vec![ Operation::new_parameter(\u0026amp;WeightMultiply, weights), Operation::new_parameter(\u0026amp;BiasAdd, bias), Operation::new_basic(\u0026amp;Sigmoid), ] } Note that the compilers lifetime inference rules is enough to find a value for \u0026lsquo;a that satisfies the lifetime we\u0026rsquo;re using them for and is the smallest such lifetime that covers all cases.\nRemember that a reference having the lifetime \u0026lsquo;a does not mean exactly that lifetime, but rather, that lifetime or longer.\nTherefore it\u0026rsquo;s rare to require multiple lifetimes and is only required when we need to track them separately through the borrow checker.\nIn our case, all of these operations will always be kept together in the neural network, so the compiler finding a single lifetime (the shortest one) for all the operations here is enough.\nWe can then initiate our weights, bias, and input:\nlet input = Array::from_shape_vec((2, 3), vec![0.5, 0.0, 3.0, 0.0, 0.25, 0.0]).unwrap(); let weights = Array::from_shape_vec((3, 1), vec![1.0, 1.0, 1.0]).unwrap(); let bias = Array::from_shape_vec((1, 1), vec![1.0]).unwrap(); Then pass the weights and bias to the neural network creation function:\nlet mut net = create_network(weights, bias); This needs to be mutable due to us wanting to call the forward and backward functions on the operations in this Vec.\nIn order to run through the chain of functions we can use a fold. The initial state of the fold is just the input array. The fold operation will return the final result which is the output of the forward pass:\nlet output = net.iter_mut().fold(input, |state, op| op.forward(\u0026amp;state)); For the backward pass we need an initial array for the output gradient. In this example we\u0026rsquo;ll just use an array of 1\u0026rsquo;s. We also know that the shape of the output gradient and output must match, so we can just make an output gradient of the same shape:\nlet output_gradient = Array::ones(output.raw_dim()); The backward pass is the same, we can use a fold again. This time starting with the output_gradient and running backwards through the chain (hence the .rev() call):\nlet input_gradient = net .iter_mut() .rev() .fold(output_gradient, |state, op| op.backward(\u0026amp;state)); ","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/operations/","summary":"This chapter is about creating the abstractions in code that enable us to think of neural networks in a modular fashion, and build up complex deep networks with simple structures.\nI will be implementing the abstractions in an accompanying library called Eidetic which is free for use and distribution, since I\u0026rsquo;m only implementing this as a learning opportunity.\nOverview The Operation construct will represent and encapsulate a single operation in a neural network, such as the sigmoid function, matrix multiplication, etc.","title":"3.1 - Operations"},{"content":"The previous post showed the most basic type of machine learning and how to train it, but the concepts learnt there (forward output calculations, backward gradient calculations) will apply equally here too.\nWhat was the problem with linear regression? The main problem with linear regression that we have is that we are training to find a set of weights, which are essentially the coefficients of an N-dimensional line. Therefore we\u0026rsquo;re trying to fit a straight line to a set of points.\nThe problem here is then that there is only so much we can do to fit a straight line to a set of points, and the error shows this. Linear regression works perfectly if the data set points are located already along some straight line, but in reality they rarely are.\nHow can we fix this? First of all, we need to introduce more tweakable values for the training algorithm to operate on. The more tweakable values that the algorithm has access to, then the more it is able to warp the resulting \u0026ldquo;line\u0026rdquo;.\nIn a neural network, what we end up finding and fitting to the data is not an N-dimensional line but an N-dimensional curve. More tweakables means more warp points.\nHowever, no matter how many sets of weights we end up using for training, the functions we use in linear regression will always produce a straight line. So additionally, as part of the modification, we will need to introduce a nonlinear function - that is, one which doesn\u0026rsquo;t map input values to output values along a straight line but along somme curve instead.\nStep 1: More weights The first part of this is to introduce the ability to have more weights that can be tweaked, and should be fairly straightforward to achieve.\nIn a standard linear regression, the formula we had for the initial \u0026ldquo;apply weights to observations\u0026rdquo; step was:\n$$ M = X \\cdot W $$\nThis formula remains the same, however if we look at the dimensionality of X, W, and M in the original linear regression we can see that we have:\n$$ [num\\_observations, num\\_features] \\cdot [num\\_features, 1] = [num\\_observations, 1] $$\nThe idea is that we change the weights from a column vector (matrix with column count of 1) to a matrix with N columns. N here is an arbitrary number of \u0026ldquo;learned features\u0026rdquo;, which can be considered nodes in the hidden layer of a neural network.\nThese \u0026ldquo;learned features\u0026rdquo; are an abstract notion, they don\u0026rsquo;t directly map to the actual features of the data set, but rather are features of the patterns in the data.\nIn short these learned features can represent whatever the network requires it to, we just won\u0026rsquo;t really know what they mean.\nLooking at the dimensionality of the new calculation we see:\n$$ [num\\_observations, num\\_features] \\cdot [num\\_features, N] = [num\\_observations, N] $$\nStep 2: A nonlinear function Once we have the weighted sum matrix, we will go ahead and feed it through a nonlinear function, which will ensure first nonlinearity in the output data, but also ensures that the output data is in the range 0 to 1.\nFor such a function, we will use the sigmoid function which has a graph that looks like this:\nThe sigmoid function is a good nonlinear function to choose for a few reasons:\n This function preserves information about the input (specifically the sign of it). If we were to use a nonlinear function such as squaring the input, then both -3 and 3 for example would give the same answer. With the sigmoid function -3 and 3 will give different outputs. It is nonlinear. This seems like an obvious point, but it is really important that we choose a nonlinear function for this step. Outputs are always in the range of 0 to 1 which is a nice property to have to keep numbers small. The most important property is that it has a derivative which is calculated easily in terms of itself. This derivative is shown below  $$ \\frac {\\partial \\sigma} {\\partial x}(x) = \\sigma(x) \\times (1 - \\sigma(x)) $$\nStep 3: Final linear regression The final step to get the actual answer is to perform one final linear regression on the outputs of the sigmoid function.\nA diagram showing the entire process can be seen below:\nConceptual diagram With the above function graph, we are able to see the actual operations that occur, but doesn\u0026rsquo;t conceptually show what a neural network looks like. From the implementation point of view the learned features are just columns in the initial matrix multiplication.\nA more common diagram showing what a neural network looks like conceptually can be seen below:\nThis shows the matrix multiplication (the lines between combinations of the input features - the initial nodes), and the output of the matrix multiplication (the middle layer of nodes which represent the learned features). Finally it shows the combination of the intermediate learned features to give a single output loss value as before.\nLoss gradients The overall loss gradient is calculated the same as before, as the mean squared sum of error between the predicted output and the target output for each observation.\nThe difference now is that we have two sets of weights and biases. One of these is for the initial matrix multiplication with the input data set and has N columns for the N learned features.\nThe second set of weights is just a single column vector with the same length as N, that gets multiplied with the learned features to give the final output.\nThe main difference between calculating the gradients in a linear regression and in this neural network is that we must calculate the partial derivatives for both sets of weights and biases.\nAs before, the caveat with calculating the gradient of the biases requires us to sum along the row axis to represent the same bias number being added to each observation.\nThe diagram below shows the overall gradient calculation process - as before we calculate the partial derivatives of each function at the outputs, and using the chain rule, multiply them together to get the final gradient.\nThe following table shows what each partial derivative is. The logic here is already covered previously in chapter 1, with the sigmoid function being explained earlier in this post.\nIn the following table, we use the function ones(X) which is terminology from the Python API and simply takes a matrix X and produces a matrix where each element has the value 1, with the same shape as X (same number of rows and columns).\nNote that in the table the function ^ is actually written as A because I couldn\u0026rsquo;t manage to get the carat symbol to escape properly in LaTeX.\n   Derivative Calculation     $$\\frac {\\partial A} {\\partial P}(P, y)$$ $$-(y - P)$$   $$\\frac {\\partial \\alpha} {\\partial M_2}(M_2, B_2)$$ $$ones(M_2)$$   $$\\frac {\\partial \\alpha} {\\partial B_2}(M_2, B_2)$$ $$ones(B_2)$$   $$\\frac {\\partial v} {\\partial W_2}(O_1, W_2)$$ $${O_1}^T$$   $$\\frac {\\partial v} {\\partial O_1}(O_1, W_2)$$ $${W_2}^T$$   $$\\frac {\\partial \\sigma} {\\partial N_1}(N_1)$$ $$\\sigma(N_1) \\times (1 - \\sigma(N_1))$$   $$\\frac {\\partial \\alpha} {\\partial M_1}(M_1, B_1)$$ $$ones(M_1)$$   $$\\frac {\\partial \\alpha} {\\partial B_1}(M_1, B_1)$$ $$ones(B_1)$$   $$\\frac {\\partial v} {\\partial W_1}(X, W_1)$$ $$X^T$$    Code The code is located HERE and is a modification of the previous post\u0026rsquo;s code to extend in the ways described here.\nThe data set is the same so the errors that we get after training can be compared to the errors we got with a simple linear regression.\nIf the neural network is able to fit the data better then the final total error should be lower.\nConclusion Surprisingly after implementation, it looks like for this data set the neural network is actually worse. As I get the following output:\nFinal error (training): 0.20948163078668652\rFinal error (testing): 0.2119911310559618\rThis shows that in fact the training error turns out worse than the linear regression method (though very slightly).\nWhen tested on the testing data, it looks like there\u0026rsquo;s a noticable gap in the error between training and testing.\nI\u0026rsquo;m not sure what\u0026rsquo;s causing it however I can speculate on two things:\n We get around 0.2 error in all cases both for the neural network and the linear regression methods. The neural network has possibly overfit the training data and so doesn\u0026rsquo;t perform so well on the testing data.  This kind of variance is to be expected on certain data sets and neural networks will really shine with more data.\nI have no reason to believe the implementation is incorrect at this point so won\u0026rsquo;t dwell on this too much and instead will continue on with the abstractions portion of the book.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/neuralnetworks/","summary":"The previous post showed the most basic type of machine learning and how to train it, but the concepts learnt there (forward output calculations, backward gradient calculations) will apply equally here too.\nWhat was the problem with linear regression? The main problem with linear regression that we have is that we are training to find a set of weights, which are essentially the coefficients of an N-dimensional line. Therefore we\u0026rsquo;re trying to fit a straight line to a set of points.","title":"2.3 - Neural Networks"},{"content":"Introduction As mentioned previously, we can represent a training set as a matrix where each row of the matrix is a record/sample, and each column is a single feature of the record.\nAdditionally we can have a weighting assigned to each feature so some features are \u0026ldquo;worth\u0026rdquo; more than others.\nHowever we also require a baseline value for when every numeric feature is a 0 (we would always give 0, but we may not want this).\nThe formula for calculating the target/output value for a given record is then given as the following which is known as a linear regression.\n$$ y_i = {\\beta}_0 + {\\beta}_1 \\times x_1 + \u0026hellip; + {\\beta}_n \\times x_k + \\epsilon $$\nIn this, we have a vector of weights for features and a vector of the actual values for a record that we can add together to get the final target value for the record.\nHowever we also include a base term (beta 0) at the beginning which is the value we use when all features of the record are 0. This lets us shift the line of the regression to better fit the samples.\nCalculating predictions without the intercept The first step is to think of how we can generate our \u0026ldquo;predictions\u0026rdquo; for a given set of feature weights, and a given batch of samples. At first it helps to look at this calculation without the additional \u0026ldquo;intercept\u0026rdquo; (the base term we add to the weighted sum).\nWithout the intercept, the calculation is simply a dot product, or matrix multiplication between the vector of feature weights, and the matrix batch of samples. This will produce a vector of predictions.\nEach element of the prediction vector will be a dot product between the feature weight vector and the corresponding row of the batch.\nThis is represented as:\n$$ P_{batch} = X_{batch} \\times W = \\begin{bmatrix} X_{11} \u0026amp; X_{12} \u0026amp; X_{13} \u0026amp; \u0026hellip; \u0026amp; X_{1k} \\\\ X_{21} \u0026amp; X_{22} \u0026amp; X_{23} \u0026amp; \u0026hellip; \u0026amp; X_{2k} \\\\ X_{31} \u0026amp; X_{32} \u0026amp; X_{33} \u0026amp; \u0026hellip; \u0026amp; X_{3k} \\end{bmatrix} \\times \\begin{bmatrix} W_1 \\\\ W_2 \\\\ W_3 \\\\ . \\\\ . \\\\ . \\\\ W_k \\end{bmatrix} = \\begin{bmatrix} P_1 \\\\ P_2 \\\\ P_3 \\end{bmatrix} $$\nCalculating error Once we have a way to calculate our predictions, we do need a way to know how \u0026ldquo;good\u0026rdquo; those predictions are. This is just a measure of how close they are to the target/measured numbers that we already have.\nA good measure of this is the \u0026ldquo;Mean Squared Error\u0026rdquo; which is simply that we take the sum of the squared differences between the predictions and our target numbers, and divide by the number of entries to get the average difference.\nA \u0026ldquo;perfect\u0026rdquo; set of weights would give us a difference of precisely 0, so the closer we get to 0 the better.\nThis \u0026ldquo;Mean Squared Error\u0026rdquo; function takes a vector of predictions and a vector of targets and produces a single number as output and is given by the formula:\n$$ MSE(P_{batch}, Y_{batch}) = MSE(\\begin{bmatrix}P_1 \\\\ P_2 \\\\ P_3\\end{bmatrix}, \\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ Y_3\\end{bmatrix}) = \\frac {(Y_1 - P_1)^2 + (Y_2 - P_2)^2 + (Y_3 - P_3)^2} 3 $$\nA diagram We can visualise the above with a little diagram. In this diagram we take the batch matrix which we call X, and the weights which we call W.\nWe apply the matrix multiplication in an operation we call V to get the prediction vector P.\nAfter we have the prediction vector P, we take in our target/result vector Y and perform the \u0026ldquo;Mean Squared Error\u0026rdquo; operation.\nWe represent this function with the ^ character, and the output is a single error value known as L.\nAdding the intercept In order to calculate the error with the intercept (bias) included. We simply perform a scalar addition of the bias value to each element of the prediction vector just before we calculate the error.\nThis operation is represented with the alpha symbol in the below diagram, and the bias term represented with the character \u0026lsquo;B\u0026rsquo; (a scalar not a vector here).\nCalculating the partial derivatives for W and B Now that we have a way to go from the input parameters to a single value indicating our total deviation from the actual values, we need to somehow use that value to update our weights and bias terms such as to minimize L and bring it towards 0.\nWe can do this using everything from chapter 1 that we learned about partial derivatives.\nWe can figure out how much a change in each of the elements of the weighting vector, or the intercept term will cause a change in L and use that to modify those terms appropriately.\nWe will work backwards when calculating the partial derivatives here (using MSE instead of ^ to indicate the mean squared error function due to LaTeX issues).\nPartial derivative of L (with respect to P) For each element in Y and P (they\u0026rsquo;re the same size) then the partial derivative of the associated input element with respect to that pair is defined as the squared difference.\nSumming the values and dividing by the number of elements is an operation that occurs afterwards and isn\u0026rsquo;t affected by the changing of any of the input values by any amount, so the partial derivative can eliminate the sum and the division.\nSince:\n$$ MSE(P, Y) = (Y - P)^2 $$\nWe can extend this by FOIL to give us:\n$$ Y_2 - (2 \\times Y \\times P) + P_2 $$\nWith this, we can see that for each element in P, if we are to increase it by a value of 1, then it has no effect on the squared Y term. For the squared P term we can use the power rule of differentiation and see that it will become (2*P).\nFor the central term (-2 * Y * P), raising P by a value of 1 will raise the total by -2 * Y.\nTherefore after this, we get:\n$$ \\frac {\\partial L} {\\partial P}(P, Y) = (-2 \\times Y) + (2 \\times P) = (-2 \\times Y) - (-2 \\times P) = -2 \\times (Y - P) $$\nDerivative of intercept addition The intercept addition operation is simply defined as\n$$ \\alpha = N + B $$\nThen clearly increasing the value of each element of N by one unit will increase the value of each output by 1 unit and so the derivative is just a vector of 1\u0026rsquo;s with the same shape as N.\nSince it\u0026rsquo;s simply an addition, then this is also the partial derivative with respect to B. Increasing the bias term by 1 unit also increases the elements of N by the same difference.\nTherefore both partial derivatives for intercept addition are just a vector of 1\u0026rsquo;s of the same shape.\nDerivative of matrix multiplication (with respect to W) As described in chapter 1, when we have a matrix multiplication and want to calculate the derivative with respect to one of the operands, that the result is just the transpose of the other operand.\nTherefore:\n$$ \\frac {\\partial v} {\\partial W}(X, W) = X^T$$\nFinal products Now that we can calculate the partial derivatives, we can calculate the total derivative using the chain rule with respect to both W and B.\nOne thing to note, however is that B is a single float value, whereas the partial derivative of L with respect to P is a vector (since each element of P contributes to a change in L). We need to take into account that the bias, B is applied to each element of N to produce P. To account for the bias being applied to each element we sum to get the final derivative of L with respect to B\nThe total derivatives can be calculated therefore by the following formulas:\n$$ \\frac {\\delta L} {\\delta W} = {\\frac {\\partial N} {\\partial W}} \\times {{\\frac {\\partial P} {\\partial N}} \\times {\\frac {\\partial L} {\\partial P}}} $$ $$ \\frac {\\delta L} {\\delta B} = sum({\\frac {\\partial L} {\\partial P}} \\times {\\frac {\\partial P} {\\partial B}}) $$\nUsing the derivatives to train Now that we\u0026rsquo;ve passed the data through the formula to get the mean squared error (L), and then done a backwards pass to get the derivatives of L with respect to W and B, we need to update W and B themselves in order to bring L towards 0.\nThis is a simple process and just involves us subtracting some proportion of the derivative to W and B. This proportion/multiplier is known as the learning rate and makes it so we don\u0026rsquo;t make too large a jump each time in the weights.\nFor training, we simply repeat the forward pass, backward derivative calculation, and weight updating over and over in a loop for a number of iterations known as epochs\nTraining set and testing set There is a problem when it comes to training linear regressions and neural networks of overfitting. This is the problem where a set of weights have been determined that work very well in producing the target output for a given entry in the sample, but which doesn\u0026rsquo;t then generalise and correctly predict for entries that haven\u0026rsquo;t been seen yet.\nIn order to solve this issue we break up the data set into two sets.\nThe training set is a data set that is used to train the weights - it\u0026rsquo;s the data that is passed through the formula, and for which we calculate derivatives to update the weights.\nThe testing set is a data set that is not used to update the weights, and is data that the network/regression hasn\u0026rsquo;t had passed through it at all yet. This lets us test whether the trained network will generalise well enough to entries that it hasn\u0026rsquo;t seen yet.\nCode Example The coded example of the above linear regression concepts can be found in the zip file HERE. As with most of the code I write nowadays, it is in Rust and will require installation of Cargo to run. The easiest way to install Rust and Cargo is through Rustup\nThe data set I am using for this linear regression can be found HERE and is a set of features about people applying for credit, along with whether they are deemed \u0026ldquo;good\u0026rdquo; or \u0026ldquo;bad\u0026rdquo; for the purposes of credit (the target feature).\nIt should be fairly easy to understand and is just a codification of the above method with a few points to note:\n Instead of training for a number of epochs, I\u0026rsquo;m training until the delta between the previous and new mean squared error values drops to within a given threshold (it\u0026rsquo;s settling around the final position). I am dividing the input data by the maximum value in the array to bring all values within 0 and 1 since the initial large values cause the squared error calculation to go off to infinity. Since the target values are 0 (bad) and 1 (good) we want all the values and weights to be close to this too. I split the input data into 3/4 training data and 1/4 testing data. Once training has determined the final weights and intercept, I apply to the testing data to see if there\u0026rsquo;s a similar error when applied to the data the system hadn\u0026rsquo;t seen yet.  After running this code, I got these values (on this particular run):\nFinal error (training): 0.20486995882901385\rFinal error (testing): 0.20561283878025513\rThis shows that the linear regression at least seems to work with this data set as there\u0026rsquo;s a very small difference in error between the training batch and the testing batch.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/linearregression/","summary":"Introduction As mentioned previously, we can represent a training set as a matrix where each row of the matrix is a record/sample, and each column is a single feature of the record.\nAdditionally we can have a weighting assigned to each feature so some features are \u0026ldquo;worth\u0026rdquo; more than others.\nHowever we also require a baseline value for when every numeric feature is a 0 (we would always give 0, but we may not want this).","title":"2.2 - Linear Regression"},{"content":"Overview Supervised learning is a branch of machine learning that can be described as finding patterns or relationships in data that has already been measured. The \u0026ldquo;supervised\u0026rdquo; part of the phrase comes from the fact that we need to monitor the output that the system gives for a given input and to tell the system how close or far away it is from the measured result.\nAs a short example, suppose we looked at 100 houses and measured both their square footage and market value. We know both the input (square footage) and the output (market value), however we want to use machine learning to discover the relationship between square footage and market value in a way that can be applied to houses not in this training set.\nWe give the system one of the measured square footage values, it gives us an output that it calculates as the market value. Since we already measured the market value in the sample, we can tell the system how close it is (or far away) from the answer we expected and it can correct.\nThis will go around in cycles through all samples in a process called training, until the system reliably gives us an answer for each input that is within a certain tolerance of the correct answer. At this stage we can consider the system trained and can apply it to new inputs.\nRepresentation Once we have the set of features that we have measured (a feature is simply one measurement about a sample that we care about. These must be numerical and represent anything, such as for example, the square footage of a house, or it\u0026rsquo;s price).\nWe can represent our sample set as a matrix of numbers, with each column being a single feature of a sample, and each row in the matrix being a single record.\nSince we can represent the sample set as a 2D matrix of numbers, we can then apply the mathematics of calculating the derivitives of functions, and any other mathematical operations on the set in a homogenous way, regardless of what those numbers actually mean. It\u0026rsquo;s just data and the interpretation we apply to the matrix makes it information.\nGoal The goal of supervised learning is to discover relationships in input data that map it to some output data. In practice, the output will be a single feature of the measurements called the target.\nIn the case of the set of data about houses, the target may be selected to be the price of the house. This will let us train the system so that given the other input data (the features about the house), we can predict the price of the house. Thus, uncovering the relationships between features like square footage or number of bedrooms, and the price.\nA More Accurate Definition After the above descriptions, we can now define a more accurate definition of supervised learning.\nThe goal of supervised learning is to find a mathematical function that takes an input matrix (set of samples) and produces a vector as output that is close enough to a target vector.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/supervisedlearning/","summary":"Overview Supervised learning is a branch of machine learning that can be described as finding patterns or relationships in data that has already been measured. The \u0026ldquo;supervised\u0026rdquo; part of the phrase comes from the fact that we need to monitor the output that the system gives for a given input and to tell the system how close or far away it is from the measured result.\nAs a short example, suppose we looked at 100 houses and measured both their square footage and market value.","title":"2.1 - Supervised Learning"},{"content":"Since setting up my Github workflows as detailed HERE, I have noticed that the code coverage report generation tool that was being used was generating a lot of false negatives and certain code (mainly .except calls) were causing a \u0026ldquo;Not covered\u0026rdquo; status to be reported.\nTarpaulin is an alternative tool that has better integration with Rust, however the downside of it is that due to the instrumentation required it\u0026rsquo;s only available on x86_64 processors and only on the Linux OS.\nLuckily however, Github workflows allow running on exactly that architecture. This is a very simple change to make, and uploading the codecov report to CodeCov is still the same, the only difference is how the report has been generated.\nThe new workflow is as follows:\nname: Test Coverage on: [push] jobs: test: name: coverage runs-on: ubuntu-latest container: image: xd009642/tarpaulin:develop-nightly options: --security-opt seccomp=unconfined steps: - name: Checkout repository uses: actions/checkout@v2 - name: Generate code coverage run: | cargo +nightly tarpaulin --verbose --all-features --workspace --lib --timeout 120 --out Xml - name: Upload to codecov.io uses: codecov/codecov-action@v2 with: # token: ${{secrets.CODECOV_TOKEN}} # not required for public repos fail_ci_if_error: true The main difference to previous is that we use a container image for running Tarpaulin on nightly, on a specific snapshot, and that to generate the code coverage, we use the one liner:\ncargo +nightly tarpaulin --verbose --all-features --workspace --lib --timeout 120 --out Xml\rOnce the report is output to the XML file, the upload to Codecov remains the same!\nThe result is that the code coverage report seems to be a lot more reliable, and also the setup and running of Tarpaulin is so much easier too.\nFor more information, Tarpaulin can be found HERE\n","permalink":"https://forgottenmaster.github.io/posts/rust/infrastructure/tarpaulin/","summary":"Since setting up my Github workflows as detailed HERE, I have noticed that the code coverage report generation tool that was being used was generating a lot of false negatives and certain code (mainly .except calls) were causing a \u0026ldquo;Not covered\u0026rdquo; status to be reported.\nTarpaulin is an alternative tool that has better integration with Rust, however the downside of it is that due to the instrumentation required it\u0026rsquo;s only available on x86_64 processors and only on the Linux OS.","title":"Tarpaulin"},{"content":"I recently found out how to add appropriate GitHub action workflows to a Rust project in a GitHub repository. It was surprisingly easy to do with GitHub actions, and even lets you put a badge in the README.md file.\nTemplate Repository Luckily I stumbled upon this template repository containing exactly the GitHub workflows that I needed. In order to add the appropriate workflows to a Cargo project, clone this repository. The repository is located at: https://github.com/BamPeers/rust-ci-github-actions-workflow\nClick \u0026ldquo;Use this template\u0026rdquo; to create a new repository from it:\nAfter this you\u0026rsquo;ll be redirected to a page to setup the new repository as usual:\nCreating a CodeCov account CodeCov is a fantastic site that hosts code coverage reports generated by tools (or in this case, by the GitHub workflows supplied by the template repository we used). It\u0026rsquo;s free to use if either there are less than 5 users, or it\u0026rsquo;s considered an open source repository.\nCreating an account linked to your GitHub account is easy, just log in using your GitHub credentials, and authorize the app:\nSeeing your reports Once your CodeCov account is linked with GitHub, and the GitHub workflows run to produce your code coverage report, you can see those reports on the main dashboard:\nClicking on one of the repositories here will take you to a report overview page for that repository, showing the current/last run of code coverage in various formats:\nThe Sunburst diagram on the left hand side is interactable and lets you see which files have good or bad coverage at a glance.\nAt the bottom there is a directory navigation panel which also displays the code coverage percentage, and lets you drill down into each individual file. Clicking on an individual file will take you to a source code display and show exactly which lines are covered and which are not:\nAnd you can see from this view, that only 1 line is red (uncovered by tests).\nBadges! Adding badges to the README.md file of your repository that reflect the current state of the workflows and/or CodeCov is easy. For the workflows, navigate to the workflows panel again, and click on an individual workflow, then the hamburger menu, and finally the \u0026ldquo;Create Status Badge\u0026rdquo; option:\nYou\u0026rsquo;ll then be presented with a popup with the markup that you can copy/paste into the README.md file\nFor CodeCov, the badge is just as easy to find. First click on one of the repositories, then Settings, and finally Badge. See the below screenshot as an example:\nPop in the README.md Finally pop these markup codes into the README.md file of your repository and now everybody can see the status of your workflows and code coverage!\n","permalink":"https://forgottenmaster.github.io/posts/rust/infrastructure/githubworkflow/","summary":"I recently found out how to add appropriate GitHub action workflows to a Rust project in a GitHub repository. It was surprisingly easy to do with GitHub actions, and even lets you put a badge in the README.md file.\nTemplate Repository Luckily I stumbled upon this template repository containing exactly the GitHub workflows that I needed. In order to add the appropriate workflows to a Cargo project, clone this repository. The repository is located at: https://github.","title":"Github Workflows"},{"content":"What is constant evaluation? Constant evaluation is basically just performing calculations at compile time rather than runtime. There are some constructs that we would like to represent with a high level programming language such as Rust or C++, but for which, if we know the parameters to the function at compile time, then we can also calculate the output at compile time as well.\nOne thing to note is that it\u0026rsquo;s not possible to do everything at compile time since we generally require I/O, and whenever I/O is involved it can no longer be performed or baked in at compile time. However we can isolate sections of the program that are able to be executed by the compiler and ensure they can be executed in a constant evaluation environment.\nWhy is it needed? Most of the time, source code is compiled by a compiler into machine code which then executes on the target machine. The compiler performs checks such as type checking, and when it outputs the machine code has additional duties such as filling in function pointers to virtual functions, etc. The source code provides a nice high level way to code using types, interfaces, and other such concepts, and the compiler has the job of turning that into CPU instructions.\nHowever, the more CPU instructions are required, the slower the execution will be. Usually we are trying to target a high frame rate and are therefore limited in the time we can spend on each frame. Every operation performed by the CPU will eat up this budget.\nThe more calculations we can move from runtime to compile time, the better the execution will be. Additionally since the program is compiled generally much less than it\u0026rsquo;s executed, it would be a better trade-off of time to have a longer compile time but shorter run time.\nAnother point is that if calculations are done at compile time, with the results stored in the binary itself in the data section, older hardware will be able to run it due to the lower load on the CPU required.\nHow do we do this in Rust? Compared to a language like C++, Rust has comparatively weaker compile time capabilities, but these capabilities are being expanded all the time due to the open source nature of the language. However, compared to other languages that have no compile time/const evaluation capabilities at all, the features we have access to are decent enough.\nThere are four main building blocks for constant evaluation in Rust. The most common method of metaprogramming is the use of macros.\nMacros A macro in rust looks like a function, except that instead of taking values as parameters, and producing a value as an output, it takes code as input and produces replacement code as output.\nIn languages without a deep macro system such as C++, macros are often implemented as simple textual substitutions with no knowledge of the types of syntax accepted.\nIn Rust there are actually 2 different types of macros:\n Declarative macros - these look a lot like functions in Rust, but are different, and identified as a macro by the little \u0026ldquo;!\u0026rdquo; that Rust requires be placed before the opening parenthesis. These are more powerful than macros in C++ for example, but more limited than the second form. Procedural macros - these are compiled in their own separate library and imported to be used, but these have access to the entire Abstract Syntax Tree (AST) that they\u0026rsquo;re decorating. Whereas declarative macros can only add/expand syntax and can\u0026rsquo;t remove syntax, proc macros can rewrite the entire item they\u0026rsquo;re attached to. These are more similar to function decorators in Python.  There\u0026rsquo;s too much to macros to explain here, so i\u0026rsquo;ll do a separate post on them which will let me explore them deeper myself too. However, for a quick example of a macro, here\u0026rsquo;s one that can create a HashMap for us:\nmacro_rules! hashmap { // This case handles the logic and is used when the parameter list does NOT  // have a trailing comma (e.g. hashmap!(22 =\u0026gt; \u0026#34;twenty-two\u0026#34;, 35 =\u0026gt; \u0026#34;thirty-five\u0026#34;))  ($($key:expr =\u0026gt; $value:expr),*) =\u0026gt; { { let mut hm = ::std::collections::HashMap::new(); $(hm.insert($key, $value);)* hm } }; } fn main() { let hm = hashmap!(\u0026#34;one\u0026#34; =\u0026gt; 1, \u0026#34;two\u0026#34; =\u0026gt; 2, \u0026#34;three\u0026#34; =\u0026gt; 3); } Generics Generics allow for complex composite types to be defined that are known in their entirety at compile time, but which the compiler might be able to compile down to a no-op in certain cases. We call these zero-cost abstractions and they\u0026rsquo;re quite common in Rust. It\u0026rsquo;s common to define a transparent wrapper around another type, for example:\n#[repr(transparent)] struct NewType\u0026lt;T\u0026gt;(T); // NewType is a wrapper around T  In the above snippet we can make a wrapper type, and we can compose it with other generic types as needed to be able to give information to the compiler for reasoning, and then have that entirely compiled away at runtime so there\u0026rsquo;s no overhead.\nThe next tool in our compile time toolbox in Rust is the const function\nConst function Whereas macros allow us to expand or replace syntax, const functions let us write familiar looking (albeit very restricted) functions in a way that let them be executed at compile time if the parameters are constant.\nIn a macro, the syntax expansion is performed at compile time but the resulting syntax is still only executed at runtime, which allows us to still make use of heap-allocated structures and such.\nIn a const function, the function itself is executed at compile time (if the parameters are constant), but can still be run at runtime (if the parameters are non-constant).\nHowever because we must write the function with the possibility that it\u0026rsquo;s executing at compile time, we have a whole host of things we can\u0026rsquo;t make use of. In addition to not being able to heap allocate structures, we can\u0026rsquo;t for example, make use of trait bounds or traits in a const function.\nDue to these limitations, we have to code in a more low level and possibly unsafe way than we have before, but this is something I\u0026rsquo;ll be exploring in the upcoming posts.\nAs a simple example, the following is a const function that takes two integers and adds them together:\nconst fn add_together(a: u32, b: u32) -\u0026gt; u32 { a + b } Const keyword The const keyword identifies to the compiler that this is a const context. The left-hand side of a const declaration must explicitly state the type (no type inference). The right-hand side of a const declaration must be entirely calculable at compile time.\nA function written as a const function (as the example add_together function above) can also be invoked at runtime with parameters deduced at runtime, however we can force it to be executed at compile time by introducing a const context as such:\nconst ADDED: u32 = add_together(add_together(1, 2), add_together(3, 4)); The compiler will execute this at compile time and the resulting binary will simply have the result (10) embedded.\nLimitations The limitations of writing code to be executable within a constant context are detailed HERE\nHowever the main limitations are:\n No heap allocations (no allocator present in the compiler) No floating point operations (floats aren\u0026rsquo;t reliable enough for use in the compiler) No bounds on generics in const functions No comparing raw pointers (pointers don\u0026rsquo;t exist in the same way in the compiler)  The other big limitation will be that large parts of the standard library aren\u0026rsquo;t executable in constant contexts and that we will have to find other means to build the same functionality.\nThere are crates that have been developed to add this functionality into a constant context but for the sake of education, we\u0026rsquo;ll be developing this kind of functionality ourselves into a package called consteval\n","permalink":"https://forgottenmaster.github.io/posts/rust/consteval/intro/","summary":"What is constant evaluation? Constant evaluation is basically just performing calculations at compile time rather than runtime. There are some constructs that we would like to represent with a high level programming language such as Rust or C++, but for which, if we know the parameters to the function at compile time, then we can also calculate the output at compile time as well.\nOne thing to note is that it\u0026rsquo;s not possible to do everything at compile time since we generally require I/O, and whenever I/O is involved it can no longer be performed or baked in at compile time.","title":"Introduction"},{"content":"Overview We often find ourselves wanting to find a smaller sequence, inside of a larger sequence. This often manifests as searching for a substring in a larger text, or determining if a list of numbers is a sublist of another.\nWe can achieve this by looking at a sequence of characters at the beginning of the larger sequence (known as the haystack) that is the same length as the smaller sequence (known as the needle). This sliding motion can be viewed as the following animation\nHowever, with the Naiive approach, when we compare the needle against a specific window in the haystack, we always start by comparing the first elements, then the second elements, and so on until we either hit a mismatch, or have compared all elements as equal.\nThis is obviously a problem for larger sequences as, every time the window slides one position to the right in the haystack, we can end up checking every character in the needle.\nHow is Rabin-Karp different? The basic movement of the \u0026ldquo;window\u0026rdquo; that we\u0026rsquo;re checking in the haystack is still the same - it slides right by one position each step. However where Rabin-Karp differs is by avoiding checking each character to find a mismatch if it knows a mismatch occurs. The way it achieves this is by using a single numeric hash code that represents the sequence of elements we are looking for (known as the fingerprint).\nThe fingerprint of the initial window can be calculated as usual by hashing all the elements in the window. However, where the efficiency comes from with Rabin-Karp is that the method used to calculate the fingerprint allows us to \u0026ldquo;roll\u0026rdquo; that fingerprint each time we move the window to the right by one step.\nWhat this means is, when we move the window to the right by one step, we drop whatever contribution to the overall hash was made by the leftmost element, and then incorporate the new element that\u0026rsquo;s coming in on the right hand side of the window - without having to recalculate all the elements in between.\nHow can it do this? We can do this by multiplying the hash code of each element by a particular, unique number and adding all the values together for a unique fingerprint.\nWe need to first choose a number as a base which we will raise to consecutive powers to determine these multipliers. It doesn\u0026rsquo;t matter too much what this number is, except that it must be greater than 1 (because raising 1 to any power gives 1 which isn\u0026rsquo;t a unique multiplier).\nThe initial fingerprint Given that B is the arbitrary numerical base we will raise, W is the window we\u0026rsquo;re calculating the fingerprint for, and L is the length of that sequence, then the formula is given as follows:\n$$ Fingerprint(W) = \\sum_{n=0}^{L-1} Hash(W_n) \\times B^{L-1-n} $$\nOr, written another way:\n$$ Fingerprint(W) = (Hash(W_0) \\times B^{L-1}) + (Hash(W_1) \\times B^{L-1-1}) + (Hash(W_2) \\times B^{L-1-2}) $$ $$ + \u0026hellip; + (Hash(W_n) \\times B^0 ) $$\n A concrete example It\u0026rsquo;s always difficult to interpret mathematical formulas sometimes, so here is a small and concrete example of calculating the hash code. Given the sequence of integers W=\\([17, 23, 49, 51]\\) we can see that L=4, and if we pick as our base B=2 then in order to calculate the fingerprint, we simply do the following:\n$$ Fingerprint(W) = (Hash(17) \\times 2^3) + (Hash(23) \\times 2^2) + (Hash(49) \\times 2) + Hash(51) $$\nIf we furthermore take Hash(X) to be X itself (as they\u0026rsquo;re already integers), then we can easily compute the final fingerprint as:\n$$ (17 \\times 8) + (23 \\times 4) + (49 \\times 2) + 51 $$ $$ = 136 + 92 + 98 + 51 $$ $$ = 377 $$\n Rolling In order to update the fingerprint after the initial calculation, we will need to do the following in sequence:\n Subtract the highest power term on the left Multiply the fingerprint by B - this has the effect of raising the remaining powers by 1 Add the new term to the fingerprint  Let\u0026rsquo;s look at these in turn\u0026hellip;\nSubtract the highest power term When we slide the window to the right, the elements on the left of the window (and the left of the above formula) will \u0026ldquo;drop off\u0026rdquo;. As we have raised each element to successive powers, we know how to calculate the multiplier added to the hash of element 0. The multiplier is simply \\(B^{L-1}\\), and the value we will subtract from the fingerprint is \\(Hash(W_0) \\times B^{L-1})\\)\nIn the concrete example shown above, we subtract \\((Hash(17) \\times 2^3)\\) and are left with:\n$$ Fingerprint(W) = (Hash(23) \\times 2^2) + (Hash(49) \\times 2^1) + (Hash(51) \\times 2^0) $$\nRaise the powers We now multiply by B (in the concrete example this is 2). Since the following is true:\n$$ (X + Y) \\times Z \\equiv (X \\times Z) + (Y \\times Z) $$\nThis has the effect of multiplying each term in the formula by B, which furthermore, has the effect of raising the power by 1 of each term.\nAfter multiplying by 2, the concrete example will be given as:\n$$ Fingerprint(W) = (Hash(23) \\times 2^3) + (Hash(49) \\times 2^2) + (Hash(51) \\times 2^1) $$\nAdd the new term This is the easiest part. Since the powers of the other terms have been raised, we only need to add the new term into the \u0026ldquo;zeroeth\u0026rdquo; power position, which is simply adding the hashcode of the element.\nIn our concrete example, say that we are rotating in a new element, 101, on the right as the window slides. The new fingerprint is represented as:\n$$ Fingerprint(W) = (Hash(23) \\times 2^3) + (Hash(49) \\times 2^2) + (Hash(51) \\times 2) + Hash(101) $$\nWhich, again assuming Hash(X) is X, gives the new fingerprint of:\n$$ (23 \\times 8) + (49 \\times 4) + (51 \\times 2) + 101 $$ $$ = 184 + 196 + 102 + 101 $$ $$ = 583 $$\nBut what about overflow? As you can see however, from this formula that for large sequence lengths, even for the smallest viable base of 2, will result in an integer growing too large to fit in any sensible datatype.\nAfter all \\(2^{500000}\\) is\u0026hellip;..huge\nWhat can we do to keep these numbers small?\u0026hellip;.Modular Arithmetic\nModular Arithmetic Using modular arithmetic involves using integer division of the number at each step by some known value (known as the modulus) and retaining the remainder. In this way all the numbers \u0026ldquo;wrap around\u0026rdquo; but the formula is still valid, it\u0026rsquo;s just being done on a number circle rather than a number line.\nAn illustration of this is with clocks. Clocks use a modulus of 12 before they wrap around, but you can still add, for example, 4 hours, or subtract 4 hours when that wraps around.\nAdapting the formulas is easy enough, we just apply the modulo operator when calculating the hash code of an element, and at every step of the calculation.\nThe tricky part of this however, is when we are calculating the powers - we can\u0026rsquo;t calculate the power and then apply the modulus as the power calculation overflows. Instead we need to recognise that:\n$$ pow(X, 3) \\equiv (X \\times X \\times X) $$\nAnd this lets us apply modulo at every step, as in (assuming modulus of M):\n$$ ((((X \\mod M) \\times X) \\mod M) \\times X) \\mod M $$\nOne thing we need to accumulate while doing this is the multiplier we have applied to the left most element (\\(B^L-1\\)) which we then can just use later when we\u0026rsquo;re subtracting the left hand term.\nCoding it up Following is a walkthrough of how to code this algorithm up in Rust, the complete source can be found at the playground link HERE\nI\u0026rsquo;ll run through the steps one by one below, explaining the code for those who may be unfamiliar with Rust syntax.\nStep 1 - HashCode trait In Rust we are able to get the hash of a type that implements the Hash trait for use in a HashMap. However this method requires passing the type through a Hasher and incurring a performance cost to generate a hash code for even basic types such as integers.\nSince an integer is its own hash code, and a character can be converted to an integer as a hash code we don\u0026rsquo;t need to do anything too tricky.\nFor the element hashing therefore, I\u0026rsquo;ve gone with creating a new trait:\ntrait HashCode { fn hash_code(\u0026amp;self) -\u0026gt; u64; } That is, a simple trait (interface in other languages) with a single function that operates on an const reference to the element (\u0026amp;self) and returns an integer hash code for it (u64).\nStep 2 - Implementing HashCode for types we want to use The next step is to actually implement the trait for the types we want to be able to use in the sequences for the algorithm. Rust allows us to implement custom traits for existing types in order to extend their functionality - even primitive types. For this, we will implement it for the following primitive types:\n i32 - A signed 32-bit integer. For this, if it\u0026rsquo;s a positive value we can simply keep the value and cast it to a wider, 64 bit integer with no loss. If it\u0026rsquo;s negative, because we know that the range of a 64-bit integer covers the range of the 32-bit integer then we will map the negative portion of i32 by negating it - and then subtracting from u64::MAX. u32 - This involves a simple widening cast which is safe since we know all unsigned 32-bit integer values will fit into an unsigned 64-bit variable. u64 - For this, the hash code is the value itself so it simply returns it. char - Characters in rust are 4 bytes large representing a unicode codepoint. For the same reason as u32 will be safely castable into u64, so are characters.  The implementation of the HashCode trait then is as follows:\nimpl HashCode for i32 { fn hash_code(\u0026amp;self) -\u0026gt; u64 { if *self \u0026gt;= 0 { *self as u64 // if we\u0026#39;re positive, just keep our value and cast as a u64  } else { std::u64::MAX - self.abs() as u64 // if we\u0026#39;re negative, just negate it and then subtract from u64 MAX to use the upper end  } } } impl HashCode for u32 { fn hash_code(\u0026amp;self) -\u0026gt; u64 { *self as u64 } } impl HashCode for u64 { fn hash_code(\u0026amp;self) -\u0026gt; u64 { *self } } impl HashCode for char { fn hash_code(\u0026amp;self) -\u0026gt; u64 { (*self).into() } } Step 3 - Generating a fingerprint In order to be able to roll a fingerprint as described previously, we first need to construct one from scratch for a given range. Given a sequence of elements (that implement our HashCode trait), along with a base to use, and a modulus to keep the numbers small as described in the section on modular arithmetic - we would like a function that will generate the fingerprint for the sequence. Additionally, we want to return the calculated multiplier for that left-most term so that we don\u0026rsquo;t need to calculate it later.\nTo allow us to keep the numbers small, we don\u0026rsquo;t use the pow function, but use an imperative loop to accumulate the fingerprint and base offset.\nThe code for this function looks like the following (note that this assumes the list isn\u0026rsquo;t empty and will panic if it is due to the [0] access):\nfn generate_fingerprint\u0026lt;T: HashCode\u0026gt;(list: \u0026amp;[T], base: u64, modulus: u64) -\u0026gt; (u64, u64) { let mut fingerprint = list[0].hash_code() % modulus; let mut base_offset = 1; for elem in \u0026amp;list[1..] { let elem_hash = elem.hash_code() % modulus; fingerprint = (((fingerprint * base) % modulus) + elem_hash) % modulus; base_offset = (base_offset * base) % modulus; } (fingerprint, base_offset) } Step 4 - Rolling the fingerprint In order to \u0026ldquo;roll\u0026rdquo; the fingerprint and generate the next one from the previous one, we need the following pieces of information:\n The previous fingerprint - for obvious reasons The calculated multiplier (called the base offset) for the left-most element - we use this to calculate the final value to subtract from the fingerprint The old element we\u0026rsquo;re rotating out - we need this to get the hash code from, which is combined with the base offset to get the value to subtract The new element we\u0026rsquo;re rotating in - we need this again to get the hash code which will be added to the fingerprint The base - we need this to allow us to \u0026ldquo;raise the powers\u0026rdquo; of all the remaining terms in the fingerprint after removing the left most term The modulus - we need this for the same reason as we needed it in generate_fingerprint. It lets us perform modular arithmetic and keep the values small  One additional thing to note which could be missed is that before removing the term we want to be rid of, we must first add the modulus to the fingerprint. This is because we\u0026rsquo;re using modular arithmetic - it might wrap around so that the current fingerprint value is less than the value we want to subtract. Adding the modulus essentially adds one full rotation to the fingerprint - allowing us to subtract the term safely.\nThe function looks like the following:\nfn roll_fingerprint\u0026lt;T: HashCode\u0026gt;( mut fingerprint: u64, base_offset: u64, old_term: \u0026amp;T, new_term: \u0026amp;T, base: u64, modulus: u64, ) -\u0026gt; u64 { let old_term_hash = old_term.hash_code() % modulus; let new_term_hash = new_term.hash_code() % modulus; let term_to_subtract = (old_term_hash * base_offset) % modulus; fingerprint = fingerprint + modulus - term_to_subtract; // remove the old term after adding the modulus on to protect against underflow.  fingerprint *= base; // power shift all other terms up by 1  (fingerprint + new_term_hash) % modulus // return new fingerprint after adding in new term and modding } Step 5 - Putting it together Now we can write the actual Rabin-Karp implementation. This function will take a sequence known as the needle, and an equal or larger sequence known as the Haystack, along with the standard base and modulus to use.\nIt will return a boolean value indicating whether the needle was found in the haystack or not. This could be extended if necessary in the future to return the index of the match.\nFor Rabin-Karp, the steps are as follows:\n Calculate the fingerprint of the needle - this will never change and is the fingerprint we\u0026rsquo;re trying to match Handle the trivial case where the needle is empty - an empty needle is always present in any list Generate the initial fingerprint for the slice of the haystack of the same length as the needle - This is the starting fingerprint to match, and also gives us the base offset to use If they match already, then we return a match Otherwise we slide the window along by 1 each time (until the right side of the window hits the end of the haystack), rolling the hash each time to remove the old left hand element and add the incoming element Each time, check if there\u0026rsquo;s a match If we\u0026rsquo;ve checked all windows and not found a match, it\u0026rsquo;s not there  One thing to note though is that it is sufficient to detect a mismatch by mismatching fingerprints however it\u0026rsquo;s not sufficient to detect a match with matching fingerprints. This is because we\u0026rsquo;re using modular arithmetic and a mathematical formula for calculating the final fingerprint - it\u0026rsquo;s possible different sequences end up with the same fingerprint.\nThe efficiency of this algorithm is in the fact we don\u0026rsquo;t have to check elementwise when we know there\u0026rsquo;s a mismatch, and only have to check elementwise in the cases that the fingerprints match.\nBecause of this requirement that elements need to be compared as equal, our Rabin-Karp function requires an additional bound on it\u0026rsquo;s generic type, that of PartialEq (which lets us use the == operator). The final function is as follows:\nfn rabin_karp\u0026lt;T: HashCode + PartialEq\u0026gt;( needle: \u0026amp;[T], haystack: \u0026amp;[T], base: u64, modulus: u64, ) -\u0026gt; bool { if needle.len() == 0 { true // we can always find the empty list inside any list  } else { // get the initial fingerprints and window  let (needle_fingerprint, _) = generate_fingerprint(needle, base, modulus); let needle_len = needle.len(); let haystack_len = haystack.len(); let mut window = \u0026amp;haystack[0..needle_len]; let (mut window_fingerprint, base_offset) = generate_fingerprint(window, base, modulus); // check the initial fingerprints/window for match  if needle_fingerprint == window_fingerprint \u0026amp;\u0026amp; needle == window { return true; } // otherwise run a starting index for the window from 1 up to and including haystack_len-needle_len.  for window_index in 1..=(haystack_len - needle_len) { let new_window = \u0026amp;haystack[window_index..window_index + needle_len]; let roll_out = \u0026amp;window[0]; let roll_in = \u0026amp;new_window[needle_len - 1]; window_fingerprint = roll_fingerprint( window_fingerprint, base_offset, roll_out, roll_in, base, modulus, ); window = new_window; if needle_fingerprint == window_fingerprint \u0026amp;\u0026amp; needle == window { return true; } } // wasn\u0026#39;t found or we\u0026#39;d have returned true on a match.  false } } Testing the implementation The following code snippet is used to test the implementation of Rabin-Karp finds a match or not in the correct cases:\nfn main() { const BASE: u64 = 253; const MODULUS: u64 = 101; println!( \u0026#34;[2, 4, 1] in [7, 8, 2, 4, 1, 5] =\u0026gt; {}\u0026#34;, rabin_karp(\u0026amp;[2, 4, 1], \u0026amp;[7, 8, 2, 4, 1, 5], BASE, MODULUS) ); println!( \u0026#34;[2, 4, 1] in [7, 8, 2, 4, 3, 5] =\u0026gt; {}\u0026#34;, rabin_karp(\u0026amp;[2, 4, 1], \u0026amp;[7, 8, 2, 4, 3, 5], BASE, MODULUS) ); } We\u0026rsquo;re using relatively small sequences, and small modulus but this algorithm is pretty efficient when scaled up at large sizes. In my experiments, a haystack of length 1,000,000 and a needle of length 500,000 resulted in a benchmark time on the naiive algorithm of over 1 minute, whereas with the Rabin-Karp implementation benchmarked the same problem at around 13 milliseconds\nThe output from this is:\n[2, 4, 1] in [7, 8, 2, 4, 1, 5] =\u0026gt; true\r[2, 4, 1] in [7, 8, 2, 4, 3, 5] =\u0026gt; false\r","permalink":"https://forgottenmaster.github.io/posts/algorithms/searching/rabinkarp/","summary":"Overview We often find ourselves wanting to find a smaller sequence, inside of a larger sequence. This often manifests as searching for a substring in a larger text, or determining if a list of numbers is a sublist of another.\nWe can achieve this by looking at a sequence of characters at the beginning of the larger sequence (known as the haystack) that is the same length as the smaller sequence (known as the needle).","title":"Rabin-Karp Pattern Search"},{"content":"This post will cover the basic concepts of Git from a high-level perspective. It will touch the object store and the index, along with touching on how objects in a repository are stored. The next post will then cover the usage of the Git CLI.\nRepository A Git repository has 2 main storages:\n  The object store This storage is used to store the actual data, commits, etc. for the repository and is a permanent record of all the changes to the repository. This is efficiently copied across when cloning an existing repository to provide an exact duplicate of not only the data but the whole history.\n  The index This is a temporary store which records changes that have yet to be made to the repository but have been staged.\n  We\u0026rsquo;ll look at these in more detail now\nObject Store The object store portion of the repository stores the 4 main atomic types of objects that Git uses. These are:\n  Blobs - The contents of a single version of a file is represented as a Blob (a contraction of \u0026ldquo;binary large object\u0026rdquo;). A blob is simply the bytes of the file, along with a hash of the contents. Git doesn\u0026rsquo;t store any metadata along with the blob here, but that\u0026rsquo;s stored in other structures within the object store.\n  Trees - A tree is the information about 1 level of the directory structure. It contains references to all the blobs for the files in one directory, along with their path names and some metadata. It may also refer to child trees in order to build up a complete hierarchy by following the root level tree.\n  Commits - A commit object contains the author name/email, along with the commit data and the message that was provided to the log. Each commit points to a single tree object which is the root tree that allows the whole state of the repository to be reproduced for a given commit. Additionally, every commit except for the initial commit will have one (or more - in the case of branch merging) parent commits which allows the software to traverse backwards through the commit chain.\n  Tags - A tag object is an arbitrary string, usually human readable which is assigned to a specific object to allow for it to be referenced by a more readable string rather than the hash that\u0026rsquo;s generated for the object. This is usually used to give certain commits descriptive names, such as for assigning releases.\n  The Index The second storage in a repository is the index, which is a temporary binary file that captures a version of the project\u0026rsquo;s structure at some moment in time. This could be a reference to a given commit (from which we can traverse the tree objects and the blob objects to recreate the directory structure of the whole repository).\nThe developer is allowed to add and remove changes to the index in incremental steps, before finally applying the contents of the index to the object store.\nWhen one runs commands such as add, rm, or mv the changes are staged in the index in a quick and efficient way.\nThe more expensive persistence of the index is done only when the developer commits those changes to the repository.\nTherefore the index is a cache of sorts that allows adding/removing/restoring files as needed in a staging environment before finally committing the index to the repository, resulting in the more heavy processing required.\nThe index also plays an important part in merges, since it can store multiple copies of the same file for merging purposes.\nHashing Every object in the object store in Git is referenced by a unique identifier that represents it, and it only.\nIn the case of Git, this identifier is the SHA-1 hash of the contents of the blob.\nTherefore the object store is basically a hash map that allows efficient retrieval of an object from the store given its unique hash of the contents.\nThis is different to the way that most source control software will store and retrieve files. In others, the files are indexed by file path derived from the directory structure on disk. However Git indexes by hashing the content of the files, meaning if you have two hashes that are the same, you can guarantee that the contents of the blob are the same.\nAdditionally, and more powerfully, since Git stores the entire history as referenced from a head (most recent) commit, and this commit is an object in the store, just like the others, the hash of a commit or tree is derived from any child trees or commits.\nThis is a very powerful statement because it means that if you have two identifiers of two commits, and they are the same - not only do you know the same changes were made in those commits, but you know that the entire history of the repository was the same in both cases too.\nPack Files Git stores the complete contents of each version of every file, since it\u0026rsquo;s deriving the identifier of the file version from the contents of the data itself, it can only operate on entire and complete file contents.\nBut how can it store many different instances of the same file when they only differ by a few characters?\nGit solves this by using pack files, and it achieves it by storing the full contents of a certain blob, and other blobs that are close to it in content are stored alongside that as a diff.\nThus if you have version A of a file, and version B of a file which only has a single character changed, then the pack file will have the full contents of A, along with a diff with the changed character.\nHowever, because Git indexes data by the content itself, and doesn\u0026rsquo;t care about directory structure, it can store different blobs that are in no way related together in a pack file, as long as they\u0026rsquo;re close to each other in nature.\nIn order to be able to locate an object within a pack, Git will store the original SHA-1 hash for each alongside either the complete content or the delta for that content.\nObject Store Diagram A picture is worth a thousand words, as the saying goes, so below is a diagram of a sample object store - showing how blobs, trees, commits, and tags all reference one another.\nIn the diagram, the blobs, being the raw data, are located at the bottom of the diagram. Blobs are standalone objects and don\u0026rsquo;t reference anything, but are referenced by tree objects. These are represented as a rectangle.\nTree objects can reference blobs, and other child trees, along with metadata identifying the path on the user\u0026rsquo;s disk so that it can be reconstructed in the same way on disk. These are represented as a triangle.\nA circle represents a commit here, which may have one or more parent commits, allowing us to traverse backward through history (the root/initial commit has no parent), and points to a single tree, that represents the entire state of the repository at that commit.\nTags point to a commit, and are represented as a parallelogram.\nAs you can see in the diagram, Git stores references from child commits, to their parents. This makes it super easy to add a new commit, as you make the commit, and point it to the previous one - then the new commit becomes the head.\nFrom the diagram, we can see that commit #11235 has information about the author, the commit message, and the timestamp. Additionally has a reference to the parent (initial) commit.\nThe commit object references a tree that in turn, references two blobs, and a child tree referencing a blob.\nThe initial commit references a tree that references two of the same blobs (dead23 and feeb1e) as the new one, but doesn\u0026rsquo;t have access to tree #1010220.\nTherefore we can see from the diagram that the new commit added a file into a subfolder, but all the other files remain the same. Because Git indexes files by their content only, which is immutable, it\u0026rsquo;s able to refer to the same instances of the blobs.\nAccessing the object store You\u0026rsquo;re able to access any given object from the object store by it\u0026rsquo;s SHA-1 hash, which as we\u0026rsquo;re now aware, is the ID of the object (and the hash of the contents).\nWe can do this with the cat-file subcommand. The following is an example of using it to retrieve a data blob from the object store:\ngit cat-file -p 2d10\rText inside the file\rNote that we don\u0026rsquo;t need the full identifier of the blob, just enough of the prefix to allow Git to locate it for display.\nWe can access any object from the store this way, for example accessing a commit\ngit cat-file -p 7a97\rtree 17efabebf0f96829e91a5b9dc358b2e2c2eec05e\rparent 8bddb3945ff1f5978c57fc298c2ab3ba88945a83\rauthor Robin Firth \u0026lt;theforgottenmaster@googlemail.com\u0026gt; 1636230599 +0000\rcommitter Robin Firth \u0026lt;theforgottenmaster@googlemail.com\u0026gt; 1636230599 +0000\rAdded file only on branch\rAnd we can see that the commit has a reference to its tree object, parent commit, and the author metadata.\n","permalink":"https://forgottenmaster.github.io/posts/git/basic_concepts/","summary":"This post will cover the basic concepts of Git from a high-level perspective. It will touch the object store and the index, along with touching on how objects in a repository are stored. The next post will then cover the usage of the Git CLI.\nRepository A Git repository has 2 main storages:\n  The object store This storage is used to store the actual data, commits, etc. for the repository and is a permanent record of all the changes to the repository.","title":"Basic Concepts"},{"content":"This post will delve into the usage of the Git CLI from a user\u0026rsquo;s perspective. It will cover basic usage such as cloning, pulling, pushing, and will touch on how the commands interact with the index and object store of the repository.\nCreating a repository There are two different ways to establish a repository in Git in contrast to other version control systems where there is only creating the initial repository.\nIn Git we are also able to clone a repository to create copy containing all of the objects and history from an existing repository.\nInitialising a new repository This is how we can create a brand new Git repository which may either be empty, or contain data/files already.\nIn order to initialise a new repository we simply use the init subcommand while inside the root folder we want to turn into a repository. For example if we want to make a brand new one.\nmkdir repo\rcd repo\rgit init\rOf course, the repo folder need not be empty, and the steps would be the same to initialise an existing folder as a repository.\nCloning an existing repository Git is also able to easily make a clone of an existing repository which will copy all of the objects and history across with it, but allows branching and commits to this new repository independant of the source one.\nIn open source development, this is the most common way, as a developer will make a clone of the central repository in order to make their own fixes that they can then request to be merged back into the central/online repository.\nIn order to clone a repository we can do\ngit clone \u0026lt;path-to-repository\u0026gt; repo\rWhere path-to-repository can either be a relative or absolute path to another local repository on the machine or something more exotic such as a URL.\nThe most popular way to clone is using the GitHub URL to the repository.\nAdding and removing files In order to add and remove files to the repository, one can use the add and rm subcommands, along with a set of files to add or remove.\nWhen a file is added or removed however, it is not immediately added to the repository, or removed from the repository but only staged as a change to make to the repository.\nIn order to add a file that is either currently untracked by the repository, or has been edited and needs updating in the repository, we can do the following\ngit add file_to_add.txt\rAnd we can remove a file that is currently tracked by the repository also\ngit rm file_to_remove.txt\rThis will add or remove the file from the staged change as mentioned previously\nViewing staged changes We can use the subcommand status to display the list of files that have been staged to be added or modified, the list of files staged for removal, and the list of files that Git has found in the folder that aren\u0026rsquo;t tracked yet.\nThis last type is the state that a file will be in when you first create it and add it to the folder (or a subfolder), but before it\u0026rsquo;s been staged for addition with the add subcommand.\nFor example, after the following sequence of commands where we add a file, and then remove a file:\ngit add file_to_add.txt\rgit rm file_to_remove.txt\rWe will see the following when we ask for the status:\ngit status\rOn branch master\rChanges to be committed:\r(use \u0026quot;git restore --staged \u0026lt;file\u0026gt;...\u0026quot; to unstage)\rnew file: file_to_add.txt\rdeleted: file_to_remove.txt\rUntracked files:\r(use \u0026quot;git add \u0026lt;file\u0026gt;...\u0026quot; to include in what will be committed)\runtracked_file.txt\rCommitting staged changes Once we\u0026rsquo;ve added and removed files to the stage, and we are ready for the repository to be updated to reflect the changes, we will need to perform a commit operation.\nWhen we commit to the repository, we are required to give a message identifying the changes that were made.\nWe can either do this by specifying it on the command line to the commit subcommand as follows:\ngit commit -m \u0026quot;Committing some changes\u0026quot;\rOr, for more detailed messages, they can be composed in a text editor. To do this, just use the commit command without specifying the message:\ngit commit\rWhich will then open the default text editor where you can type a more detailed message, save, and exit - the commit will then be applied.\nOnce staged changes are committed, using the status subcommand again will show that the stage is upto date with the repository, this state is referred to as clean.\nConfiguration The subcommand config can be used to specify various parameters affecting the operation of Git. Before you can make a commit, Git needs to know your name and email address as a minimum.\nThese can be set as such:\ngit config user.name \u0026quot;Name Here\u0026quot;\rgit config user.email \u0026quot;myemailaddress@example.com\u0026quot;\rViewing commits The log subcommand will allow us to see the history of commits done to the repository. They will be displayed from most recent to the oldest, since Git always keeps the head of the commit chain and accesses the others from there.\nEach entry will show the commit hash, along with the author name/email, the date of the commit, and the message.\nFor example after doing two commits to a repository, we may see something like the following on the command line:\ngit log\rcommit 2f988a4223baa3d8bd1db16f8cc777181eb390cd (HEAD -\u0026gt; master)\rAuthor: Robin Firth \u0026lt;theforgottenmaster@googlemail.com\u0026gt;\rDate: Sat Nov 6 19:34:57 2021 +0000\rChanged the text from \u0026quot;Some text\u0026quot; to \u0026quot;Changed text\u0026quot;\rcommit 18dd164cf53fdb05bb757d3e0b452c4bfcf0b38a\rAuthor: Robin Firth \u0026lt;theforgottenmaster@googlemail.com\u0026gt;\rDate: Sat Nov 6 19:33:55 2021 +0000\rInitial commit\rAn individual commit can be viewed with the show subcommand. This is useful if you are given a commit hash in conversation with someone and you want to see what was changed in that commit.\nFor example, to see the initial commit above we can use show and see the following:\ngit show 18dd164cf53fdb05bb757d3e0b452c4bfcf0b38a\rcommit 18dd164cf53fdb05bb757d3e0b452c4bfcf0b38a\rAuthor: Robin Firth \u0026lt;theforgottenmaster@googlemail.com\u0026gt;\rDate: Sat Nov 6 19:33:55 2021 +0000\rInitial commit\rdiff --git a/file_to_add.txt b/file_to_add.txt\rnew file mode 100644\rindex 0000000..9dbfeee\r--- /dev/null\r+++ b/file_to_add.txt\r@@ -0,0 +1 @@\r+Some text\r\\ No newline at end of file\rThis shows the same information as in the log, however also shows the file differences that were part of that commit.\nDiffing commits If you have any 2 commit hashes, you can easily determine the diff that is applied to the first commit, in order to reach the second commit.\nFor example, in order to see the diff that was applied between the initial commit, and the second commit above, we will use the command and see something like:\ngit diff 18dd164cf53fdb05bb757d3e0b452c4bfcf0b38a 2f988a4223baa3d8bd1db16f8cc777181eb390cd\rdiff --git a/file_to_add.txt b/file_to_add.txt\rindex 9dbfeee..41c4a21 100644\r--- a/file_to_add.txt\r+++ b/file_to_add.txt\r@@ -1 +1 @@\r-Some text\r\\ No newline at end of file\r+Changed text\r\\ No newline at end of file\rAnd we can see fromm the diff that \u0026ldquo;Some text\u0026rdquo; was removed, and \u0026ldquo;Changed text\u0026rdquo; was added in its place.\nRenaming files We could achieve the same result as a rename by removing the file, renaming it, and adding it back again, however there is a Git command for renaming or moving a file. For example if we want to rename file_to_add.txt to renamed_file.txt, we can do so as follows:\ngit mv file_to_add.txt renamed_file.txt\rA move can be achieved by keeping the destination filename the same, but specifying the folder that it should be placed under.\nAfter the above rename, running git status shows:\nOn branch master\rChanges to be committed:\r(use \u0026quot;git restore --staged \u0026lt;file\u0026gt;...\u0026quot; to unstage)\rrenamed: file_to_add.txt -\u0026gt; renamed_file.txt\rAdvanced configuration There are actually 3 levels of configuration for the Git config subcommand\n Repository-specific. These are the default level if you use the config subcommand without specifying one of the others. These will take precedence if set and override any system or user level configuration settings. User-specific. These are next lowest in the hierarchy and if not overridden will be applied for all repositories for that user only. These are set by using the \u0026ndash;global option. System-wide. These are the lowest priority and if not overridden will apply to all repositories, of all users on that system. This is specified with the \u0026ndash;system option.  Example: setting name For an example, we\u0026rsquo;ll show setting the user.name configuration setting we showed before, at the 3 levels of configuration.\n Repository-specific  git config user.name \u0026quot;User Name For This Repo Only\u0026quot;\rUser-specific  git config --global user.name \u0026quot;User Name For All Repositories For This User\u0026quot;\rSystem-wide  git config --system user.name \u0026quot;User Name For Everybody\u0026quot;\rViewing configuration settings We can view all the set configuration settings by passing the \u0026ldquo;-l\u0026rdquo; flag to the config subcommand. Which will display something like the following:\ngit config -l\rdiff.astextplain.textconv=astextplain\rfilter.lfs.clean=git-lfs clean -- %f\rfilter.lfs.smudge=git-lfs smudge -- %f\rfilter.lfs.process=git-lfs filter-process\rfilter.lfs.required=true\rhttp.sslbackend=openssl\rhttp.sslcainfo=C:/Program Files/Git/mingw64/ssl/certs/ca-bundle.crt\rcore.autocrlf=true\rcore.fscache=true\rcore.symlinks=false\rcore.editor=\u0026quot;C:\\\\Program Files\\\\Microsoft VS Code\\\\bin\\\\code.cmd\u0026quot; --wait\rpull.rebase=false\rcredential.helper=manager-core\rcredential.https://dev.azure.com.usehttppath=true\rinit.defaultbranch=master\ruser.email=theforgottenmaster@googlemail.com\ruser.name=Robin Firth\rcore.repositoryformatversion=0\rcore.filemode=false\rcore.bare=false\rcore.logallrefupdates=true\rcore.symlinks=false\rcore.ignorecase=true\rUnsetting a configuration setting The \u0026ndash;unset flag can be passed to unset a specific configuration setting at any of the three levels (by omitting, or including either \u0026ndash;global or \u0026ndash;system flags). For unsetting the user.name setting for example at all levels, we could do:\ngit config --unset user.name\rgit config --unset --global user.name\rgit config --unset --system user.name\rAliases You can set your own aliases for more complex commands in Git. This allows you to run complex commands with a shorter subcommand.\nFor example we can make an alias which will commit with a default message, and run it:\ngit config alias.default-commit -- \u0026quot;commit -m 'Default commit message'\u0026quot;\rgit default-commit\r","permalink":"https://forgottenmaster.github.io/posts/git/basic_usage/","summary":"This post will delve into the usage of the Git CLI from a user\u0026rsquo;s perspective. It will cover basic usage such as cloning, pulling, pushing, and will touch on how the commands interact with the index and object store of the repository.\nCreating a repository There are two different ways to establish a repository in Git in contrast to other version control systems where there is only creating the initial repository.","title":"Basic Usage"},{"content":"What are traits? Traits in Rust can basically be thought of as interfaces in C#. They can do everything that a C# interface can do except with a few more capabilities. We will start off by equating the common functionality of traits in Rust with C# interfaces, and then explore the additional capabilities we get with Rust traits.\nDescribes capabilities In C# interfaces, we can describe a set of function signatures which will tell the user the capabilities of that interface, such that they know when they call something what data to pass in, and what they should get back. The caller need not know how the function is implemented by a specific type, just that it does what is described on the tin.\nFor example, a C# interface which provides the capability to get an identifier through a GetIdentifier function which takes no arguments, and returns the ID of the instance as an integer can look as follows:\ninterface IGetIdentifier { int GetIdentifier(); } In Rust we can describe the exact same functionality. Note however a few changes in order to follow naming conventions or required by the language syntax:\n interface keyword is renamed to trait In Rust, the naming convention for a trait that has a single method is just the name of the method itself Rust has sized integer types so int is replaced with i32 (can be any of the other integer types too) Rust requires each member function to take explicitly the object we\u0026rsquo;re calling on. This allows us to tell the compiler/caller whether we\u0026rsquo;re taking an immutable reference, mutable reference, or taking ownership. In this case we only need an immutable reference to get an identifier Rust naming convention for functions and variables is snake_case. Therefore GetIdentifier method is renamed get_identifer.  The equivalent Rust definition is therefore:\ntrait GetIdentifier { fn get_identifier(\u0026amp;self) -\u0026gt; i32; } Generics In C# we are also able to make an interface generic. This is useful if we need to implement an interface multiple times for a given type for different situations. We can\u0026rsquo;t implement the same interface multiple times, but each different set of generic parameters is essentially a different interface.\nLet\u0026rsquo;s say we extend the above interface to work for any identifier type and not just integers. The C# snippet would be extended to look as follows:\ninterface IGetIdentifier\u0026lt;T\u0026gt; { T GetIdentifier(); } Likewise, the Rust snippet is extended with the exact same syntax:\ntrait GetIdentifier\u0026lt;T\u0026gt; { fn get_identifier(\u0026amp;self) -\u0026gt; T; } Bounded generics This is the ability to constrain generic parameter types to ones that only implement certain interfaces. In C# we have the ability to constrain based on interface or base class type, however in Rust we don\u0026rsquo;t have struct inheritance but do have trait inheritance. As a result, in Rust we can only constrain on interface. But this is good practice anyways as inheritance of behaviour is better than inheritance of state.\nIn C# if we want to bound the above interface to only allow T\u0026rsquo;s that implement an \u0026ldquo;IIdentifier\u0026rdquo; interface, that is an interface that allows the type to be used as an identifier, would look like this:\ninterface IGetIdentfier\u0026lt;T\u0026gt; where T : IIdentifier { T GetIdentifier(); } In Rust, we have two options for defining trait bounds, we can do it as above with a \u0026ldquo;where\u0026rdquo; syntax. This is useful when there are lots of bounds for a type and we can break them up over multiple lines which is more readable in most cases:\ntrait GetIdentifier\u0026lt;T\u0026gt; where T: Identifier { fn get_identifier(\u0026amp;self) -\u0026gt; T; } However we also have the ability to define these trait bounds inline, which for fewer trait bounds could be neater:\ntrait GetIdentifier\u0026lt;T: Identifier\u0026gt; { fn get_identifier(\u0026amp;self) -\u0026gt; T; } Default implementations Since C#8 we have the ability to provide default implementations for methods, to be used if the implementor doesn\u0026rsquo;t provide their own implementation. This lets us define some required methods that must be implemented, by not defining a default implementation, and provided methods (which can still be overridden, but don\u0026rsquo;t need to be) by giving a default.\nFor example in C#, if we assume that the definition of the \u0026ldquo;IIdentifier\u0026rdquo; interface is as follows:\ninterface Identifier { string GetString(); } Then we can add a provided method to the GetIdentifier interface which will print out the string identifier (note that we still require the GetIdentifier function to be implemented since there\u0026rsquo;s no possible way we can know how to get one). We get the resulting code:\ninterface IGetIdentifier\u0026lt;T\u0026gt; where T: IIdentifier { T GetIdentifier(); void PrintIdentifier() { System.Console.WriteLine($\u0026#34;{GetIdentifier().GetString()}\u0026#34;); } } We can do the same in Rust too in the same way as the following code snippet shows:\ntrait GetIdentifier\u0026lt;T: Identifier\u0026gt; { fn get_identifier(\u0026amp;self) -\u0026gt; T; fn print_identifier(\u0026amp;self) { println!(\u0026#34;{}\u0026#34;, self.get_identifier().get_string()); } } Implementing We can of course implement an interface or trait in both C# and Rust (what use would an interface be if we couldn\u0026rsquo;t!?). However the syntax is different, and this is where we start to see C# and Rust diverge quite dramatically.\nIn C# we have to provide the implementation of the interface at the same point as we define the implementing class/struct itself. For example, implementing the IIdentifier interface for a struct Foo:\nstruct Foo : IIdentifier { public int identifier; public string GetString() =\u0026gt; $\u0026#34;{identifier}\u0026#34;; } And implementing the IGetIdentifiertrait for a second struct FooGetter:\nstruct FooGetter : IGetIdentifier\u0026lt;Foo\u0026gt; { public Foo identifier; public Foo GetIdentifier() =\u0026gt; identifier; } In Rust however, the difference is that we define implementation blocks separate to the variables inside the struct itself. This allows us to break up behaviour/functions from the pure data contained in the structure. Each trait has its own implementation block so the implementation of the above structures and trait implementations will look as follows:\n#[derive(Clone)] struct Foo { pub i32 identifier; } impl Identifier for Foo { pub fn get_string(\u0026amp;self) -\u0026gt; String { format!(\u0026#34;{}\u0026#34;, self.identifier) } } struct FooGetter { pub Foo identifier; } impl GetIdentifier\u0026lt;Foo\u0026gt; for FooGetter { pub fn get_identifier(\u0026amp;self) -\u0026gt; Foo { self.identifier.clone() } } Note the Clone implementation required on Foo, and the clone function call in get_identifier. This is required because in Rust every type is movable, and clones are explicit. Adding the #[derive(Clone)] attribute to the struct allows us to automatically derive a deep clone implementation as long as all the struct fields implement Clone.\nNote that this is not idiomatic Rust, clones are rarely used as we can pass references around safely and the compiler will check the ownership and lifetime rules for us.\nIs that it? So you might look at the previous content and think that Rust traits are just C# interfaces with a different syntax. They can do everything C# interfaces can do right?. Well, yes, except that there are more capabilities that Rust gives us that just aren\u0026rsquo;t possible in C#\nThe following few sections then will only contain Rust snippets as there\u0026rsquo;s no valid way to represent them in C# (or C++, or most languages I\u0026rsquo;ve used - except Haskell, which makes traits similar to typeclasses).\nAssociated Types In the above code snippets, we had a generic trait GetIdentifier which was implemented on FooGetter with the generic parameter Foo. However this opens the way for us to have multiple implementations on FooGetter with different types. However what if we want to force the user to define a maximum of 1 implementation of GetIdentifier?\nWell, we have to remove the generics, and we end up with a trait as follows:\ntrait GetIdentifier { fn get_identifier(\u0026amp;self) -\u0026gt; ??? // what type goes here? } However, we have a problem here since we don\u0026rsquo;t know what the actual identifier type is now for any given implementation. We\u0026rsquo;ve removed the ability to specify it in a generic parameter, so in C# the only way to do this would be to fix the concrete type we return. That means all implementors of GetIdentifier returns the same type.\nTechnically in C# we can do it by returning the IIdenfier interface itself:\ninterface IGetIdentifier { IIdentifier GetIdentifier(); } Which indeed will allow each implementor to determine what the actual type they\u0026rsquo;re returning is, as long as it implements the IIdenfier interface.\nThe downside here is that we are forced to box the result which means a heap allocation, which means garbage collector tracking overhead.\nWe can do the same thing in Rust, we have to be explicit about returning a dynamic trait object in a box though:\ntrait GetIdentifier { fn get_identifier(\u0026amp;self) -\u0026gt; Box\u0026lt;dyn Identifier\u0026gt;; } However this still requires allocating heap storage and returning. An additional downside is type erasure. We\u0026rsquo;ve lost all information about the actual concrete type, all we know is the Box has an Identifier in it, so can only access the methods of the Identifier trait and nothing more.\nThere must be a better way!?. Enter associated types:\ntrait GetIdentifier { type IdentifierType: Identifier; fn get_identifier(\u0026amp;self) -\u0026gt; Self::IdentifierType; fn print_identifier(\u0026amp;self) { println!(\u0026#34;{}\u0026#34;, self.get_identifier().get_string()); } } Problems solved!. There\u0026rsquo;s a bit of new syntax here, but the main points are:\n We define an associated type on the trait with the type T syntax, allowing each implementor to specify a different concrete type We place a trait bound on it so that only concrete types implementing the Identifier trait can be used We return Self::IdentifierType from the get_identifier function. Importantly this is the concrete type, meaning no heap allocations and no type erasure  One further thing with associated types is that we can add trait bounds that force them to a specific concrete type. For example, say that we want to create a function that will take any GetIdentifier and print it, but only if the identifier type is an i32. We can do this as a trait bound with the following syntax:\nfn call_print_identifier_if_i32\u0026lt;T: GetIdentifier\u0026lt;IdentifierType = i32\u0026gt;\u0026gt;(t: \u0026amp;T) { t.print_identifier(); } We can also even place trait bounds on associated types within trait bounds!. For example if we want a function that will accept any GetIdentifier, but only if the IdentifierType implements Clone, we can do so:\nfn do_something\u0026lt;T\u0026gt;(t: \u0026amp;T) where T: GetIdentifier, \u0026lt;T as GetIdentifier\u0026gt;::IdentifierType: Clone { let c = t.get_identifier().clone(); // do something with cloned instance } Associated methods Unlike C# (and most other languages) interfaces which can only define methods tied to the specific instance of the implementing type, due to requiring dynamic dispatch and using a vtable, in Rust we can also define methods at the type level. These would be called static methods in other languages but in Rust, they are known as associated methods.\nA simple example of a trait making use of this functionality is the Default trait in the standard library. If we were to define it ourselves, we can do it like this:\ntrait Default { fn default() -\u0026gt; Self; } Self here is the implementing type, and notice how an associated method is indicated not by a keyword, but by the lack of self, \u0026amp;self, or \u0026amp;mut self in the first argument position.\nThese associated methods can be called with :: syntax, for example if our type Foo implements Default, we can call it as:\nlet default_foo = Foo::default(); Extension traits The next feature Rust provides us with respect to traits is as a side effect of having to implement them separate to the type we\u0026rsquo;re implementing on. This implies we can implement traits for types that we don\u0026rsquo;t actually own. This isn\u0026rsquo;t possible in C# or C++ where the implementation of a type is defined at the same time as the type itself.\nThis means that we are able to add functionality to existing types, even standard library types or primitive types by creating a trait and implementing it.\nFor example, say that we want to add a AsBytes trait which specifies that the type has a method called as_bytes which returns a vector of u8\u0026rsquo;s representing the bytes of the type.\nSuch a trait can be defined like:\ntrait AsBytes { fn as_bytes(\u0026amp;self) -\u0026gt; Vec\u0026lt;u8\u0026gt;; } And we can implement that for our own types, however, unlike in most other languages, we can implement this for existing types even primitives. For example on a u32:\nimpl AsBytes for u32 { fn as_bytes(\u0026amp;self) -\u0026gt; Vec\u0026lt;u8\u0026gt; { self.to_le_bytes().to_vec() } } to_le_bytes is a function that the standard library provides for us, that will give us an array of u8\u0026rsquo;s of length 4 with the bytes of the u32 in it. We can call to_vec on this to turn it into a dynamically sized vector instead to return.\nBlanket implementations The final feature for traits that we have with Rust is the ability to implement a trait for all types, optionally bounded with trait bounds.\nLet\u0026rsquo;s say we want to add a method to all iterators which will result in a new iterator that prints out the item (for all items that are displayable) as it iterates them.\nNote that we can do this with a map call to decorate a function, taking the input, printing it and returning it again to make a new iterator. This would look something like this:\nlet iter = (0..10).map(|elem| { println!(\u0026#34;{}\u0026#34;, elem); elem }); However this requires the user to roll the function themselves to print and return, and is a bit unwieldy. What we\u0026rsquo;d like is:\nlet iter = (0..).print(); We can do this with a blanket implementation that adds this print function to all iterators. First we need a structure to wrap the iterator that will step through and do the printing:\nstruct Print\u0026lt;T\u0026gt; { iterator: T } We will need to implement the Iterator trait here for the new structure, so that we can step over and print the elements. However we can only do this if the elements implement the Display trait. We can use trait bounds to ensure that:\n T is an Iterator The elements from T implement Display  This will look as follows:\nimpl\u0026lt;T: Iterator\u0026gt; Iterator for Print\u0026lt;T\u0026gt; where \u0026lt;T as Iterator\u0026gt;::Item: Display { type Item = \u0026lt;T as Iterator\u0026gt;::Item; // just passing the items through  fn next(\u0026amp;mut self) -\u0026gt; Option\u0026lt;Self::Item\u0026gt; { let item = self.iterator.next()?; println!(\u0026#34;{}\u0026#34;, item); Some(item) } } The little ? syntax when we call the next function of the iterator we\u0026rsquo;re wrapping is a little outside of the scope of the article, but the easiest way to think of it is if that call returns None, then we return None immediately. Otherwise item is set to the value inside the Some (which we then print and return).\nFinally we need to actually add the convenience function to all iterators. We can do this by creating an extension trait called IteratorPrint with the print function we want:\ntrait IteratorPrint where Self: Sized { fn print(self) -\u0026gt; Print\u0026lt;Self\u0026gt; { Print { iterator: self } } } We need to specify Self: Sized because traits in Rust can be implemented even for dynamically sized types which can\u0026rsquo;t exist on their own and must be boxed or put behind a reference.\nSince we need to put self into the Print structure, it will need to have a compile time known size, so we specify we can only use it with such types.\nNow we have all the boilerplate setup, we can actually do the blanket implementation. We\u0026rsquo;ll add it only to compatible iterators, otherwise we end up with a more cryptic compile error. The actual final code for this is as follows:\nimpl\u0026lt;T: Iterator\u0026gt; IteratorPrint for T where \u0026lt;T as Iterator\u0026gt;::Item: Display { } Then we can take any iterator that has elements that are displayable, and use this print function on it, even if we didn\u0026rsquo;t write the iterator type ourselves!. For example the following is valid:\n(0..10).print().for_each(|_| {}); The for_each call will just apply the closure to each element, in our case we just want to do nothing, but it will trigger the prints as it iterates.\n","permalink":"https://forgottenmaster.github.io/posts/rust/whyrust/traits/","summary":"What are traits? Traits in Rust can basically be thought of as interfaces in C#. They can do everything that a C# interface can do except with a few more capabilities. We will start off by equating the common functionality of traits in Rust with C# interfaces, and then explore the additional capabilities we get with Rust traits.\nDescribes capabilities In C# interfaces, we can describe a set of function signatures which will tell the user the capabilities of that interface, such that they know when they call something what data to pass in, and what they should get back.","title":"Traits"},{"content":"Move By Default In most programming languages, there is no one true \u0026ldquo;owner\u0026rdquo; of any given piece of data. Data can be allocated on the stack or the heap but references to it can be passed around as needed. There isn\u0026rsquo;t a way of the compiler to track ownership of a piece of data in most languages such as C++.\nAdditionally expensive copies of structures may be done without us realising, or being able to opt out of it. For example in C++ and C# the following code will result in making a copy of SomeLargeStruct\nSomeExpensiveStruct s1; SomeExpensiveStruct s2 = s1; // this will make a copy of s1  In Rust, each piece of data has a single owner at any one time. If the owning binding or function goes out of scope, the data is dropped and this gives an opportunity for the type to do any resources allocated. This is similar to a destructor in C++. Due to the single ownership model, Rust moves data by default on assigment, or when passing into and out of functions.\nAn example of the flow of ownership in Rust is the following\nlet v1 = vec![1, 2, 3]; // v1 is a binding that owns a vector with the integers 1, 2, and 3 let v2 = v1; // v1 is *moved* to binding v2. v2 is now the owner of the vector, v1 has relinquished control println!(\u0026#34;{}\u0026#34;, v1); // here we are trying to print out v1, but v1 isn\u0026#39;t the owner of any data and so can\u0026#39;t give out references, etc.  In the above code snippet, the binding v1 transfers ownership of the data to the binding v2, trying to use v1 afterwards results in the following compiler output\nerror[E0382]: use of moved value: `v1`\r--\u0026gt; src/main.rs:4:10\r|\r2 | let v1 = vec![1, 2, 3];\r| -- move occurs because `v1` has type `Vec\u0026lt;i32\u0026gt;`, which does not implement the `Copy` trait\r3 | let v2 = v1;\r| -- value moved here\r4 | dbg!(v1);\r| ^^ value used here after move\r Rust won\u0026rsquo;t let us compile a program which violates the ownership rules like this.\nThe above example is of a move that occurs when assigning from one binding to another, but the same will happen if we passed into a function, for example\nfn foo(v: Vec\u0026lt;i32\u0026gt;) { // v is a vector of integers, ownership is transferred into the function from outside  // the function is the owner of the data, so at the end of the function, the data is dropped } fn main() { let v = vec![1, 2, 3]; foo(v); // ownership is transferred into the function here  dbg!(v); } Will result in a similar compiler error as before:\nerror[E0382]: use of moved value: `v`\r--\u0026gt; src/main.rs:7:10\r|\r5 | let v = vec![1, 2, 3];\r| - move occurs because `v` has type `Vec\u0026lt;i32\u0026gt;`, which does not implement the `Copy` trait\r6 | foo(v);\r| - value moved here\r7 | dbg!(v);\r| ^ value used here after move\r Use After Free In languages such as C++, there are no mechanisms which prevent a use after free error. For example the following code is completely valid\nstruct SomeStruct { public: const char* name; }; SomeStruct\u0026amp; allocAndReturn() { SomeStruct s; s.name = \u0026#34;Hello, World!\u0026#34;; return s; } int main() { SomeStruct\u0026amp; s = allocAndReturn(); std::cout \u0026lt;\u0026lt; s.name \u0026lt;\u0026lt; std::endl; // Boom!, we\u0026#39;ve used an invalid reference  return 0; } As the C++ compiler doesn\u0026rsquo;t track ownership or lifetimes, it doesn\u0026rsquo;t prevent the function from returning a reference to an object that then goes out of scope, then when it tries to access s.name the program segfaults.\nContrast this in Rust, where the compiler tracks lifetimes, it is able to detect a use after free and will fail to compile. If we attempt to return a reference to an object created in a function as in the above program, we won\u0026rsquo;t be able to compile. As it\u0026rsquo;s difficult to do the above function in Rust without delving into lifetime annotations, a smaller program demonstrates the same issue\nfn main() { let v = { let v = vec![4, 5, 6]; \u0026amp;v }; dbg!(v); } As Rust is an expression based language, we can create an arbitrary scope or code block that ends in an expression, and use that to assign to a binding, or anywhere else an expression is accepted.\nIn the code snippet, we are creating a vector v inside the block, and then trying to return a reference to it, out of the block and bind it to the outer v binding.\nHowever Rust detects that we are trying to keep a reference for longer than the object is in scope and gives the following compiler error\nerror[E0597]: `v` does not live long enough\r--\u0026gt; src/main.rs:4:9\r|\r2 | let v = {\r| - borrow later stored here\r3 | let v = vec![4, 5, 6];\r4 | \u0026amp;v\r| ^^ borrowed value does not live long enough\r5 | };\r| - `v` dropped here while still borrowed\r This is the Rust\u0026rsquo;s borrow checker and lifetime rules at work.\nModification While Reading In other languages, it\u0026rsquo;s common for a mutable and immutable reference to exist at the same time. For example in C++ we may do the following:\nSomeStruct s; const SomeStruct \u0026amp;s1 = s; SomeStruct\u0026amp; s2 = s; The implications of this is that someone that is holding an immutable reference to an object will observe changes made to the object (through a mutable reference). This is contrary to what the definition should be of an immutable reference, the holder shouldn\u0026rsquo;t see any changes as it\u0026rsquo;s immutable.\nIf we attempt to do this in Rust, it will fail to compile as we cannot take a mutable and immutable reference at the same time (though we may take as many immutable references as we\u0026rsquo;d like since none will observe any changes).\nfn main() { let mut v = vec![1, 2, 3]; let v1 = \u0026amp;v; let v2 = \u0026amp;mut v; dbg!(v1); } We are greeted with the following compile error\nerror[E0502]: cannot borrow `v` as mutable because it is also borrowed as immutable\r--\u0026gt; src/main.rs:4:14\r|\r3 | let v1 = \u0026amp;v;\r| -- immutable borrow occurs here\r4 | let v2 = \u0026amp;mut v;\r| ^^^^^^ mutable borrow occurs here\r5 | dbg!(v1);\r| -- immutable borrow later used here\r Rust guarantees memory safety here by allowing either 0 or more immutable references OR 0 or 1 mutable reference. It is forbidden to have a mutable reference while ANY references are taken, and only 1 mutable reference can be taken at a time.\nThis means if you hold a mutable reference to an object, the compiler guarantees that only you can make changes to the object. It also guarantees that if you hold an immutable reference you will never see any state changes through that.\nThis isn\u0026rsquo;t strictly true\u0026hellip;.there are types in the standard library that allow for interior mutability, that is, mutating through an immutable reference. However, they are also perfectly safe to use because they perform the check at runtime instead. When using those types, you are only allowed to mutate a value if nothing else currently holds a borrow to it.\nIn conclusion, most programming languages have no real protections against multiple sources writing to an object, even when other sources have read-only access. Rust has a policy of only 1 writer at a time, and only when there are no readers in existence.\nThis does require some code restructuring to separate the reading from the writing, but results in much safer code and prevents common problems such as modifying a collection while iterating over it, which is impossible in Rust due to the borrow checker.\nThread Safety The last topic on memory safety in Rust will be a few words on how the safety rules also apply in a multithreaded context.\nMost languages don\u0026rsquo;t have any guarantees about whether an object is safe to be accessed from multiple threads at a time, and if an object is not safe, but is accessed concurrently from 2 threads, this is called a data race, and is not a good thing.\nRust provides a system by which the compiler will fail to compile a program if we try to use a non thread-safe type in a multithreaded context.\nThe way it achieves this is by using two core traits called Send and Sync\nThese are implemented (or not implemented) by types automatically based on the data inside the types. If all the members of a custom type are Send, or Sync then the type itself is Send or Sync.\nSend means that instances of this type can be transferred over to a different thread. This is transfer of ownership across a thread boundary.\nSync means that instances of this type can be accessed from multiple threads at the same time, that is references to the instance are Send.\nThe developer can implement Send and Sync manually on types if those types aren\u0026rsquo;t automatically deemed Send/Sync, however to do so is an unsafe operation as it requires that the developer has checked that the type is indeed thread-safe.\n","permalink":"https://forgottenmaster.github.io/posts/rust/whyrust/memorysafety/","summary":"Move By Default In most programming languages, there is no one true \u0026ldquo;owner\u0026rdquo; of any given piece of data. Data can be allocated on the stack or the heap but references to it can be passed around as needed. There isn\u0026rsquo;t a way of the compiler to track ownership of a piece of data in most languages such as C++.\nAdditionally expensive copies of structures may be done without us realising, or being able to opt out of it.","title":"Memory Safety"},{"content":"The final piece of the puzzle in the mathematical foundations needed is to determine how we can trace back a derivative with respect to matrix inputs.\nIn the previous post we saw that calculating the derivative of two vectors of equal length when passing through the dot product operation results in the transpose of the other.\nMatrix Multiplication Refresher The first step is to recall how we can multiply two matrices together. In deep learning typically this will be performed with a matrix of samples, and a matrix of weights to be applied. The result being a weighted sum of the samples with weights.\nIn order for the product of two matrices to be defined, the number of columns in the first matrix must equal the number of rows in the second. The resulting matrix will be of the outer dimensions.\nThat is, given a matrix X, of dimensions m x n and a matrix W, of dimensions n x p then the multiplication is defined, and the resulting matrix is of dimensions m x p.\nIn the resulting matrix, an element at row i and column j is the result of the dot product of the ith row of the first matrix, and the jth row of the second matrix.\nAs a concrete example, suppose we have\n$$ X = \\begin{bmatrix} X_{11} \u0026amp; X_{12} \u0026amp; X_{13} \\\\ X_{21} \u0026amp; X_{22} \u0026amp; X_{23} \\\\ X_{31} \u0026amp; X_{32} \u0026amp; X_{33} \\end{bmatrix} $$ $$ W = \\begin{bmatrix} W_{11} \u0026amp; W_{12} \\\\ W_{21} \u0026amp; W_{22} \\\\ W_{31} \u0026amp; W_{32} \\end{bmatrix} $$\nThen the resulting matrix can be represented as\n$$ X \\cdot W = \\begin{bmatrix} XW_{11} \u0026amp; XW_{12} \\\\ XW_{21} \u0026amp; XW_{22} \\\\ XW_{31} \u0026amp; XW_{32} \\end{bmatrix} $$\nWhere \\(XW_{ij}\\) is the dot product of the ith row of X and the jth row of W\nDerivative Of Matrices In order for us to be able to calculate the rate of change in the output of a function with respect to the elements of one of the input matrices we will need to have the output be either:\n A matrix of the same size as an input matrix - derivative can be calculated elementwise in that case between input and output matrices A scalar value - derivative can be calculated elementwise for each element in the input matrix to determine how this affects the single output  For deep learning, we want to be able to first multiply a matrix of samples (X) by a matrix of weights (W) in order to get the weighted sum of the records in a resulting matrix, then we would like to pass the resulting matrix to a function that produces a single value that could be used to activate a neuron for example.\nTherefore what we have is a function F which can be defined as\n$$ F = F(Y) $$\nThat is it takes some matrix Y and performs an operation on it (in our case likely summing the elements). The matrix Y will be produced by multiplying the two input matrices, that is:\n$$ Y = X \\cdot W $$\nIn order to calculate the total derivative with respect to the input matrices, we can apply the chain rule. The first step of which is easy, that is to take Y and calculate how a change in each element would affect the resulting single value. The derivative of F therefore is a matrix representing these partial derivatives. We will call this G, that is \\(G = F'(Y)\\)\nThe derivative of Y depends on two variables X and W which are multiplied together. The formula for the total derivative therefore is\n$$ dY = dX \\cdot W + X \\cdot dW $$\nThat is, changes in the elements of X would end up being multiplied by W, and also changes in the elements of W would end up being multiplied by X.\nThen, the derivative of the total function can be given (using the chain rule) as:\n$$ dF = G:dY $$\nAnd expanding out dY with the above formula gives\n$$ dF = G:dX \\cdot W + G:X \\cdot dW $$\nFor notation, X:Y means the elementwise multiplication of matrices X and Y, that must be the same dimensions (also known as frobenius inner product).\nIsolating dX and dY In the above formula, we have G being multiplied elementwise by a matrix that is the result of matrix multiplication between dX and W or X and dW.\nWhat we want to do is to isolate dX and dY so that they are multiplied elementwise with the other matrix. How can we get W and X onto the other side?\nWell,\n$$ G : (dX \\cdot W) = (G \\cdot W^T) : dX $$\nAnd\n$$ G : (X \\cdot dW) = (X^T \\cdot G) : dW $$\nThat is, the total derivative can be calculated as\n$$ dF = (G \\cdot W^T) : dX + (X^T \\cdot G) : dW $$\nPartial Derivatives Now from the above formula, we can easily calculate the partial derivatives of F by holding either X or W as constant, meaning dX or dW respectively is 0.\nDoing this will eliminate the term from the formula leaving only the other one. This shows us how the partial derivatives are\n$$ \\frac{\\partial F}{\\partial X} = G \\cdot W^T $$ $$ \\frac{\\partial F}{\\partial W} = X^T \\cdot W $$\nAdditional Notes The book at this point actually didn\u0026rsquo;t explain how we get to using the transpose matrices for derivatives, instead the book says in a handwavey fashion that \u0026ldquo;the way the mathematics works out\u0026rdquo;.\nI found the above explanation at This Page and above tried to break it down as best I could.\nI will likely need to revisit matrix multiplication in the future but for now understand it enough to move on to learning more about the structure of a neural network and applying these rules in practice.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/matrixinputs/","summary":"The final piece of the puzzle in the mathematical foundations needed is to determine how we can trace back a derivative with respect to matrix inputs.\nIn the previous post we saw that calculating the derivative of two vectors of equal length when passing through the dot product operation results in the transpose of the other.\nMatrix Multiplication Refresher The first step is to recall how we can multiply two matrices together.","title":"1.6 - Matrix Inputs"},{"content":"After learning what a derivative of a function is, and how to apply the chain rule to a composite function, we then learned how to calculate the derivative of a function with multiple inputs by tracing back through the chain of functions following the route of one of the parameters while holding the others constant.\nThe next step in the prerequisite mathematics we need to build the foundations of a neural network is to determine what the derivative of a function means when one or more of the inputs is a vector.\nGiven the following block diagram for a chain of functions\nThat is, a function, f which takes two inputs both of which are vectors (denoted by the bar above the name), and a function, g which operates on the resulting value.\nFurthermore, let\u0026rsquo;s say that f is actually the dot product function. This function is written as follows, if we assume that X and W are both vectors of length 3:\n$$ f(X, W) = X \\cdot W = X_1 \\times W_1 + X_2 \\times W_2 + X_3 \\times W_3 $$\nAs a reminder, the derivative of a function is the ratio of the change in the output given a change in the input, at a specific input value (point on the graph of the function). This is easy to determine when the inputs and outputs are plain values as it\u0026rsquo;s just a ratio of the two deltas.\nWhen an input is a vector though, what does a change in the input mean?. In fact, this is easily interpreted as a change in one of the components of the input.\nAs seen in the dot product formula (though the function can be any function on vectors), a vector can be decomposed into a list of individual numbers/components. Therefore we can calculate the derivative of the function with respect to an individual component by holding the others to be constant and varying the one we\u0026rsquo;re interested in.\nAfter applying this first function though, the second is operating on a single value and so the chain rule applies as before. In the case of the dot product, this gives us 6 partial derivatives: 3 with respect to the 3 components in the input vector X and 3 with respect to the components of W.\nThe formulas for calculating the derivatives then for the composite function will be as follows in accordance with the chain rule (where n is a valid index for the input vector):\n$$ (g \\circ f)_{X_n} = f_{X_n}(X, W) \\times g'(f(X, W)) $$ $$ (g \\circ f)_{W_n} = f_{W_n}(X, W) \\times g'(f(X, W)) $$\nLooking back at the formula for calculating the dot product we can see that holding all values constant except for a single component we are interested in causes a change in the output proportional to the matching component of the other vector. For example if we let \\(X_1\\) increase by 1 while holding all other components constant, we can see that the only part of the formula containing \\(X_1\\) is\n$$ X_1 \\times W_1 $$\nIncreasing \\(X_1\\) by 1 then increases the output only by \\(W_1\\)\nWhen we calculate the derivative for the other components then, we find out that elementwise, the derivatives of the dot product with respect to X can be represented by:\n$$ \\begin{bmatrix} W_1 \u0026amp; W_2 \u0026amp; W_3 \\end{bmatrix} $$\nand by the same token, the component wise derivatives of the dot product with respect to W is:\n$$ \\begin{bmatrix} X_1 \u0026amp; X_2 \u0026amp; X_3 \\end{bmatrix} $$\nWhen we write the dot product as row and column vectors in order to have the correct shapes for performing the dot product:\n$$ \\begin{bmatrix} X_1 \u0026amp; X_2 \u0026amp; X_3 \\end{bmatrix} \\cdot \\begin{bmatrix} W_1 \\\\ W_2 \\\\ W_3 \\end{bmatrix} $$\nWe can see that the component wise derivative with respect to X will be \\( \\begin{bmatrix} W_1 \u0026amp; W_2 \u0026amp; W_3 \\end{bmatrix} \\), that is \\(W^T\\) and the component wise derivative with respect to W will be \\( \\begin{bmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{bmatrix} \\), that is \\(X^T\\)\nConclusion In conclusion, calculating the derivative of a function with vector inputs is just calculating the derivative of the function with respect to each component in the vector. Additionally, and most importantly for deep learning, the derivative of the dot product of a row vector X with a column vector W turns out to be \\(W^T\\) and \\(X^T\\) respectively.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/vectorinputs/","summary":"After learning what a derivative of a function is, and how to apply the chain rule to a composite function, we then learned how to calculate the derivative of a function with multiple inputs by tracing back through the chain of functions following the route of one of the parameters while holding the others constant.\nThe next step in the prerequisite mathematics we need to build the foundations of a neural network is to determine what the derivative of a function means when one or more of the inputs is a vector.","title":"1.5 - Vector Inputs"},{"content":"So far, we\u0026rsquo;ve learned how to chain functions together, and how to calculate the derivative of a function at a specific input value. We\u0026rsquo;ve also learned how to apply the chain rule when we are chaining multiple functions together. However, how do we calculate the derivative of a function, or chain of functions when a function has multiple inputs?.\nAs it turns out, we can calculate the derivative in the same way!, we just need to, in the case where there are multiple inputs, calculate the partial derivative with respect to the given parameter.\nLets start off with a single function, one that takes two inputs and returns a single output\nIn such a function, the change in the output can be affected by a change in either input (x or y). Because of this, calculating the derivative with respect to x at a given value might result in a different value to what is given when calculating the derivative for input y. These are the partial derivatives of the function, where each partial derivative is how the output changes with a given change of the input (while keeping the other inputs constant).\nThe formulas for these partial derivatives can be given as:\n$$ \\frac {\\partial f} {\\partial x} = \\lim_{\\Delta \\to 0} \\frac {f(x + \\Delta, y) - f(x - \\Delta, y)} {2 \\times \\Delta} $$ $$ \\frac {\\partial f} {\\partial y} = \\lim_{\\Delta \\to 0} \\frac {f(x, y + \\Delta) - f(x, y - \\Delta)} {2 \\times \\Delta} $$\nApplying the chain rule We can apply the chain rule in the same way when calculating the partial derivative of a composite function with respect to one of the inputs. Since the change in input causes a change in the input to the next function, which then causes a change in the input to the next, and so on, we will still do a backward pass the same way as we would with a single input. We just follow it back to the point where the input is given. For example with the following\nWe can see that the first function has two inputs whereas g and h have only 1 input. So there will be two partial derivatives for this composite function. These can be given as follows (applying the chain rule):\n$$ (f \\circ g \\circ h)_x = f_x(x, y) \\times g'(f(x, y)) \\times h'(g(f(x, y))) $$ $$ (f \\circ g \\circ h)_y = f_y(x, y) \\times g'(f(x, y)) \\times h'(g(f(x, y))) $$\nNote that here we are using Legrange\u0026rsquo;s Notation as it can be cleaner than the Leibniz notation. In this notation we can use \\(f'(x)\\) to denote the derivative of function f taking a single input, at value x.\nWith multiple inputs, the input we\u0026rsquo;re describing the partial derivative of can be given as a subscript. e.g. \\(f_x(x, y)\\) would be saying we are interested in how much a change in x will cause a change in the output of the function, at the point defined by the given values of x and y.\nOverall the chain rule works the same way as a single input, except we are defining the input we\u0026rsquo;re interested in when back propagating to work out the derivative.\nLet\u0026rsquo;s assume the following functions to calculate the derivative of a given 2 input function with respect to each of the inputs:\nfn derivative_x(f: impl Fn(f64, f64) -\u0026gt; f64, x: f64, y: f64, delta: f64) -\u0026gt; f64 { (f(x + delta, y) - f(x - delta, y)) / (2.0 * delta) } fn derivative_y(f: impl Fn(f64, f64) -\u0026gt; f64, x: f64, y: f64, delta: f64) -\u0026gt; f64 { (f(x, y + delta) - f(x, y - delta)) / (2.0 * delta) } Along with the existence of the previous function, \u0026ldquo;derivative\u0026rdquo; which performs the same calculation but on a single input function. We can then calculate the derivatives of the composite function as shown above with the following functions:\nfn derivative_x_chain(f: impl Fn(f64, f64) -\u0026gt; f64, g: impl Fn(f64) -\u0026gt; f64, h: impl Fn(f64) -\u0026gt; f64, x: f64, y: f64, delta: f64) -\u0026gt; f64 { // forward pass to calculate inputs  let f_of_xy = f(x, y); let g_of_f_of_xy = g(f_of_xy); // backward pass to calculate partial derivatives  let derivative_f = derivative_x(f, x, y, delta); let derivative_g = derivative(g, f_of_xy, delta); let derivative_h = derivative(h, g_of_f_of_xy, delta); // total derivative with respect to x of compound function  derivative_f * derivative_g * derivative_h } fn derivative_y_chain(f: impl Fn(f64, f64) -\u0026gt; f64, g: impl Fn(f64) -\u0026gt; f64, h: impl Fn(f64) -\u0026gt; f64, x: f64, y: f64, delta: f64) -\u0026gt; f64 { // forward pass to calculate inputs  let f_of_xy = f(x, y); let g_of_f_of_xy = g(f_of_xy); // backward pass to calculate partial derivatives  let derivative_f = derivative_y(f, x, y, delta); let derivative_g = derivative(g, f_of_xy, delta); let derivative_h = derivative(h, g_of_f_of_xy, delta); // total derivative with respect to x of compound function  derivative_f * derivative_g * derivative_h } For a runnable example of comparing calculating the derivative of a composite function both directly and via the chain rule, I have written a simple example on the Rust Playground which shows that the derivative calculated for a composite function directly results in the same value as a derivative calculated using the chain rule (both single and multiple input functions are tested).\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/multipleinputs/","summary":"So far, we\u0026rsquo;ve learned how to chain functions together, and how to calculate the derivative of a function at a specific input value. We\u0026rsquo;ve also learned how to apply the chain rule when we are chaining multiple functions together. However, how do we calculate the derivative of a function, or chain of functions when a function has multiple inputs?.\nAs it turns out, we can calculate the derivative in the same way!","title":"1.4 - Multiple Inputs"},{"content":"C Style Enums In C#, C++, and a lot of other popular programming languages, we have access to a type called an \u0026ldquo;enumeration\u0026rdquo; (or enum for short). This is simply a type safe collection of named constant values.\nFor example in C++, making an enum whose variants represent a set of allowed colors for a hypothetical UI framework could be written as (with the values of the variants explicitly typed out for transparency):\nenum Color { Red = 1, Green = 2, Blue = 3 }; A function can then go ahead and accept a \u0026ldquo;Color\u0026rdquo; and the user will be able to pass only the named colors. Except that in C++, this is not true. Nothing stops the caller from casting an arbitrary integer as a \u0026ldquo;Color\u0026rdquo;. For example calling a SetColor function that takes a Color, the caller can do:\nwidget.SetColor(static_cast\u0026lt;Color\u0026gt;(10)); // what even is color with value 10???. It hasn\u0026#39;t been defined so likely won\u0026#39;t be correctly handled.  This is not desired as SetColor can\u0026rsquo;t assume that the given Color is only one that was specified in the enumeration. If someone can arbitrarily cast an integer to Color, what is the function supposed to do with it?.\nIn Rust, we can have value type enums the same way:\nenum Color { Red = 1, Green = 2, Blue = 3 } And casting such an enum value into an integer is totally fine, the following snippet will print \u0026ldquo;Selected color is: 3\u0026rdquo;:\n// enum variant to integral value is safely supported because it\u0026#39;s a total function - all enum variants in this style of enumeration // can be cast to the respective integer value. println!(\u0026#34;Selected color is: {}\u0026#34;, Color::Blue as u8); This is totally fine and allowed by Rust because all variants of this C-style enumeration can be casted safely to an integer. We say it\u0026rsquo;s infallible\nHowever, casting an integer to an enumeration type is not infallible because not all possible integral values have a variant in the enumeration, we can\u0026rsquo;t do the following - it simply does not compile:\n// does not compile as integral to enum conversion is not implemented since it can fail for certain values of integer. let int_as_color = 10 as Blue; Rust is a safe and cautious language and the compiler will just not allow operations that could fail, therefore converting from enum to integral isn\u0026rsquo;t supported by default, however the enum creator can implement the TryFromtrait for any integral types.\nHowever this is boilerplate that\u0026rsquo;s already been done and available in a crate. Therefore to allow for C-style enums in Rust with safe conversions in both directions, it\u0026rsquo;s best to use https://crates.io/crates/num_enum\nTuple Enums In C++, the above is all you get, loosely typed integers that aren\u0026rsquo;t even that safe. With Rust, enums become more powerful with the ability to store different data inside of each variant.\nThese are similar to an algebraic data type such as Haskell has. It could be thought of similar to a union in C++ in that an element of the enum type takes up the amount of space required for the biggest variant (allowing it to store in an array), but strongly typed so you can\u0026rsquo;t access data you shouldn\u0026rsquo;t.\nAs an example, suppose we want to have an \u0026ldquo;Angle\u0026rdquo; enumeration. An angle could be stored in either Degrees or Radians, but both are stored as floats. In this case the tuple enum would look as follows:\nenum Angle { Degrees(f32), Radians(f32) } In both of these variants, we store an f32, but behind the scenes each instance of Angle is tagged with its discriminant (Degrees or Radians) and the only way to access the data inside is through pattern matching. This means we literally can\u0026rsquo;t access data that we shouldn\u0026rsquo;t for the variant we have. An example of implementing the Intotrait for this would be:\nimpl Into\u0026lt;f32\u0026gt; for Angle { fn into(self) -\u0026gt; f32 { match self { Self::Degrees(val) =\u0026gt; val, Self::Radians(val) =\u0026gt; val } } } An example of an enumeration with differing types could be a Color, where we can choose between different color formats:\nenum Color { RGBF32(f32, f32, f32), RGBAF32(f32, f32, f32, f32), RGBU8(u8, u8, u8), RGBAU8(u8, u8, u8, u8) } That is, we can choose between colors with RGB or RGBA components, and can choose the type of the components we\u0026rsquo;re storing. However because this is essentially a strongly typed union, and Rust requires all types to have a defined size to be stored, the largest size would still be picked, in this case each Color would be 16 bytes large (corresponding to the size of RGBAF32 which is largest).\nPattern matching works the exact same way. For example, a method on Color which can return whether the color supports transparency could be written as follows:\nimpl Color { fn supports_transparency(\u0026amp;self) -\u0026gt; bool { match self { Self::RGBF32(..) =\u0026gt; false, Self::RGBAF32(..) =\u0026gt; true, Self::RGBU8(..) =\u0026gt; false, Self::RGBAU8(..) =\u0026gt; true } } } Named Field Enums A variant in an enum can use the tuple syntax for defining the types it contains, and pattern matching as described above, however we can also store values associated with an enum variant by name in a record/struct like syntax. We can freely mix and match these on a per-variant basis. For example an enumeration which represents an Error. We might support storing an ErrorMessage, ErrorCode, or both. This might look as follows:\nenum Error { Message(String), Code(i32), Both { message: String, code: i32 } } When pattern matching on tuple types, we need to use the tuple patterns. When matching on record types, we need to use that syntax. An example of a function to try to get an error code from an Error would be:\nimpl Error { fn try_get_code(\u0026amp;self) -\u0026gt; Option\u0026lt;i32\u0026gt; { match self { Self::Message(..) =\u0026gt; None, Self::Code(c) =\u0026gt; Some(*c), Self::Both{code: c, ..} =\u0026gt; Some(*c) } } } Empty Enums!? Empty enums can\u0026rsquo;t be constructed. This may sound kind of pointless, what does\nenum Void { } Even mean if it can\u0026rsquo;t be constructed?\nAs it turns out this can be very useful for statically proving that we can\u0026rsquo;t ever take a particular branch of code in some cases, and is sometimes seen in generic code.\nFor example, in Rust we have the Result type which has two type parameters. One is the success type, and one is the error type. Say that a trait requires a return type of Result\u0026lt;SuccessType, ErrorType\u0026gt; from a function\ntrait TryOperation { type SuccessType; type ErrorType; fn try_operation(\u0026amp;mut self) -\u0026gt; Result\u0026lt;Self::SuccessType, Self::ErrorType\u0026gt; } In order to implement such a trait, we must provide an ErrorType to satisfy the signature of the trait, but for an infallible operation which is guaranteed to not fail, what do we choose for an ErrorType?. Any type is as good as any other type if we never need to construct it and we never return it:\nstruct InfallibleOperation { } impl TryOperation for InfallibleOperation { type SuccessType = (); // Unit type is like \u0026#34;void\u0026#34; in other languages, it\u0026#39;s a type we can use when we don\u0026#39;t need to return any information.  type ErrorType = ???; // What do we put here if this operation never fails?  fn try_operation(\u0026amp;mut self) -\u0026gt; Result\u0026lt;Self::SuccessType, Self::ErrorType\u0026gt; { println!(\u0026#34;Hello, World!\u0026#34;); } } The implementation of this function simply prints to the console and never fails, we\u0026rsquo;re \u0026ldquo;trying\u0026rdquo; to perform the operation but it will always succeed. It turns out in these cases we can communicate this at the type level to the caller by using the empty enum as the ErrorType (e.g. Void). If the caller sees that the signature for this is:\nfn try_operation(\u0026amp;mut self) -\u0026gt; Result\u0026lt;(), Void\u0026gt; Then they can easily see that the error case can never possibly happen (because an instance of Void physically cannot be constructed). In this case, the caller knows it\u0026rsquo;s safe to not even handle the possibility of error as this is ensured by the compiler in the types.\n","permalink":"https://forgottenmaster.github.io/posts/rust/whyrust/enums/","summary":"C Style Enums In C#, C++, and a lot of other popular programming languages, we have access to a type called an \u0026ldquo;enumeration\u0026rdquo; (or enum for short). This is simply a type safe collection of named constant values.\nFor example in C++, making an enum whose variants represent a set of allowed colors for a hypothetical UI framework could be written as (with the values of the variants explicitly typed out for transparency):","title":"Enums"},{"content":"The chain rule allows us to determine the derivative of a composite function as a product of the derivatives of each of the individual functions.\nThe reason this is a product is because, say you had a function \u0026ldquo;f\u0026rdquo; which has a derivative of 3 at a particular value of x. That means, when you increase the input by 1, the output increases by 3.\nThen say you have a second function \u0026ldquo;g\u0026rdquo; which has a derivative of 5 at the value output by \u0026ldquo;f\u0026rdquo; for x. This again means that when you increase the input by 1, then the output increases proportionally, this time by 5.\nHowever, the compound function \\( g \\circ f \\) will chain these two together in such a way that increasing the input to f increases the output of f by 3, and because they were chained, this is the same as increasing the input to g by 3.\nSince the gradient of g is 5 meaning that the output change is 5 times the change of the input, then raising the input to that function by 3 must result in a raise of the output by 15.\nSince derivatives are calculated as the gradient at a particular value, we need to know the inputs and outputs of both functions in order to calculate the partial derivatives.\nOnce we have these, we can just multiply them together.\nThe formula for the chain rule in this situation will be:\n$$ (g \\circ f)'(x) = g'(f(x)) \\times f'(x) $$\nThat is, calculating the rate of change (gradient) of the initial function f at the given value of x. After this, calculating the gradient of the second function, g, at the value output by f.\nTherefore in order to calculate these compound derivatives, we need to run two passes.\nThe first pass, called the forward pass, is where we simply are obtaining the input and output values for all functions involved. The second pass, called the backward pass, is where we use the inputs and outputs of the functions in order to calculate their gradients at that point. The final step is then just to multiply together to get the gradient of the composite function.\nThis two-pass sequence can be visualised with the following diagram. In it, we have three functions and an x value that\u0026rsquo;s passed in. we run the x value through the functions in order to get \\( f(x) \\), then \\( g(f(x)) \\), and finally \\( h(g(f(x))) \\).\nThe dashed lines represents the backward pass where we take the calculated values to determine the derivatives of f, g, and h at the appropriate input values.\n We can code up the calculation of the derivative of such a chain of functions as displayed in the diagram. For this section of code, we\u0026rsquo;ll assume the existence of a helper function called \u0026ldquo;derivative\u0026rdquo; which, given a function and a value at which the calculate the derivative, will approximate the derivative using the logic described in the previous post.\n// forward pass, calculating all inputs/outputs let x = 42.0; // initial input let f_of_x = f(x); // output from f (also input to g) let g_of_f_of_x = g(f_of_x); // output from g (also input to h) let h_of_g_of_f_of_x = h(g_of_f_of_x); output from h which is the final output // backward pass, calculating the derivatives of the functions at the calculated inputs let deriv_h = derivative(h, g_of_f_of_x); // calculate the derivative of h at the value that was output from g let deriv_g = derivative(g, f_of_x); // calculate derivative of g at the value that was output from f let deriv_f = derivative(f, x); // calculate derivative of f at the value of x  // multiply together to get the final rate of change of the output with change to x deriv_h * deriv_g * deriv_f ","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/chainrule/","summary":"The chain rule allows us to determine the derivative of a composite function as a product of the derivatives of each of the individual functions.\nThe reason this is a product is because, say you had a function \u0026ldquo;f\u0026rdquo; which has a derivative of 3 at a particular value of x. That means, when you increase the input by 1, the output increases by 3.\nThen say you have a second function \u0026ldquo;g\u0026rdquo; which has a derivative of 5 at the value output by \u0026ldquo;f\u0026rdquo; for x.","title":"1.3 - Chain Rule"},{"content":"Maths The derivative of a function is the rate at which the output changes with respect to a change in the input at a specific value for the input. This last part is important as the derivative of a function is essentially the gradient, or tangent of the graph of that function at a specific point which can of course change depending on where you are on the number line.\nThe formula for calculating the derivative of a given function \u0026ldquo;f\u0026rdquo; with respect to it\u0026rsquo;s input parameter \u0026ldquo;x\u0026rdquo;, at a given value of x which we call \u0026ldquo;a\u0026rdquo; here, can be written as:\n$$ \\frac{\\partial f}{\\partial x}(a) = \\lim_{\\Delta \\to 0} \\frac{f(a + \\Delta) - f(a - \\Delta)}{2 \\times \\Delta} $$\nLet\u0026rsquo;s break this down a little.\nWe\u0026rsquo;ll take the left hand side of the formula first, that is \\( \\frac{\\partial f}{\\partial x}(a) \\) which is the partial derivative of f with respect to x, at the given value of a. If we wanted to take another partial derivative of f, for example if the function took multiple parameters, the parameter we\u0026rsquo;re taking the partial derivative with respect to will appear as the denominator. For example, in a function such as:\n$$ f(x, y, z) = x + y + z $$\nThen the partial derivatives could be written as: \\( \\frac{\\partial f}{\\partial x} \\), \\( \\frac{\\partial f}{\\partial y} \\), and \\( \\frac{\\partial f}{\\partial z} \\)\nWhat\u0026rsquo;s a limit?\nThe first symbol in the right hand side is this \\( \\lim_{\\Delta \\to 0} \\) which means \u0026ldquo;as \\(\\Delta\\) approaches 0\u0026rdquo;. Limits are a way of talking about the way the function reacts to bringing certain parameters closer to a given limit. Essentially, the closer the parameter is to the limit, the more accurate the approximation is. In our case we talk about the function being as \\(\\Delta\\) approaches 0, the right hand side becomes more accurately the correct value for the derivative.\nOf course, \\(\\Delta\\) can never be 0, as this would result in a divide by zero operation which is undefined, but we can get arbitrarily close.\nWhat\u0026rsquo;s delta?\nThe symbol \\(\\Delta\\) is the greek symbol \u0026ldquo;Delta\u0026rdquo; which we use here to mean a change. Since to calculate the derivative we\u0026rsquo;re applying a small change to the value, and measuring how the output is affected by this small change.\nHow is the derivative calculated?\nThe rest of the right hand side is simply calculating the actual derivative at the value a, with the given \\(\\Delta\\) value. For a pretty close approximation of the true derivative, we can choose a very small value of delta, for example 0.001. We first add the small value to the value at which we\u0026rsquo;re calculating the derivative, this is the \\( f(a + \\Delta) \\)\nThis gives us the output of the function at a point ever so slightly in front of the test value (a). In order to measure a rate of change, two samples are required. We could have simply applied the function to a itself, however in order to account for asymmetry in the graph, we can get a better test by applying the function to the value located slightly behind the test value, this is \\( f(a - \\Delta) \\).\nFinally we take the difference of these two sample points, that is the difference in their outputs. This gives us the difference in the function output \\( f(a + \\Delta) - f(a - \\Delta) \\).\nIn order to get the gradient, once we have the change in the output of the function, we must divide by the change in the input. Since we both added and subtracted \\( \\Delta \\) to get our sample input points, the range (change in input) is \\( 2 \\times \\Delta \\)\n Code We can code up a function that can calculate the derivative of another function at a given input value, and given the value of the small delta we\u0026rsquo;re applying.\nThe function we write here will take a float in and output a float for simplicity. We could of course use generics to allow this to work for various types. With floats being the data type used, it would look as follows\nfn derivative(f: impl Fn(f64) -\u0026gt; f64, a: f64, delta: f64) -\u0026gt; f64 { let front_result = f(a + delta); let back_result = f(a - delta); let input_change = delta * 2.0; let output_change = front_result - back_result; output_change / input_change } An example call of finding the derivative of a function:\n$$ f(x) = x^2 $$\nat the input value 42 might look as follows:\nlet f = |x| x * x; let a = 42.0; let delta = 0.001; derivative(f, a, delta)  Diagram A derivative of a function being the rate of change of the output with change in the input can be visualised by the following diagram. This diagram shows a function f, which maps \u0026ldquo;x\u0026rdquo; to \u0026ldquo;y\u0026rdquo;, and shows a change in the input (dx) producing a change in the output (dy). The ratio of \\( \\frac{dy}{dx} \\) gives us the derivative\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/derivatives/","summary":"Maths The derivative of a function is the rate at which the output changes with respect to a change in the input at a specific value for the input. This last part is important as the derivative of a function is essentially the gradient, or tangent of the graph of that function at a specific point which can of course change depending on where you are on the number line.","title":"1.2 - Derivatives"},{"content":"There isn\u0026rsquo;t too much to say for what a function actually is, since being a programmer, we use them every day. However for completeness I\u0026rsquo;ll include a post here.\nFor our purposes it will help to think of a function as a black box, that takes one or more inputs and returns an output. We can then chain these functions together by taking the output of function 1, and passing it to the next function and so on.\nThe advantage of thinking of them as a chain like this rather than a stack as programmers are used to thinking of them, is that this chain will easily map to the concept of the forward pass of a neural network later on.\nAssuming the existence of the following two functions, each of which takes a single parameter:\n$$ f(x) = x^2 $$ $$ g(x) = x + 10 $$\nWe can code these functions as below, along with how they\u0026rsquo;re chained together to act as one big composite function that takes input into the first, and extracts output from the last:\nlet f = |x: f64| x * x; let g = |x: f64| x + 10.0; let input = 42.0; // input into the first function in the chain let intermediate = f(input); // after invoking function \u0026#34;f\u0026#34;, the output is an intermediate result let output = g(intermediate); // to get the final input, we can pass this intermediate result through \u0026#34;g\u0026#34;  A diagram showing function omposition would look something like this, where the chain of two functions results in the same as running through a single function that is the composite of the two\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/functions/","summary":"There isn\u0026rsquo;t too much to say for what a function actually is, since being a programmer, we use them every day. However for completeness I\u0026rsquo;ll include a post here.\nFor our purposes it will help to think of a function as a black box, that takes one or more inputs and returns an output. We can then chain these functions together by taking the output of function 1, and passing it to the next function and so on.","title":"1.1 - Functions"},{"content":" Ubisoft Reflections\nSenior Gameplay Programmer - Assassin\u0026rsquo;s Creed VR\nApril 2020 - Present\nAs a senior gameplay programmer I have been responsible for architecting and implementing large systems in Assassin\u0026rsquo;s Creed VR. Where the product statement isn\u0026rsquo;t clear or is incomplete, I have worked with other disciplines to ensure the final design does what is required. During this time I have gained experience in Unity3D including DOTS and best practices for efficient programs. Additionally, I attempt to always share my knowledge as I develop it myself so as to allow others to also build their skills. My main epiphany or learning from this project has been the now popular \u0026ldquo;Entity Component System\u0026rdquo; (ECS) architecture which I have put into practice in a large feature of the project.\nUbisoft Reflections\nGameplay Programmer - Tom Clancy\u0026rsquo;s The Division 2\nMarch 2018 - April 2020\nAs a gameplay programmer I have helped design and implement some fairly large systems for \u0026ldquo;Tom Clancy\u0026rsquo;s The Division 2\u0026rdquo; with the main feature being clans. As part of the clans feature I worked with designers, as well as the online team in order to develop clan missions and rewards, and clan spaces.\nUbisoft Reflections\nJunior Gameplay Programmer - Tom Clancy\u0026rsquo;s The Division\nNovember 2014 - March 2018\nIn this position I worked on \u0026ldquo;Tom Clancy\u0026rsquo;s The Division\u0026rdquo; as well as the associated expansion packs and DLC. I gained industry experience using C++ in a large codebase, and also with the in-house engine used to develop these (Snowdrop). I gained experience in using visual scripting and node graph systems.\n Eutechnyx Ltd.\nProgrammer - Auto Club Revolution\nJuly 2010 - November 2014\nWhile working at Eutechnyx I was part of the team developing \u0026ldquo;Auto Club Revolution\u0026rdquo;, a social network and gaming site revolving around car enthusiasts. I gained experience with front and back end web technologies using HTML/CSS/JavaScript for the frontend, and Python for the backend. Additional technologies used were MongoDB for the database management software, along with Redis for caching. Being a dynamic website, I gained experience interfacing with popular APIs including Facebook, and payment providers.\n Newcastle University\nMSc, Computer Games Engineering\n2008-2009\nDissertation titled \u0026ldquo;Games with a purpose\u0026rdquo;\nGraduated with merit\nNewcastle University\nBSc, Computing Science\n2005-2008\nDissertation titled \u0026ldquo;Video records of everyday life\u0026rdquo;\nGraduated with first class degree\n","permalink":"https://forgottenmaster.github.io/resume/","summary":"Ubisoft Reflections\nSenior Gameplay Programmer - Assassin\u0026rsquo;s Creed VR\nApril 2020 - Present\nAs a senior gameplay programmer I have been responsible for architecting and implementing large systems in Assassin\u0026rsquo;s Creed VR. Where the product statement isn\u0026rsquo;t clear or is incomplete, I have worked with other disciplines to ensure the final design does what is required. During this time I have gained experience in Unity3D including DOTS and best practices for efficient programs.","title":"Résumé"}]