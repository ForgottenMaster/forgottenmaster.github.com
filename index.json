[{"content":"In this post we\u0026rsquo;ll take a look at implementing the next (and final) level of abstraction for a neural network, the Network type itself.\nJust as how we encapsulated multiple operations in a Layer type, the Network is nothing more than a wrapper around multiple layers. However this isn\u0026rsquo;t entirely true, since the Network will also require a loss function in order to calculate the final single value measure of error for the predictions. We will then use the loss function also on the backward pass.\nThe way we train the network is the same as how we train the layers/operations and will consist of the following steps for a certain number of epochs:\n Feed the data batch into each layer, taking the output of that layer and passing it to the next, until the output of the final layer is captured. Feed the final output into the loss function, along with the expected targets to get the loss value. Perform the backward pass on the loss function to get the output gradient to feed to the last layer, and continue passing the input gradient of a layer to the output gradient of the previous one.  Additionally, we\u0026rsquo;ll need a way to get an iterator over all the parameters and parameter gradients of the network for training purposes, but as we\u0026rsquo;ll see, this is nothing more than a wrapper over the layers as well.\nThe Definition The type definition itself is very straightforward:\npub struct Network\u0026lt;T\u0026gt; { layers: Vec\u0026lt;Layer\u0026lt;T\u0026gt;\u0026gt;, loss: Loss\u0026lt;T\u0026gt;, } T here is, as in the layers/operations, the underlying element type of the Array type we\u0026rsquo;re using to store the data.\nFor the Network type itself, it\u0026rsquo;s just a Vec of layers, and a loss function as mentioned above.\nThe \u0026ldquo;Constructor\u0026rdquo; For abstraction to allow for future private implementation changes, we will provide the user with a \u0026ldquo;new\u0026rdquo; method to create the Network as we do with the other types.\nThe type of T for creating a new Network doesn\u0026rsquo;t need to be constrained, so the implementation is straightforward:\nimpl\u0026lt;T\u0026gt; Network\u0026lt;T\u0026gt; { pub fn new(layers: Vec\u0026lt;Layer\u0026lt;T\u0026gt;\u0026gt;, loss: Loss\u0026lt;T\u0026gt;) -\u0026gt; Self { Self { layers, loss } } } The rest of the methods we will implement require T to be clonable (due to the layers/operations needing to store off copies of their input for use on the backpropagation), these will therefore be in an impl block that looks like:\nimpl\u0026lt;T: Clone\u0026gt; Network\u0026lt;T\u0026gt; { ... } Forward Function We want a public function that represents the forward pass of the network, but doesn\u0026rsquo;t require a secondary testing array, since once the network is trained, we would like to use it to get predictions for new data such as from a testing data set.\nThis function does need to take a mutable reference to the Network since the forward pass causes copies of the input data to be stashed off in the layers/operations of the network. The signature for this function is pretty straightforward, just taking the input data and returning a batch of predictions:\npub fn forward(\u0026amp;mut self, batch: Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { ... } As for the implementation, this is little more than running through each of the layers of the network, and calling its \u0026ldquo;forward\u0026rdquo; method to obtain output from that layer, then passing it through to the next and so on. In Rust with the iterator API we can easily model this as a fold expression:\nself.layers .iter_mut() .fold(batch, |state, layer| layer.forward(\u0026amp;state)) Forward Loss Function While we\u0026rsquo;re training the network, we will want to run the forward pass, but also obtain the loss value at the end. This will never be called externally, and only is part of training the network, so it doesn\u0026rsquo;t need to be public.\nFor the signature, we will take the input batch as before, but also because we\u0026rsquo;re calculating the loss value will need to accept the array of targets as well.\nThe return value will be a tuple containing the predictions, and loss value.\nSince the function is very simple, I\u0026rsquo;ll just paste it in its entirety here:\nfn forward_loss(\u0026amp;mut self, batch: Array\u0026lt;T, Ix2\u0026gt;, targets: Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; (Array\u0026lt;T, Ix2\u0026gt;, T) { let predictions = self.forward(batch); let loss = self.loss.forward(predictions.clone(), targets); (predictions, loss) } Backward Pass Now that we have the forward pass covered, we need to look at the backward pass. Again, this is a private helper function that\u0026rsquo;s only called as part of the training process and not independently. For the backward pass, we don\u0026rsquo;t actually need to take any arguments.\nWe do however still need mutable access to the Network since the backward pass will calculate and update parameter gradients as it goes through.\nAs with the forward pass, this is modeled with a fold expression, calling the \u0026ldquo;backward\u0026rdquo; function on each layer with the result of the previous one. The code looks as follows:\nfn backward(\u0026amp;mut self) { self.layers .iter_mut() .rev() .fold(self.loss.backward(), |gradient, layer| { layer.backward(\u0026amp;gradient) }); } A couple of points to talk about:\n We need to run through the layers in reverse order, because this is the backward pass. This is achieved by the rev() iterator adapter call. In order to get the initial gradient, we call backward on the loss function. For the rest, we pass it through the layer.  Training When we train the network, we are always performing a forward pass, followed by a backward pass. Therefore the public API for training the network doesn\u0026rsquo;t expose the backward, or forward loss functions since the caller should only use the train function.\nThis function will take the input data, and target array. It will run the forward pass, followed by a backward pass. Finally it will return the loss that was determined from the forward pass because the training code may use this.\nThe function itself is just a wrapper around the forward_loss and backward function calls and looks as follows:\npub fn train(\u0026amp;mut self, batch: Array\u0026lt;T, Ix2\u0026gt;, targets: Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; T { let (_, loss) = self.forward_loss(batch, targets); self.backward(); loss } Parameter \u0026amp; Gradient Access This is the last part that\u0026rsquo;s missing, which is the ability after training for an epoch, to obtain the parameters and parameter gradients from the network so that they can be optimized (e.g. via gradient descent).\nAgain, the function itself is very simple since all the complexity is now buried in the lower levels. Network is a wrapper, or composite over the layers and loss function, and so we\u0026rsquo;re just composing the parameters and gradients from the layers.\nWhat we want, is a parameters_and_gradients method that will return an iterator over each parameter and gradient in the first layer, then in the second, and so on.\nWe can achieve this in Rust by using the iterator adapter flat_map.\nflat_map takes each element of the iterator it\u0026rsquo;s adapting (in our case an iterator over the layers), and calls a closure on it. This closure returns an iterator, which Rust will chain together.\nFor our case, calling parameters_and_gradients on a Layer also returns an iterator, so it fulfils the contract for flat_map.\nThe full function then looks as follows:\npub fn parameters_and_gradients( \u0026amp;mut self, ) -\u0026gt; impl Iterator\u0026lt;Item = (\u0026amp;mut Array\u0026lt;T, Ix2\u0026gt;, \u0026amp;Array\u0026lt;T, Ix2\u0026gt;)\u0026gt; { self.layers .iter_mut() .flat_map(|layer| layer.parameters_and_gradients()) } Conclusion And that\u0026rsquo;s all there is to the Network abstraction, just a bunch of composing and encapsulation!.\nThe final part of the implementation is to create types responsible for running the training of the Network, and for optimizing the parameters given the calculated gradients.\nWe\u0026rsquo;ll look at implementing these in the next post.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/network/","summary":"In this post we\u0026rsquo;ll take a look at implementing the next (and final) level of abstraction for a neural network, the Network type itself.\nJust as how we encapsulated multiple operations in a Layer type, the Network is nothing more than a wrapper around multiple layers. However this isn\u0026rsquo;t entirely true, since the Network will also require a loss function in order to calculate the final single value measure of error for the predictions.","title":"The Network Type"},{"content":"After trying to make a start on the next level of abstraction with the NeuralNetwork class, it\u0026rsquo;s becoming apparent that dealing with lifetimes at the lower levels of abstraction (Operation and Layer trait objects) is causing it more difficult to make a nice abstraction at the upper level. We will take a brief interlude to look at what the problem is, and why we need to do a refactor at this point.\nThe Problem The problem we run into with lifetimes is that we are required to keep the items for which we are borrowing around at least as long as the struct we\u0026rsquo;re instantiating and when we use the literal reference extension \u0026ldquo;feature\u0026rdquo; of Rust, it\u0026rsquo;s simply syntactic sugar.\nFor example, taking an example from one of the previous tests if we have a line such as:\nlet net = \u0026amp;mut [ Operation::new_parameter(\u0026amp;WeightMultiply, weights), Operation::new_parameter(\u0026amp;BiasAdd, bias), Operation::new_basic(\u0026amp;Sigmoid), Operation::new_basic(\u0026amp;Linear), ]; Rust compiler will see that this is using temporary literals, and everything is all in the same scope, it can see that net here is a mutable reference, which means that nothing else can modify it, and more importantly it knows that net can\u0026rsquo;t be moved due to the reference. The compiler is fine with this because it can desugar it by putting all these temporaries inside let bindings:\nlet _1 = WeightMultiply; let _2 = BiasAdd; let _3 = Sigmoid; let _4 = Linear; let mut _5 = [ Operation::new_parameter(\u0026amp;_1, weights), Operation::new_parameter(\u0026amp;_2, bias), Operation::new_basic(\u0026amp;_3), Operation::new_basic(\u0026amp;_4) ]; let net = \u0026amp;mut _5; And everything is happy here because the temporaries are literals and can be evaluated/verified to exist at compile time. It can be desugared into local variables, and doesn\u0026rsquo;t have to worry about the network moving without the data inside because it can\u0026rsquo;t be moved due to us only having access to a borrow.\nHowever if we take a look at the following snippet, which is a very stripped down version what we would like to have for the next abstraction (the neural network):\nstruct Network\u0026lt;\u0026#39;a, T\u0026gt; { layers: Vec\u0026lt;Layer\u0026lt;\u0026#39;a, T\u0026gt;\u0026gt; } #[test] fn test_throwaway() { let n: Network\u0026lt;\u0026#39;_, f64\u0026gt; = Network { layers: vec![Layer::new(1, \u0026amp;DenseLayerSetup::new(\u0026amp;Sigmoid))] }; println!(\u0026#34;{:?}\u0026#34;, n.layers.len()); } As we can see, we simply would like a Vec of Layers in our Network. However the important thing here is the following:\n\u0026amp;DenseLayerSetup::new(\u0026amp;Sigmoid) As mentioned in the previous test, Sigmoid is a literal and so the Rust compiler will extend it with the reference. However, in our case we aren\u0026rsquo;t creating a DenseLayerSetup literal. To do so, we would need access, and specify all the fields inside a DenseLayerSetup, but we don\u0026rsquo;t want to do this as it\u0026rsquo;s exposing implementation details and can cause code to break if the structure of the\u0026hellip;..structure changes.\nInstead, we use the \u0026ldquo;new\u0026rdquo; associated method to create a new instance. The problem here then, is this is no longer a literal, it returns an instance of DenseLayerSetup created at runtime and not (as in the case of the above test with literal extension) at compile time.\nSince it\u0026rsquo;s a runtime expression, Rust can no longer use its lifetime extension desugaring to store off the temporary.\nIf we try to compile this code, we get the following error message indicating that the temporary returned from the new function doesn\u0026rsquo;t live long enough:\nSolution #1 - Get the API user to store intermediate values The first of the two solutions we can use to get around this is simply to not care, continue using lifetimes all the way up the abstraction tree, and instead of having a Vec of layer instances, we instead have a slice of layer instances. In this solution we would change the definition of the Network structure to be thus:\nstruct Network\u0026lt;\u0026#39;a, T\u0026gt; { layers: \u0026amp;\u0026#39;a mut [Layer\u0026lt;\u0026#39;a, T\u0026gt;] } This will work, for sure, but it requires that the caller ensure that the storage for the slice is as long as the Network itself. This leads to code that isn\u0026rsquo;t as succinct, such as:\nlet dense_layer_setup = DenseLayerSetup::new(\u0026amp;Sigmoid); let mut layers = [Layer::new(1, \u0026amp;dense_layer_setup)]; let n: Network\u0026lt;\u0026#39;_, f64\u0026gt; = Network { layers: \u0026amp;mut layers }; Which has a couple of issues:\n It\u0026rsquo;s not exactly nice to write. We would like to be able to write the whole network construct as a single expression. We can\u0026rsquo;t move the network around. Since it doesn\u0026rsquo;t own the data, it\u0026rsquo;s kind of pinned in place by virtue of the fact that the data it\u0026rsquo;s borrowing is not movable (can\u0026rsquo;t be moved while borrowed).  Note that neither is necessarily a problem, and in environments where we have no heap allocation it might even be necessarily to give stack control to the caller. However, we are trying to write a nice, usable API and as such, these limitations are not really acceptable for us.\nSolution #2 - Use boxing instead of borrows The other option is for the structures to own their component parts instead of borrowing them. This will also remove the requirement to have an explicit lifetime annotation since owning the data means it is guaranteed to live as long as the object itself.\nIn order to do this with Rust, we simply use the Box type. We can use the Box::new constructor to get a boxed/owned trait object rather than taking a reference to it. We can also use Vec rather than slice to get an owned collection of things. Thus, instead of doing the following:\nlet dense_layer_setup = DenseLayerSetup::new(\u0026amp;Sigmoid); let mut layers = [Layer::new(1, \u0026amp;dense_layer_setup)]; let n: Network\u0026lt;\u0026#39;_, f64\u0026gt; = Network { layers: \u0026amp;mut layers }; We would simply like to be able to do:\nlet n: Network\u0026lt;f64\u0026gt; = Network { layers: vec![Layer::new( 1, Box::new(DenseLayerSetup::new(Box::new(Sigmoid))), )], }; Replacing the references with boxed and owned instances. However, we can do one better by having the new function return the new instance already boxed. However, to make it explicit, we\u0026rsquo;ll call it new_boxed as it\u0026rsquo;s good practice for a function named \u0026ldquo;new\u0026rdquo; to return an instance of the exact type it\u0026rsquo;s called on. The proposed API would then be something like:\nlet n: Network\u0026lt;f64\u0026gt; = Network { layers: vec![Layer::new( 1, DenseLayerSetup::new_boxed(Sigmoid::new_boxed()), )], }; We could go one step further and have the DenseLayerSetup internally construct the Sigmoid activation function trait object here for storage, and rely on generics. This would result in code looking like the following:\nlet n: Network\u0026lt;f64\u0026gt; = Network { layers: vec![Layer::new( 1, DenseLayerSetup::new_boxed::\u0026lt;Sigmoid\u0026gt;(), )], }; However there are a couple of issues with this:\n It doesn\u0026rsquo;t support operations/activation functions with additional setup. It relies entirely on the presence of the Default trait/method. It requires all types implementing BasicOperation to then implement Default, which might not be wanted (they might want to ensure that data is provided to construct an instance).  Therefore we won\u0026rsquo;t take this step, and stick with having it passed in.\nSolution #3 - Insanity There is a third solution which allows the network to be constructed with the nice syntax, and which can be moved around and such, however the code becomes much less readable, especially when looking at the types of layers/operations.\nThis solution is of course to use generics such that the type of the neural network overall statically encodes the concrete setup of the network.\nI will be ignoring this as a viable solution, at least until finishing going through the book with a more traditional runtime solution, however I may revisit it in the future as a fun exercise.\nSolution Choice All things considered the best choice for this API for the time being is solution number 2. We are assuming we\u0026rsquo;re running on hardware that has an allocator setup in some fashion, and we\u0026rsquo;re assuming that the allocations will be very small.\nAdditionally, we only allocate once up front when constructing the neural network and after that don\u0026rsquo;t need any further allocations. This solution also allows for easy API usage, especially if we add the new_boxed methods to remove the visual clutter of Box::new calls.\nFollowing is a brief overview of the changes that need to be made.\nOperationData/Operation We change the OperationData structure so as to not accept a reference trait object but a boxed one. These BasicOperation and ParameterOperation instances are usually (if not always) going to be zero sized types, since they don\u0026rsquo;t store any state themselves and only exist to act as a vtable for the actual calculations of the operation.\nIn Rust, boxing a ZST is guaranteed to not cause an allocation, therefore we introduce no allocations at this level (in general, but someone could still theoretically provide a BasicOperation or ParameterOperation that does have data).\nWe are able to remove the lifetime annotations since the trait object will now be owned rather than borrowed. Operation data will then change from:\nenum OperationData\u0026lt;\u0026#39;a, T\u0026gt; { Basic(\u0026amp;\u0026#39;a dyn BasicOperation\u0026lt;T\u0026gt;), Parameter { operation: \u0026amp;\u0026#39;a dyn ParameterOperation\u0026lt;T\u0026gt;, parameter: Array\u0026lt;T, Ix2\u0026gt;, parameter_gradient: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, }, } To:\nenum OperationData\u0026lt;T\u0026gt; { Basic(Box\u0026lt;dyn BasicOperation\u0026lt;T\u0026gt;\u0026gt;), Parameter { operation: Box\u0026lt;dyn ParameterOperation\u0026lt;T\u0026gt;\u0026gt;, parameter: Array\u0026lt;T, Ix2\u0026gt;, parameter_gradient: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, }, } Which then also lets us remove the explicit lifetime annotation from Operation as well, since it only existed to relay that information into the OperationData type being used. Therefore the new Operation will become:\npub struct Operation\u0026lt;T\u0026gt; { input: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, input_gradient: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, output: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, data: OperationData\u0026lt;T\u0026gt;, } We also replace the references with boxed trait objects in the new_basic and new_parameter functions, but we don\u0026rsquo;t need to show this here.\nLayerSetup The LayerSetup trait previously had an explicit lifetime parameter so as to be able to return a Vec of Operation with the correct lifetime. However we wish to remove this lifetime annotation which we now can do since Operation no longer takes one.\nThe new definition of the LayerSetup trait then becomes:\npub trait LayerSetup\u0026lt;T\u0026gt; { fn layer_setup( \u0026amp;self, num_neurons: usize, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, seed: \u0026amp;Option\u0026lt;u64\u0026gt;, ) -\u0026gt; Vec\u0026lt;Operation\u0026lt;T\u0026gt;\u0026gt;; } DenseLayerSetup Previously this took a borrow to a BasicOperation to be used at the end of the layer (mimicking how OperationData::Basic had a borrow to one). As part of this lifetime annotation removal, we wish to hold a boxed instance (which doesn\u0026rsquo;t cause an allocation generally if BasicOperation is a ZST).\nThe new definition of DenseLayerSetup then is simply:\npub struct DenseLayerSetup\u0026lt;T\u0026gt;(Box\u0026lt;dyn BasicOperation\u0026lt;T\u0026gt;\u0026gt;); Layer Previously the Layer struct held a borrow to the LayerSetup trait object like in the other cases, but we will replace this with a boxed trait object and can then remove the lifetime annotation as it\u0026rsquo;s no longer needed.\nThe new definition of Layer then becomes:\npub struct Layer\u0026lt;T\u0026gt; { num_neurons: usize, operations: Option\u0026lt;Vec\u0026lt;Operation\u0026lt;T\u0026gt;\u0026gt;\u0026gt;, setup: Box\u0026lt;dyn LayerSetup\u0026lt;T\u0026gt;\u0026gt;, output: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, seed: Option\u0026lt;u64\u0026gt;, } Loss Finally, the Loss struct is also storing its vtable/trait object by borrow. We can do the same kind of replacement as elsewhere there too and end up with:\npub struct Loss\u0026lt;T\u0026gt; { implementation: Box\u0026lt;dyn LossImpl\u0026lt;T\u0026gt;\u0026gt;, prediction: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, target: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, } Conclusion After changing these types, we then need to go through the compiler errors that we now get as a result of the change from reference to owned box. While we\u0026rsquo;re doing this, we will make it so that all the appropriate types have a \u0026ldquo;new_boxed\u0026rdquo; associated method for convenience.\nThis means that instead of writing Box::new(Foo::new()) people can then just write Foo::new_boxed() which is a little leaner.\nAfter these changes we can now use the API in a way we expect, for example the following code now compiles where it wouldn\u0026rsquo;t in the previous version:\nstruct Network\u0026lt;T\u0026gt; { layers: Vec\u0026lt;Layer\u0026lt;T\u0026gt;\u0026gt;, } #[test] fn test_throwaway() { let n: Network\u0026lt;f64\u0026gt; = Network { layers: vec![Layer::new( 1, DenseLayerSetup::new_boxed(Sigmoid::new_boxed()), )], }; println!(\u0026#34;{:?}\u0026#34;, n.layers.len()); } ","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/lifetimerefactor/","summary":"After trying to make a start on the next level of abstraction with the NeuralNetwork class, it\u0026rsquo;s becoming apparent that dealing with lifetimes at the lower levels of abstraction (Operation and Layer trait objects) is causing it more difficult to make a nice abstraction at the upper level. We will take a brief interlude to look at what the problem is, and why we need to do a refactor at this point.","title":"Lifetime Refactor"},{"content":"Overview Once we\u0026rsquo;ve passed the data through the hidden layers of our neural network, and passed it through a final output layer (which should have only 1 neuron to bring the dimensionality of the data back to 1 feature/column) we will end up with a vector of predictions as mentioned previously (1 for each observation in the data). We already have a vector of targets which are the actual measured answers. We know that we will need to calculated derivatives on the backpropagation pass, but how do we know what to use as an initial value?\nThis is where the \u0026ldquo;loss function\u0026rdquo; (or error function) comes in, which will take the vector of predictions, and a vector of targets, and will produce a single number which identifies just how far away our predictions are from the target values.\nThis loss function will be implemented in a similar fashion to how we have done Operations and Layers so far, in that it will provide the framework and public API to call for use in the forward and backward passes, as well as asserting the shapes of arrays along the way.\nIt will delegate the actual calculations to another vtable like trait object though for customisation.\nLossImpl First of all we define a trait for us to use to wrap a trait object. This allows the customisation point for choosing exactly what kind of calculation we would like. This trait is parameterised over the type T (type of the Array elements in use) as the others have been, but otherwise looks fairly uneventful:\npub trait LossImpl\u0026lt;T\u0026gt; { ... } This trait will define two functions. The first is a function that will calculate a single value (of type T of course) when given the Array of predictions and targets. The signature looks as expected from such a function:\nfn calculate_output(\u0026amp;self, prediction: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, target: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; T; That function will be used on the forward pass in order to calculate the single value, total \u0026ldquo;error\u0026rdquo;. The other function we will need is a way of calculating what the gradients should be of the predictions (since these are what are varying and calculated) to kick off the backpropagation step. This function also takes the predictions and targets arrays, except that it will produce an Array of gradients, of the same shape as predictions:\nfn calculate_input_gradient( \u0026amp;self, prediction: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, target: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; MeanSquaredError This is an implementation of a loss function, and is one of the most commonly used ones, the average squared error. This is popular also because it\u0026rsquo;s easy to calculate the gradient for as we\u0026rsquo;ll see soon. Like the customisation points for Operation and Layer, this type doesn\u0026rsquo;t need to store any state, so is defined as a simple unit struct:\npub struct MeanSquaredError; Next we will need to implement the LossImpl trait for it so we can use it as a trait object. We need to place some generic bounds on this though for T, so I\u0026rsquo;ll show the impl block first, and then explain why we need the bounds:\nimpl\u0026lt;T: LinalgScalar + From\u0026lt;u32\u0026gt; + ScalarOperand\u0026gt; LossImpl\u0026lt;T\u0026gt; for MeanSquaredError { }  LinalgScalar - This trait is required for the same reason as we needed it in our Operation implementations. It allows for us to use arithmetic operations on Array\u0026rsquo;s of this type, which will be required to calculate the mean squared error. From - We need to be able to convert from numbers into our type T at a couple of points. One is when calculating the gradient, we need to be able to multiply the error by 2. However most types implement multiplication with both operands as the same type, therefore we need to be able to convert the integer 2 into an instance of type T. Additionally, we want to divide by the number of entries, which again should be an instance of type T. The downside here is that array.nrows() will give a usize, but we use Frominstead because floating point types implement it not conversion from usize. The program will crash if there are more than u32 observations in the data. ScalarOperand - This is required because we multiply an array by a scalar in calculating the input gradient.  With that out of the way, we can implement the two functions!\nFirst up is the calculate_output function. In the case of the mean squared error, this is just finding the difference between prediction and target arrays, then squaring that difference (we can use the elementwise multiplication operator * on the error array). Then we take the squared error Array and find its sum. Finally we divide by the number of rows. The code is fine to follow so I\u0026rsquo;ll just post it here:\nfn calculate_output(\u0026amp;self, prediction: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, target: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; T { let error = prediction - target; let squared_error = \u0026amp;error * \u0026amp;error; let squared_error_sum = squared_error.sum(); squared_error_sum / (prediction.nrows() as u32).into() } As mentioned above, in the last line you can see that in order to divide by the number of rows (count), we need to turn that number into an instance of type T. We can do this because we put the trait bound Fromwhich allows us to use the \u0026ldquo;into\u0026rdquo; function on a u32 to get an instance of T to divide by.\nThe second function is also kind of simple. The formula for calculating the gradient is 2 * (prediction - target) / count. There\u0026rsquo;s another instance of converting the count into an instance of type T, however because we\u0026rsquo;re also multiplying by 2, we need to get an instance of type T that represents the number \u0026ldquo;2\u0026rdquo;:\nfn calculate_input_gradient( \u0026amp;self, prediction: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, target: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { let two_of: T = 2.into(); let error = prediction - target; let count: T = (prediction.nrows() as u32).into(); let average_error = error / count; average_error * two_of } Loss For the actual Loss type, we would like to allow customization via specifying a LossImpl trait object as a \u0026ldquo;vtable\u0026rdquo; as previously described. Additionally, on the forward pass we will need to cache the predictions and targets in order to calculate the gradient with the last used pair on the backward pass. As with Operation and Layer, these are initially None and then on the first forward pass will be set to Some. Therefore they are of type Option. As our LossImpl trait object is a reference that can be on the stack, we will require a lifetime annotation (\u0026lsquo;a in the code). The full definition then looks as follows:\npub struct Loss\u0026lt;\u0026#39;a, T\u0026gt; { implementation: \u0026amp;\u0026#39;a dyn LossImpl\u0026lt;T\u0026gt;, prediction: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, target: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, } The first function we will need is a way to construct a new instance. We will do this with the idiomatic \u0026ldquo;new\u0026rdquo; function. The only thing we are required to provide when constructing one is the LossImpl trait object as the other two fields will be initialized to None like so:\npub fn new(implementation: \u0026amp;\u0026#39;a dyn LossImpl\u0026lt;T\u0026gt;) -\u0026gt; Self { Self { implementation, prediction: None, target: None, } } The second function we need is a function to call on the forward pass to calculate the total loss value. For this we will pass it the predictions and targets arrays, which are required to be the same size. We delegate the actual output calculation to the LossImpl provided. We will also stash the provided predictions and targets for use in the backward pass. The full function is fairly simple so I\u0026rsquo;ll list it here:\npub fn forward(\u0026amp;mut self, prediction: Array\u0026lt;T, Ix2\u0026gt;, target: Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; T { assert_eq!(prediction.raw_dim(), target.raw_dim()); let loss_value = self.implementation.calculate_output(\u0026amp;prediction, \u0026amp;target); self.prediction = Some(prediction); self.target = Some(target); loss_value } The final method we need is to perform the backwards pass. This is also simple and just passes the previously stashed predictions and targets from the forward pass into the input gradient calculation function of LossImpl. We can use the standard \u0026ldquo;as_ref().unwrap()\u0026rdquo; pattern on Optionto get a \u0026amp;T from it. After we get the input gradient back from the implementation, we need to check that the shape matches that of the predictions since we\u0026rsquo;ll be starting the backpropagation with it. The function implementation is then as follows:\npub fn backward(\u0026amp;mut self) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { let input_gradient = self.implementation.calculate_input_gradient( self.prediction.as_ref().unwrap(), self.target.as_ref().unwrap(), ); assert_eq!( self.prediction.as_ref().unwrap().raw_dim(), input_gradient.raw_dim() ); input_gradient } ","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/lossfunction/","summary":"Overview Once we\u0026rsquo;ve passed the data through the hidden layers of our neural network, and passed it through a final output layer (which should have only 1 neuron to bring the dimensionality of the data back to 1 feature/column) we will end up with a vector of predictions as mentioned previously (1 for each observation in the data). We already have a vector of targets which are the actual measured answers.","title":"Loss Function"},{"content":"Overview The lowest level of abstraction as mentioned in the previous post is the operation. These were the lowest level building block that is able to perform a single unit of calculation on the forward pass, and single unit of partial derivative calculation on the backward pass.\nDuring that chapter, we could see how multiple of these operations are able to be chained together, though we don\u0026rsquo;t want to do this by hand. Instead, we would like an abstraction that has the same interface as the operations (a forward and backward method) but which internally handles routing the data through the operations that are part of the layer.\nWe define a layer as a series of linear operations, followed by a final (optional) non-linear operation known as the \u0026ldquo;activation function\u0026rdquo;.\nIn the context of the simple neural network implemented in the last chapter, we had 5 operations that were:\n A WeightMultiply operation which takes the initial input and selection of weights and multiplies them together, ending with a matrix that represents the weighted sum for each of the \u0026ldquo;learned features\u0026rdquo; that we wanted in this \u0026ldquo;hidden layer\u0026rdquo;. A BiasAdd operation which added in the bias term to the weighted multiplication. A Sigmoid function that took the results and \u0026ldquo;squashed\u0026rdquo; them to a non-linear range between 0 and 1. A WeightMultiply operation which took the inputs from the learned features, and multiplied them with a second set of weights. A BiasAdd operation which added in the bias term to the calculated weighted sum (which was a single value at this point).  We can see that there are 2 layers here:\n Operations 1 to 3 comprise the first layer which takes the initial input, operates on it, and produces values for a set of learned features (or as we\u0026rsquo;ll use here, \u0026ldquo;neurons\u0026rdquo;) Operations 4 and 5 will comprise the final layer which takes the learned hidden features to the final output  A diagram below shows this grouping of operations (with the loss function tacked onto the end):\nNeurons Rather than looking at the individual operations within the layers, we instead look at neural networks at the abstraction of these layers. A layer is, as mentioned previously a series of non-linear operations followed by a linear one, but another important facet of a layer is the number of neurons it contains.\nIn the brain, neurons will take electrical impulses from other connected neurons and will fire if that energy exceeds some certain threshold (which then causes other neurons to possibly fire, etc.). In the case of artificial neural networks, this activation is modeled by transformation of the input to an output through a non-linear function (the Sigmoid function for example), which we also can call the activation function for a layer. The values coming out of this are the activations for that layer.\nThe number of neurons in an artificial neural network will correspond to the number of features in the input. Therefore in the simple neural network seen in the last chapter, and whose operations are briefed above we can see that:\n The number of neurons in the input layer is X where X is the number of features in the data set. The number of neurons in the hidden layer is N where N is some number that we define. The output from this layer will be a matrix with the number of rows as in the input, and the number of columns equal to the neuron count for that layer. The number of neurons from the output layer is 1. This is because we\u0026rsquo;re collapsing the values down to a single prediction for each observation (to allow us to use the loss function to detect error).  Deep learning is simply using neural networks with more than one hidden layer.\nA more useful diagram below doesn\u0026rsquo;t show the operations inside the layers, but abstracts the layers only to show their inputs and outputs:\nA small detour Before delving into the implementation for the Layer abstraction, we first address a need for a function that will be required to be added to the \u0026ldquo;Operation\u0026rdquo; type that we implemented in the previous section.\nFor ParameterOperation instances we stored the parameter being used, along with the last computed gradient for the parameter in the backward pass. One thing that we will need and will lift to the Layer abstraction is the ability to access temporarily the parameter and parameter gradients in use.\nIn order to achieve this, we will add the following function to the Operation type:\npub fn parameter_and_gradient(\u0026amp;mut self) -\u0026gt; Option\u0026lt;(\u0026amp;mut Array\u0026lt;T, Ix2\u0026gt;, \u0026amp;Array\u0026lt;T, Ix2\u0026gt;)\u0026gt; { ... } This function will return the parameter as a mutable reference, since we will eventually need to be able to update the parameter, given the gradient. However the gradient doesn\u0026rsquo;t need to be mutable so we return an immutable reference.\nOne thing to note here is that not all operations have a secondary parameter (those that are BasicOperations) - therefore we return an Option to allow for this possibility.\nThe implementation of this function then look as follows:\nmatch \u0026amp;mut self.data { OperationData::Parameter { parameter, parameter_gradient, .. } =\u0026gt; Some((parameter, parameter_gradient.as_ref().unwrap())), _ =\u0026gt; None, } That is, we must match on the OperationData enum value we have. In the case of it being a parameter operation, we can get the references we want, and return these in a \u0026ldquo;Some\u0026rdquo; variant. If not, then we return None as there\u0026rsquo;s no parameters to get.\nLayerSetup Just as we have the BasicOperation and ParameterOperation traits to allow for customization of the calculations for specific operations, we would also like to define a static structure for a Layer while allowing construction of that layer (determining which operations, etc.) to be customized.\nThis is the purpose of the LayerSetup trait. We will shortly see that the structure of Layer is static while delegating the actual setup of the layer to a trait object implementing this trait.\nThe trait header looks as follows:\npub trait LayerSetup\u0026lt;\u0026#39;a, T\u0026gt; { ... } This again requires the same generic parameters that are required by Operation. That is:\n \u0026lsquo;a - This is the lifetime of the borrow for the BasicOperation or ParameterOperation vtables in use T - This is the type of the elements inside the ndarray::Array  Looking at the trait itself, it only requires implementors to provide the following function:\nfn layer_setup( \u0026amp;self, num_neurons: usize, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, seed: \u0026amp;Option\u0026lt;u64\u0026gt;, ) -\u0026gt; Vec\u0026lt;Operation\u0026lt;\u0026#39;a, T\u0026gt;\u0026gt;; This is a function that, given the number of neurons/features that the layer should have, along with the input (used to determine the number of features in the input) can return the list of Operations required in that layer.\nThe additional parameter is an optional seed used for random number generation. If this is None then it\u0026rsquo;s expected that the numbers should be truly randomly generated, if it\u0026rsquo;s Some then the seed shouild be used to construct the PRNG stream.\nDenseLayerSetup Previously we saw that the layers (the hidden and output layers) in our neural network fully connects the neurons of the previous layers and consists of a sequence of WeightMultiply -\u0026gt; BiasAdd -\u0026gt; ActivationFunction where ActivationFunction is some non-linear function. For the hidden layer we used Sigmoid and for the output layer we used a Linear activation function which is just a passthrough function.\nHowever the basic concept of fully connecting the neurons of the previous layer, and using a weighted sum and bias addition is consistent between layers. The activation function needs to be tweakable.\nThe terminology for these fully connected layers is a dense layer (due to it being densely connected).\nThis, therefore will be known as the DenseLayerSetup.\nAs mentioned above, we will need to modify the activation function in use, so we will keep this as a field. As before, we will need to make it generic over the lifetime, \u0026lsquo;a, and the type T. We know that the activation function should be one that doesn\u0026rsquo;t take an additional parameter (if this proves an incorrect assumption we can change later), so we can take just the BasicOperation trait object rather than a wrapped Operation.\nThe structure definition therefore is as follows:\npub struct DenseLayerSetup\u0026lt;\u0026#39;a, T\u0026gt;(\u0026amp;\u0026#39;a dyn BasicOperation\u0026lt;T\u0026gt;); Next, we want a simple constructor function for this. We don\u0026rsquo;t want to make the fields public since they could be changed after construction, so instead we keep the field private and provide a constructor function for external use.\nimpl\u0026lt;\u0026#39;a, T\u0026gt; DenseLayerSetup\u0026lt;\u0026#39;a, T\u0026gt; { pub fn new(activation: \u0026amp;\u0026#39;a dyn BasicOperation\u0026lt;T\u0026gt;) -\u0026gt; Self { Self(activation) } } Finally, we have the actual trait implementation. First of all, we have the header for the impl block:\nimpl\u0026lt;\u0026#39;a, T: LinalgScalar\u0026gt; LayerSetup\u0026lt;\u0026#39;a, T\u0026gt; for DenseLayerSetup\u0026lt;\u0026#39;a, T\u0026gt; where Standard: Distribution\u0026lt;T\u0026gt;, { ... } A couple of things to note here:\n We require a trait bound of LinalgScalar on the generic parameter, T. This is because the layer needs to use the WeightMultiply and BiasAdd operations that both put a restriction of LinalgScalar on their generic parameter (due to requiring the arithmetic operations, etc.) We are using random generation for the initial parameters on the weight and bias operations. In order to use this random number generation, we require that our type T is such that the structure \u0026ldquo;Standard\u0026rdquo; implements Distribution.  Next is the actual function implementation. I won\u0026rsquo;t repeat the function signature here as it was specified in the LayerSetup section.\nThe first step in the implementation is to get the random number generator instance. This is going to depend on if there\u0026rsquo;s a random seed specified or not. If there is, we will use it to construct the generator. If not, we get a truly random generator. This code looks as followws:\nlet mut r = match seed { Some(seed) =\u0026gt; StdRng::seed_from_u64(*seed), None =\u0026gt; StdRng::from_rng(thread_rng()).unwrap(), }; Next, we need to generate the random weights and bias values to use as parameters to the appropriate operations. The steps for this are as follows:\n We figure out what dimensions we require for the weights matrix. The number of rows in the weights matrix will match the number of columns/features in the input data - this allows us to perform the matrix multiplication. The number of columns in the weights matrix will then be the number of output neurons we require in the layer. Calculate the number of elements in the matrix which we use for creating the buffer in the next step. This is just the number of rows multiplied by number of columns. Create a vector with a number of elements calculated in step #2. Each element is a randomly generated value using the generator we made previously. In order to make these, we can use the repeat_with function from std::iter which takes a function and calls it repeatedly. We provide a function that just produces random values. Then we use the take adapter to take the appropriate number of elements, and finally collect them into a vector. Finally, we use the Array::from_shape_vec constructor to turn the Vec into an Array.  The code for the above looks as follows:\nlet weight_dim = (input.ncols(), num_neurons); let weight_count = weight_dim.0 * weight_dim.1; let weights = repeat_with(|| r.gen::\u0026lt;T\u0026gt;()).take(weight_count).collect(); let weights = Array::from_shape_vec(weight_dim, weights).unwrap(); Creating the bias matrix is the same, except the dimensions are different. The bias matrix always consists of a single row, and the number of columns is the number of neurons we have. This looks as follows:\nlet bias_dim = (1, num_neurons); let bias_count = bias_dim.0 * bias_dim.1; let bias = repeat_with(|| r.gen::\u0026lt;T\u0026gt;()).take(bias_count).collect(); let bias = Array::from_shape_vec(bias_dim, bias).unwrap(); Finally we have the parameters we need to create the layer. We make sure to include the activation function that we stored as a field as the last operation in the sequence:\nvec![ Operation::new_parameter(\u0026amp;WeightMultiply, weights), Operation::new_parameter(\u0026amp;BiasAdd, bias), Operation::new_basic(self.0), ] Layer Finally we can implement the abstraction for a Layer. As with Operation, I\u0026rsquo;ll split this up into sub-headers to break it up a little.\nDefinition The structure definition looks as follows:\npub struct Layer\u0026lt;\u0026#39;a, T\u0026gt; { num_neurons: usize, operations: Option\u0026lt;Vec\u0026lt;Operation\u0026lt;\u0026#39;a, T\u0026gt;\u0026gt;\u0026gt;, setup: \u0026amp;\u0026#39;a dyn LayerSetup\u0026lt;\u0026#39;a, T\u0026gt;, output: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, seed: Option\u0026lt;u64\u0026gt;, } As always, we need to take the type of elements in the arrays (T) and lifetime of the OperationData vtables (\u0026lsquo;a).Following is a description of each parameter within the definition however:\n num_neurons - This is the number of columns/features in the output of the layer. It\u0026rsquo;s used to generate weights and bias of a correct size to produce the desired dimension operations - This is an owned vector of the Operations in the layer. It\u0026rsquo;s an Option here because we can default to None until the first forward pass is performed. We could use an empty Vector but that would not be as explicit. This way if someone tries to use the list of operations without doing a forward pass first then they\u0026rsquo;ll see it\u0026rsquo;s invalid by the program panicking setup - A reference to the LayerSetup to use to produce the list of Operations on the first forward pass output - Store the calculated output from the forward pass which can be used then on a backward pass seed - The optional random seed to use for generating weights/bias terms in the layer  Factory Functions Secondly we have 2 factory functions to create a Layer. One of which takes the random seed to use, and the other which doesn\u0026rsquo;t (so the RNG will be different every time). The basic version will take the number of neurons we want in the layer, along with a required LayerSetup implementation.\nEverything else is set to None by default:\npub fn new(num_neurons: usize, setup: \u0026amp;\u0026#39;a dyn LayerSetup\u0026lt;\u0026#39;a, T\u0026gt;) -\u0026gt; Self { Self { num_neurons, operations: None, setup, output: None, seed: None, } } The fixed seed version is more or less the same, except it sets the seed to the provided value:\npub fn new_with_seed(num_neurons: usize, setup: \u0026amp;\u0026#39;a dyn LayerSetup\u0026lt;\u0026#39;a, T\u0026gt;, seed: u64) -\u0026gt; Self { Self { num_neurons, operations: None, setup, output: None, seed: Some(seed), } } Forward Pass Both the forward and backward functions are inside an impl block that looks as follows:\nimpl\u0026lt;\u0026#39;a, T: Clone\u0026gt; Layer\u0026lt;\u0026#39;a, T\u0026gt; { ... } That is, we place the bound of Clone on the type T. The reason for this is simply that the forward method takes \u0026ldquo;input\u0026rdquo; by reference, however we will need it by value for folding as we\u0026rsquo;ll see shortly, so we need to clone it.\nThe signature of the forward method itself is identical to the forward method on Operation. This is not a coincidence as Layer is wrapping multiple Operations while keeping the API the same. This is the Composite design pattern where one larger thing has the same API as the smaller unit, but which encapsulates multiple of those smaller units.\npub fn forward(\u0026amp;mut self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { ... } The first step on the forward pass is to make sure that we generate (using the LayerSetup) the list of Operations on the first forward pass. We can actually do this nicely in Rust by using a method on Optioncalled get_or_insert_with. This function takes a callback, and calls it if the Option is currently None. The return value from the function is then set as the value of the Option for further runs. Finally a reference to the got or inserted entry is returned.\nlet operations = self .operations .get_or_insert_with(|| self.setup.layer_setup(self.num_neurons, input, \u0026amp;self.seed)); In order to perform the forward pass, we basically just want to start off with the input we were provided, and then pass it through the chain of operations. We can easily do this with the fold method on iterators. This method takes an initial state (starts as the provided input), and a function that is called on every item in the iterator that can update and return new state. Once we\u0026rsquo;ve gone through all elements, the final value is returned, which we\u0026rsquo;ll stash in self.output:\nself.output = Some( operations .into_iter() .fold((*input).clone(), |state, operation| { operation.forward(\u0026amp;state) }), ); Finally we can return a clone of the output (we can\u0026rsquo;t return the output itself as we need a copy for the backward pass):\n(self.output.as_ref().unwrap()).clone() Backward Pass For the backward pass, just like with Operation, we will first need to validate the shape of the provided output gradient is correct before passing it backwards through the operations list.\nThe signature also is very familiar as it mimics the same for Operation:\npub fn backward(\u0026amp;mut self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { ... } We firstly verify that the shape of the provided output gradient matches that of the output of the layer, as calculated from the forward pass:\nassert_eq!( self.output.as_ref().unwrap().raw_dim(), output_gradient.raw_dim() ); Passing the output gradient through the series of Operations to get an input gradient to return is the same as the forward pass almost. We do need to remember to run backwards through the vector of operations. We can get a reversed iterator with the rev() method:\nself.operations .as_mut() .unwrap() .into_iter() .rev() .fold((*output_gradient).clone(), |state, operation| { operation.backward(\u0026amp;state) }) Parameters And Gradients Finally we need a way to process all the parameters and gradients from ParameterOperations on the layer. This is why we added the parameter_and_gradient method to Operation, so that we can access those references from the layer.\nThis function is a bit tricky to write however, because the reference to the parameter is mutable.\nRust doesn\u0026rsquo;t allow multiple mutable references to the same item (or even a mutable and immutable reference active at the same time) and so we can\u0026rsquo;t just return references as returned from parameter_and_gradient. This is because those are tied to the lifetime of the Operation itself, rather than the lifetime of the iterator.\nTo convince Rust that the returned references have the same lifetime as the iterator, we can specify explicitly that the references have the same lifetime as the \u0026amp;mut self parameter to the method. The iterator owns the \u0026amp;mut self borrow until it goes out of scope, so we can tie the lifetime of the references returned as items to the lifetime as the borrow of Self fairly easy:\npub fn parameters_and_gradients\u0026lt;\u0026#39;b\u0026gt;(\u0026amp;\u0026#39;b mut self) -\u0026gt; impl Iterator\u0026lt;Item = (\u0026amp;\u0026#39;b mut Array\u0026lt;T, Ix2\u0026gt;, \u0026amp;\u0026#39;b Array\u0026lt;T, Ix2\u0026gt;)\u0026gt; { ... } Here we are introducing the lifetime parameter \u0026lsquo;b, and saying that this borrow \u0026amp;mut self lasts for that length of time. Then we\u0026rsquo;re saying that the items in the returned iterator also live for this time too.\nThe next step is that we require a little unsafe\u0026hellip;.\nThe problem we have now is that the vector of Operation instances we have is of the following type:\nVec\u0026lt;Operation\u0026lt;\u0026lsquo;a, T\u0026raquo;\nNow \u0026lsquo;a is the lifetime of the Operation itself (or more specifically the lifetime of the \u0026amp; dyn BasicOperation/\u0026amp; dyn ParameterOperation references inside). So if we were to iterate over that as it is now, then it would produce references with lifetime \u0026lsquo;a.\nThis means the produced references from the iterator would live for \u0026lsquo;a, but the iterator itself only for \u0026lsquo;b. Thus, there\u0026rsquo;s a conflict and the compiler will complain.\nIn order to solve this, we can use (unsafely) std::mem::transmute to very carefully change Operation\u0026lt;\u0026lsquo;a, T\u0026gt; into Operation\u0026lt;\u0026lsquo;b, T\u0026gt;.\nNow this is perfectly safe in fact because we know that the references actually live for \u0026lsquo;a, which is guaranteed to live at least as long as \u0026lsquo;b. Therefore there\u0026rsquo;s no harm in shortening the lifetime to \u0026lsquo;b on those Operation instances.\nThe code for that is as follows (note that I\u0026rsquo;m coercing the Vec into a slice and transmuting that, but either works):\nlet operations = unsafe { transmute::\u0026lt;\u0026amp;\u0026#39;b mut [Operation\u0026lt;\u0026#39;a, T\u0026gt;], \u0026amp;\u0026#39;b mut [Operation\u0026lt;\u0026#39;b, T\u0026gt;]\u0026gt;( self.operations.as_mut().unwrap(), ) }; Once we\u0026rsquo;ve done this transmutation to shorten the invariant lifetime on Operation, the rest of the function is simple enough and is just a case of doing a filter_map and calling parameter_and_gradient on each operation.\nparameter_and_gradient returns None if it\u0026rsquo;s a BasicOperation, or Some((param, grad)) if it\u0026rsquo;s a ParameterOperation so we can return this from filter_map directly.\nfilter_map will only pass forward any Some values and will unwrap them as it runs through them:\noperations .into_iter() .filter_map(|op| op.parameter_and_gradient()) ","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/layers/","summary":"Overview The lowest level of abstraction as mentioned in the previous post is the operation. These were the lowest level building block that is able to perform a single unit of calculation on the forward pass, and single unit of partial derivative calculation on the backward pass.\nDuring that chapter, we could see how multiple of these operations are able to be chained together, though we don\u0026rsquo;t want to do this by hand.","title":"Layers"},{"content":"This chapter is about creating the abstractions in code that enable us to think of neural networks in a modular fashion, and build up complex deep networks with simple structures.\nI will be implementing the abstractions in an accompanying library called Eidetic which is free for use and distribution, since I\u0026rsquo;m only implementing this as a learning opportunity.\nOverview The Operation construct will represent and encapsulate a single operation in a neural network, such as the sigmoid function, matrix multiplication, etc.\nAn operation should be able to take inputs in and produce an output, as well as take a computed partial derivative from the output side and add its own derivative into the mix (i.e. the chain rule).\nAn Operation structure will have forward and backward methods which achieve this, and we will deal exclusively with 2-dimensional ndarray::Array instances as this is the library of choice to do the lower level matrix representation and operations we\u0026rsquo;ve picked to use in Rust.\nThere are two types of Operations - ones which take only a single array and produces an output array, and ones that take two arrays.\nAn example of the arity-1 operations would be the sigmoid function, and an example of the arity-2 operations would be the matrix multiplication, or bias addition functions.\nAdditionally there are some operations which will change the shape of the input array as output, and some that will retain the shape.\nAn example of shape preserving operation is the sigmoid function. And an example of a shape changing operation is matrix multiplication.\nFor computing the gradients, an operation will have an output gradient that represents the total gradient calculated by the network to the right/output side of an operation, and will have an input gradient which is what is propagated to the left to other operations on the input side.\nThe general rule is an Operation must make sure that the shape of the input gradient matches the input, and shape of the output gradient matches the output to allow us to correctly reason about these derivatives.\nDiagrams An example of an Operation which takes only one input and produces an output is shown in the diagram below:\nAnd an example of an Operation which takes two inputs and produces an output is shown in the following diagram:\nImplementation Approach In order to ensure that the correct checks are done during use, with respect to the shapes of the input/input gradient, and output/output gradient arrays (additionally parameter/parameter gradient arrays if this is an operation that takes a secondary parameter), we will make the Operation type a concrete type, letting us fix forward/backward functions and perform the appropriate invariant checks.\nSince there are two types of operation, and one taking a secondary parameter needs to do additional work/checks on the parameter and parameter gradient, we will make this type an enumeration type.\nWithin these two broad categories of Operation, we can implement anything that\u0026rsquo;s able to take an input and produce an output, as well as take an output gradient and produce a gradient of its input.\nIn order to achieve this in Rust, we can use a trait object that we pass into the Operation. This will act as a sort of vtable.\nThis approach lets us hard-code the parts that are important, while allowing customisation on the parts that should change. The following sections will detail each type and explain why the code is that way.\nBasicOperation This is a trait for the \u0026ldquo;vtable\u0026rdquo; I mentioned above. It represents the inner calculation for an operation that does not take a secondary parameter.\nThe signature is as follows and is generic over some type T\npub trait BasicOperation\u0026lt;T\u0026gt; The generic parameter here as we\u0026rsquo;ll see shortly is the underlying data type that\u0026rsquo;s stored inside of the ndarray::Array instances that we\u0026rsquo;ll be using.\nThe first function defined in the trait is the one that will be used to calculate an output array from a given input array on the forward pass through this operation.\nfn calculate_output(\u0026amp;self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; A few things to note are:\n We don\u0026rsquo;t need mutable access to self. It\u0026rsquo;s intended that these vtables for the inner calculations of operations are stateless, thus it should be sufficient to use the provided input parameter to calculate an output. We don\u0026rsquo;t put a bound on the type T at this point as it\u0026rsquo;s good practice in Rust to only put bounds when absolutely necessary so as to not cause bound propagation whenever the type is used. We know that we\u0026rsquo;re working with 2-dimensional data (the rows representing the observations in the data, and the columns representing the features), so we hardcode the dimensionality of these arrays to ndarray::Ix2. We don\u0026rsquo;t need to have ownership of the input so to avoid unnecessary cloning, we take by reference. This means that the previous Operation can stash the input, and provide a reference to it, rather than needing to clone and have it discarded anyway.  The second function that is defined by this trait is the function that will be called on the backward pass to calculate the gradient of its inputs given the output gradient coming in from the rest of the network.\nfn calculate_input_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; The points above are valid here too, with a couple of additional things to note:\n The implementor MUST combine their partial derivative with the output gradient to produce the final gradient. We can\u0026rsquo;t do this externally in the Operation type where we do shape validation, because the way in which they\u0026rsquo;re combined varies based on the operation (could be elementwise multiplication or matrix multiplication, etc.). We take the input again here, because calculating the input gradient necessarily needs to know what the input was. Since these implementations are stateless (so they can be reused for multiple operations if needed), we get given the previous input.  ParameterOperation The second broad category of operation are those that take a secondary parameter array as well as the input.\nAs with the basic operation we need the ability to calculate the output on the forward pass and the input gradient on the backward pass. However, unlike the BasicOperation, we need to also be able to calculate the gradient of the parameter, as well as route the parameter through the other calculation functions.\nThe signature looks the same as BasicOperation, again here we\u0026rsquo;re taking a T to define the underlying data type of the elements in the ndarray::Array instances:\npub trait ParameterOperation\u0026lt;T\u0026gt; The next element required by implementations is again, a calculate_output function. This signature is almost identical to that defined by BasicOperation, however since we know there\u0026rsquo;s a secondary parameter, this signature takes that into account:\nfn calculate_output(\u0026amp;self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; Likewise, the calculate_input_gradient is almost identical to the BasicOperation version, except this too takes an additional parameter (since the gradient of the input can be defined in terms of the secondary parameter):\nfn calculate_input_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; This trait however defines a third item. Since this is an operation that takes a secondary parameter, we need to be able to calculate the gradient for it to be able to update (for example the weights). This third item is identical to calculate_input_gradient except that it instead calculates the gradient of the parameter:\nfn calculate_parameter_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt;; WeightMultiply Now that we\u0026rsquo;ve defined the traits for the two broad categories of operation, we can define the concrete implementations of these operations.\nThe first up is the WeightMultiply operation which we use to multiply our matrix of observations together with the current vector of weights (not including the bias term).\nAs mentioned before, these \u0026ldquo;vtables\u0026rdquo; will be stateless, and as such the definition of the WeightMultiply structure is suitably underwhelming:\npub struct WeightMultiply; That is just a unit struct that doesn\u0026rsquo;t have any fields, but allows us to implement the calculations on it.\nThe definition of the implementation block for WeightMultiply is as expected, just implementing ParameterOperation on WeightMultiply. We need to implement ParameterOperation rather than BasicOperation because the weights are taken in as the secondary parameter. The impl block then is:\nimpl\u0026lt;T: LinalgScalar\u0026gt; ParameterOperation\u0026lt;T\u0026gt; for WeightMultiply { ... } The only thing to note is that we are doing a blanket implementation for all types T, however, this would not allow us to perform any arithmetic operations on the arrays with this element type. We require the ability to calculate the dot product and for the dot product to be available on ndarray::Array then we need to place a bound on the generic type T such that we only accept T\u0026rsquo;s which conform to the LinalgScalar trait.\nLooking at the definition for this trait we can see that it\u0026rsquo;s just requiring a bunch of mathematical traits so we can multiply and add values (required for dot product):\npub trait LinalgScalar: \u0026#39;static + Copy + Zero + One + Add\u0026lt;Output = Self\u0026gt; + Sub\u0026lt;Output = Self\u0026gt; + Mul\u0026lt;Output = Self\u0026gt; + Div\u0026lt;Output = Self\u0026gt; { } Getting into the actual implementations of the ParameterOperation trait now, and starting with the easiest one\u0026hellip;.calculating the output on the forward pass. This is just a matrix multiplication between the input matrix of data, and the secondary parameter, our weights:\nfn calculate_output(\u0026amp;self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { input.dot(parameter) } Now we can calculate the gradients on the backward pass. Thinking back to the previous posts, we know that when a dot product is involved, the partial derivative is just the transpose of the other array to the one we care about. We can see this in the definition of the calculate_input_gradient function:\nfn calculate_input_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, _input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { output_gradient.dot(\u0026amp;(*parameter).clone().reversed_axes()) } Here we are doing the following sequence of events when calculating the input gradient:\n We dereference the input parameter array reference to get a value (we can\u0026rsquo;t move out of this value though as we got it from dereferencing a borrow that we don\u0026rsquo;t own) We clone the parameter matrix, which gives us a brand new copy that we do own and have permission to consume We call reversed_axes to get the transpose matrix. This function requires self by value and consumes it, re-using the memory. This is why we had to clone the parameter array, because we can\u0026rsquo;t consume out of a borrow We then take a borrow to the newly transposed matrix. This is required because we\u0026rsquo;re calling the dot product function on a reference to an array, and so requires the right hand side operand to also be a reference  The partial gradient, and contribution from this operation is the transposed parameter matrix. However the dot product is required since we are returning the total gradient thus far, incorporating our partial gradient into what we were told from the rest of the network.\nThe calculation of the parameter gradient is similar, however notice that we have the output_gradient on the right hand side this time, not the left:\nfn calculate_parameter_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, _parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { (*input).clone().reversed_axes().dot(output_gradient) } BiasAdd The second operation we need to implement is the ability to add a bias to the newly minted weighted product from the previous operation.\nThis too, is a ParameterOperation as we need to set the bias term as the additional parameter.\nThe struct definition is a unit struct just like WeightMultiply as we just use it as a trait object to get the behaviour from:\npub struct BiasAdd; As before too, for implementation we can only implement the trait for elements that conform to the LinalgScalar trait. In this case, it\u0026rsquo;s because we need the elementwise multiplication, and addition:\nimpl\u0026lt;T: LinalgScalar\u0026gt; ParameterOperation\u0026lt;T\u0026gt; for BiasAdd { ... } The calculate_output method implementation here is very simple, it\u0026rsquo;s just adding the parameter to the input. However, we must check that the bias term consists of only 1 row (with n elements, where n is the number of features).\nThe bias term is smaller in size than the input, but ndarray Arrays will broadcast the addition across the whole matrix.\nfn calculate_output(\u0026amp;self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { assert_eq!(parameter.nrows(), 1); input + parameter } For calculating the input gradient, we know from the reasoning in chapters 1 and 2 that the partial derivative for addition will just be a matrix with the same shape as the input but full of all 1\u0026rsquo;s. As before we need to multiply this by the output gradient to take into account the chain rule.\nIn ndarray, we can get an array of 1\u0026rsquo;s of a certain shape by using the associated function \u0026ldquo;ones\u0026rdquo;. This will take a tuple defining the shape we want. We can use the method \u0026ldquo;raw_dim\u0026rdquo; to get the shape of another array (in this case our input matrix):\nfn calculate_input_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, _parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { Array::ones(input.raw_dim()) * output_gradient } Finally, the calculation of the parameter gradient. This will start off being calculated the same way as we calculated the input gradient above, however we need to sum the resulting matrix along the axis of the rows.\nThis, as mentioned a few times previously, is to account for the bias term being added multiple times to the input data.\nWe will get a column vector from this, but to conform to the shape expected from the input gradient we convert to a row vector.\nIn order to get a view along a specific axis of an array, we can use the map_axis method which takes the axis we\u0026rsquo;re traversing along, and a callback function which is called for each subview. In our case, for each subview, we want to get the sum.\nThe method into_shape lets us reshape the same data, in our case we ensure we are producing a row vector. This can fail if the provided shape isn\u0026rsquo;t compatible with the provided data, but we just unwrap it here as I don\u0026rsquo;t care too much about error handling at the moment.\nThe full code for this method then is:\nfn calculate_parameter_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, _input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, parameter: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { let parameter_gradient = Array::ones(parameter.raw_dim()) * output_gradient; parameter_gradient .map_axis(Axis(0), |view| view.sum()) .into_shape((1, parameter_gradient.ncols())) .unwrap() } Sigmoid The sigmoid activation function is our only BasicOperation so far. It doesn\u0026rsquo;t require a secondary parameter as it just modifies each element of the array by applying the sigmoid function.\nThis function as explained previously is defined as:\n$$ f(x) = \\frac 1 {1 + e^{-x}} $$\nAnd the derivative is defined in terms of itself as:\n$$ \\frac {\\partial f} {\\partial x}(x) = f(x) \\times (1 - f(x)) $$\nFirst we have the struct definition which, you guessed it!, is a unit struct as we don\u0026rsquo;t have a need for state:\npub struct Sigmoid; The trait bounds for Sigmoid here are a little different however than previously. The previous trait bound of LinalgScalar doesn\u0026rsquo;t work for us here as that only provides basic arithmetic operations. However what we need is:\n The ability to calculate the exponential function with the type T The ability to produce a quantity of \u0026ldquo;one T\u0026rdquo; The basic mathematical operators of +, -, *, / The ability to use unary negation  Looking in the num_traits crate we can see that two traits provide the \u0026ldquo;exp\u0026rdquo; method. These are Real and Float\u0026hellip;.however Real is just a subtrait of Float with additional stuff. Since we\u0026rsquo;re trying to be least restrictive we pick Float.\nThis trait requires a bunch of others that ultimately end up satisfying all the criteria we want.\nimpl\u0026lt;T: Float\u0026gt; BasicOperation\u0026lt;T\u0026gt; for Sigmoid { ... } To calculate the input, we simply apply the sigmoid function to each element in the input array to produce the output array. We can use mapv to call a function on each element of an Array to produce a new array of the same shape:\nfn calculate_output(\u0026amp;self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { input.mapv(|elem| T::one() / (T::one() + (-elem).exp())) } The only other function we need to implement (with this being a BasicOperation) is the calculate_input_gradient function.\nSince the partial derivative is defined in terms of itself, we first recalculate the output from the input. We can then use mapv again on this to apply the partial derivative formula elementwise before finally multiplying it together with the current output gradient as per the chain rule:\nfn calculate_input_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { let output = self.calculate_output(input); // recalculate the output at the input.  let partial_derivative = output.mapv(|elem| elem * (T::one() - elem)); partial_derivative * output_gradient } Linear We implement a passthrough operation for those situations where we need to provide an operation but don\u0026rsquo;t actually require it to do anything. Since the code is so simple, and the functions have been defined above, I\u0026rsquo;ll just list the code in its entirety here:\npub struct Linear; impl\u0026lt;T: Clone\u0026gt; BasicOperation\u0026lt;T\u0026gt; for Linear { fn calculate_output(\u0026amp;self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { (*input).clone() } fn calculate_input_gradient( \u0026amp;self, output_gradient: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, _input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { (*output_gradient).clone() } } Operation Finally the big bad Operation struct. This is the type that wraps around these vtables we\u0026rsquo;ve been making and provides the invariant/shape checking functionality, as well as providing a single type to let us store different operations together.\nNote that we could have made Operation a trait, and stored a list of trait objects. However doing this would allow a user to bypass the array shape checks, and caching, etc. which we definitely do not want.\nThe way we have it set up now is that the user only is able to customise the calculations but not able to opt-out of checks, and the general flow.\nSince this is the largest type, I\u0026rsquo;ll split this section up into a sub section per item.\nOperationData This is the smallest unit so we\u0026rsquo;ll start here. This wraps over either a BasicOperation or a ParameterOperation and provides a homogenous type to allow us to store in the Operation wrapper.\nThis is defined as an enumeration because we need to store different data depending on whether we\u0026rsquo;re wrapping a BasicOperation or ParameterOperation.\nIn the case of the basic operation, we need a BasicOperation trait object only to provide the calculations.\nIn the case of parameter operations, we need to additionally provide storage for the parameter array and the parameter gradient.\nThe definition of this enum is as follows:\nenum OperationData\u0026lt;\u0026#39;a, T\u0026gt; { Basic(\u0026amp;\u0026#39;a dyn BasicOperation\u0026lt;T\u0026gt;), Parameter { operation: \u0026amp;\u0026#39;a dyn ParameterOperation\u0026lt;T\u0026gt;, parameter: Array\u0026lt;T, Ix2\u0026gt;, parameter_gradient: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, }, } Notes on this are:\n We are storing a reference to the BasicOperation/ParameterOperation implementation. We could use a Box but this would involve putting it on the heap. As we are going to be constructing the network up front, there\u0026rsquo;s no need for it to live on the heap so we store by reference instead. The downside here of course, is that we now need to add a lifetime annotation for the borrow. The parameter field is an Array because as we\u0026rsquo;ll see in a minute, the \u0026ldquo;constructor\u0026rdquo; takes a parameter and so we\u0026rsquo;re guaranteed to have it from construction time. However the parameter gradient is only guaranteed to be calculated after the first backward pass. We store this as an Option so that we get a panic if we try to unwrap before a backward pass is performed.  Operation This is the actual type that users will be dealing with and store in a list of operations within the layer later on.\nIt\u0026rsquo;s a simple struct that stores the input and input gradient (both Option as they\u0026rsquo;re invalid until the first forward/backward pass is performed), and the calculated output also invalid by default.\nThe last thing it stores is the OperationData so it can access the vtable (and parameter, parameter_gradient fields if appropriate).\nAgain, we need to plumb the lifetime annotation through. This time though, we\u0026rsquo;re not storing a reference directly in Operation, but we do need to plumb it through to OperationData so it knows how long the reference it has lives for.\nFinished definition of the Operation struct is therefore:\npub struct Operation\u0026lt;\u0026#39;a, T\u0026gt; { input: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, // last provided input on a forward pass  input_gradient: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, // last calculated input_gradient on the backward pass  output: Option\u0026lt;Array\u0026lt;T, Ix2\u0026gt;\u0026gt;, // last calculated output on the forward pass  data: OperationData\u0026lt;\u0026#39;a, T\u0026gt;, // stores additional data that isn\u0026#39;t common between the two kinds of operation } Constructors We have two \u0026ldquo;constructors\u0026rdquo; for the two categories of operation (basic and parameterised). These are super simple and as expected just create the instance.\nIn the case of a basic operation, the only thing that\u0026rsquo;s required is a BasicOperation \u0026ldquo;vtable\u0026rdquo; trait object that we can use for calculations. Again we need to ensure that the lifetimes match up correctly (lifetime \u0026lsquo;a is defined in the impl block):\npub fn new_basic(operation: \u0026amp;\u0026#39;a dyn BasicOperation\u0026lt;T\u0026gt;) -\u0026gt; Self { Self { input: None, input_gradient: None, output: None, data: OperationData::Basic(operation), } } The second is for parameterised operations. This takes a ParameterOperation trait object along with an initial value for the parameter:\npub fn new_parameter( operation: \u0026amp;\u0026#39;a dyn ParameterOperation\u0026lt;T\u0026gt;, parameter: Array\u0026lt;T, Ix2\u0026gt;, ) -\u0026gt; Self { Self { input: None, input_gradient: None, output: None, data: OperationData::Parameter { operation, parameter, parameter_gradient: None, }, } } Forward Pass Now we can implement the final two functions. These two (forward and backward) are the meat of the neural network and performs the necessary validation during the backward pass, as well as ensuring data is cached on the forward pass for use in gradient calculation.\nThe forward function will cache the input array. However, because we\u0026rsquo;re taking the input array by reference will cache a clone of it.\nThe signature and this input stashing line are:\npub fn forward(\u0026amp;mut self, input: \u0026amp;Array\u0026lt;T, Ix2\u0026gt;) -\u0026gt; Array\u0026lt;T, Ix2\u0026gt; { self.input = Some((*input).clone()); The next section of code is to calculate the output array, which we also stash. We do this by delegating to the OperationData trait object we\u0026rsquo;re wrapping over.\nDue to this being an enumeration, we need to do a match on the data and in the case of a parameter operation, remember to pass along the parameter with the input:\nself.output = Some(match \u0026amp;self.data { OperationData::Basic(operation) =\u0026gt; operation.calculate_output(input), OperationData::Parameter { operation, parameter, .. } =\u0026gt; operation.calculate_output(input, \u0026amp;parameter), }); Finally we need to return the output. But we can\u0026rsquo;t just return the stashed array as we need that for shape checking on the backward pass. We therefore need to clone this array to get a new one to return. Due to us holding the array in an Option, we need to do this with the as_ref method:\n(*self.output.as_ref().unwrap()).clone() Backward Pass The backward pass looks more complex than forward because of the asserts checking the shapes of the arrays that we have. However at its core, we\u0026rsquo;re still delegating the actual calculation to the internal OperationData.\nWe begin by checking that the shape of the output gradient we got passed in matches the shape of the last processed output that was generated on the forward pass:\nassert_eq!( self.output.as_ref().unwrap().raw_dim(), output_gradient.raw_dim() ); Again, because we\u0026rsquo;re storing the output in an Option, we need to use the as_ref().unwrap() sequence.\nThe next step is to get the input gradient. As with the output gradient we can just do a match on our vtable type and ask it to calculate for us. However the match arm for parameterised operations is larger because we also need to calculate the parameter gradient (and verify its shape) in the process of calculating the input gradient:\nself.input_gradient = Some(match \u0026amp;mut self.data { OperationData::Basic(operation) =\u0026gt; { operation.calculate_input_gradient(output_gradient, self.input.as_ref().unwrap()) } OperationData::Parameter { operation, parameter, parameter_gradient, } =\u0026gt; { *parameter_gradient = Some(operation.calculate_parameter_gradient( output_gradient, self.input.as_ref().unwrap(), \u0026amp;parameter, )); assert_eq!( parameter_gradient.as_ref().unwrap().raw_dim(), parameter.raw_dim() ); operation.calculate_input_gradient( output_gradient, self.input.as_ref().unwrap(), \u0026amp;parameter, ) } }); We then need to assert the shape of the input gradient we just calculated matches the shape of the input we processed in the forward pass:\nassert_eq!( self.input.as_ref().unwrap().raw_dim(), self.input_gradient.as_ref().unwrap().raw_dim() ); Finally we need to return the calculated input gradient, but since we\u0026rsquo;re keeping it for later querying by the optimizer, we need to return a clone:\n(*self.input_gradient.as_ref().unwrap()).clone() Usage The next step in defining our abstractions will be to bundle multiple operations into a single layer, but already see an example of the usage of these Operation instances we have.\nFor example, below is a chain of a multiply, bias add, and sigmoid. We first build the network as a vector of Operations and running through the network is just a case of running through each of the operations.\nWe can create the neural network in a separate function to which we pass the initial parameters. In this code we take the weights and bias terms and set up a very simple sequence of operations:\nfn create_network\u0026lt;\u0026#39;a\u0026gt;(weights: Array\u0026lt;f64, Ix2\u0026gt;, bias: Array\u0026lt;f64, Ix2\u0026gt;) -\u0026gt; Vec\u0026lt;Operation\u0026lt;\u0026#39;a, f64\u0026gt;\u0026gt; { vec![ Operation::new_parameter(\u0026amp;WeightMultiply, weights), Operation::new_parameter(\u0026amp;BiasAdd, bias), Operation::new_basic(\u0026amp;Sigmoid), ] } Note that the compilers lifetime inference rules is enough to find a value for \u0026lsquo;a that satisfies the lifetime we\u0026rsquo;re using them for and is the smallest such lifetime that covers all cases.\nRemember that a reference having the lifetime \u0026lsquo;a does not mean exactly that lifetime, but rather, that lifetime or longer.\nTherefore it\u0026rsquo;s rare to require multiple lifetimes and is only required when we need to track them separately through the borrow checker.\nIn our case, all of these operations will always be kept together in the neural network, so the compiler finding a single lifetime (the shortest one) for all the operations here is enough.\nWe can then initiate our weights, bias, and input:\nlet input = Array::from_shape_vec((2, 3), vec![0.5, 0.0, 3.0, 0.0, 0.25, 0.0]).unwrap(); let weights = Array::from_shape_vec((3, 1), vec![1.0, 1.0, 1.0]).unwrap(); let bias = Array::from_shape_vec((1, 1), vec![1.0]).unwrap(); Then pass the weights and bias to the neural network creation function:\nlet mut net = create_network(weights, bias); This needs to be mutable due to us wanting to call the forward and backward functions on the operations in this Vec.\nIn order to run through the chain of functions we can use a fold. The initial state of the fold is just the input array. The fold operation will return the final result which is the output of the forward pass:\nlet output = net.iter_mut().fold(input, |state, op| op.forward(\u0026amp;state)); For the backward pass we need an initial array for the output gradient. In this example we\u0026rsquo;ll just use an array of 1\u0026rsquo;s. We also know that the shape of the output gradient and output must match, so we can just make an output gradient of the same shape:\nlet output_gradient = Array::ones(output.raw_dim()); The backward pass is the same, we can use a fold again. This time starting with the output_gradient and running backwards through the chain (hence the .rev() call):\nlet input_gradient = net .iter_mut() .rev() .fold(output_gradient, |state, op| op.backward(\u0026amp;state)); ","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/operations/","summary":"This chapter is about creating the abstractions in code that enable us to think of neural networks in a modular fashion, and build up complex deep networks with simple structures.\nI will be implementing the abstractions in an accompanying library called Eidetic which is free for use and distribution, since I\u0026rsquo;m only implementing this as a learning opportunity.\nOverview The Operation construct will represent and encapsulate a single operation in a neural network, such as the sigmoid function, matrix multiplication, etc.","title":"Operations"},{"content":"The previous post showed the most basic type of machine learning and how to train it, but the concepts learnt there (forward output calculations, backward gradient calculations) will apply equally here too.\nWhat was the problem with linear regression? The main problem with linear regression that we have is that we are training to find a set of weights, which are essentially the coefficients of an N-dimensional line. Therefore we\u0026rsquo;re trying to fit a straight line to a set of points.\nThe problem here is then that there is only so much we can do to fit a straight line to a set of points, and the error shows this. Linear regression works perfectly if the data set points are located already along some straight line, but in reality they rarely are.\nHow can we fix this? First of all, we need to introduce more tweakable values for the training algorithm to operate on. The more tweakable values that the algorithm has access to, then the more it is able to warp the resulting \u0026ldquo;line\u0026rdquo;.\nIn a neural network, what we end up finding and fitting to the data is not an N-dimensional line but an N-dimensional curve. More tweakables means more warp points.\nHowever, no matter how many sets of weights we end up using for training, the functions we use in linear regression will always produce a straight line. So additionally, as part of the modification, we will need to introduce a nonlinear function - that is, one which doesn\u0026rsquo;t map input values to output values along a straight line but along somme curve instead.\nStep 1: More weights The first part of this is to introduce the ability to have more weights that can be tweaked, and should be fairly straightforward to achieve.\nIn a standard linear regression, the formula we had for the initial \u0026ldquo;apply weights to observations\u0026rdquo; step was:\n$$ M = X \\cdot W $$\nThis formula remains the same, however if we look at the dimensionality of X, W, and M in the original linear regression we can see that we have:\n$$ [num\\_observations, num\\_features] \\cdot [num\\_features, 1] = [num\\_observations, 1] $$\nThe idea is that we change the weights from a column vector (matrix with column count of 1) to a matrix with N columns. N here is an arbitrary number of \u0026ldquo;learned features\u0026rdquo;, which can be considered nodes in the hidden layer of a neural network.\nThese \u0026ldquo;learned features\u0026rdquo; are an abstract notion, they don\u0026rsquo;t directly map to the actual features of the data set, but rather are features of the patterns in the data.\nIn short these learned features can represent whatever the network requires it to, we just won\u0026rsquo;t really know what they mean.\nLooking at the dimensionality of the new calculation we see:\n$$ [num\\_observations, num\\_features] \\cdot [num\\_features, N] = [num\\_observations, N] $$\nStep 2: A nonlinear function Once we have the weighted sum matrix, we will go ahead and feed it through a nonlinear function, which will ensure first nonlinearity in the output data, but also ensures that the output data is in the range 0 to 1.\nFor such a function, we will use the sigmoid function which has a graph that looks like this:\nThe sigmoid function is a good nonlinear function to choose for a few reasons:\n This function preserves information about the input (specifically the sign of it). If we were to use a nonlinear function such as squaring the input, then both -3 and 3 for example would give the same answer. With the sigmoid function -3 and 3 will give different outputs. It is nonlinear. This seems like an obvious point, but it is really important that we choose a nonlinear function for this step. Outputs are always in the range of 0 to 1 which is a nice property to have to keep numbers small. The most important property is that it has a derivative which is calculated easily in terms of itself. This derivative is shown below  $$ \\frac {\\partial \\sigma} {\\partial x}(x) = \\sigma(x) \\times (1 - \\sigma(x)) $$\nStep 3: Final linear regression The final step to get the actual answer is to perform one final linear regression on the outputs of the sigmoid function.\nA diagram showing the entire process can be seen below:\nConceptual diagram With the above function graph, we are able to see the actual operations that occur, but doesn\u0026rsquo;t conceptually show what a neural network looks like. From the implementation point of view the learned features are just columns in the initial matrix multiplication.\nA more common diagram showing what a neural network looks like conceptually can be seen below:\nThis shows the matrix multiplication (the lines between combinations of the input features - the initial nodes), and the output of the matrix multiplication (the middle layer of nodes which represent the learned features). Finally it shows the combination of the intermediate learned features to give a single output loss value as before.\nLoss gradients The overall loss gradient is calculated the same as before, as the mean squared sum of error between the predicted output and the target output for each observation.\nThe difference now is that we have two sets of weights and biases. One of these is for the initial matrix multiplication with the input data set and has N columns for the N learned features.\nThe second set of weights is just a single column vector with the same length as N, that gets multiplied with the learned features to give the final output.\nThe main difference between calculating the gradients in a linear regression and in this neural network is that we must calculate the partial derivatives for both sets of weights and biases.\nAs before, the caveat with calculating the gradient of the biases requires us to sum along the row axis to represent the same bias number being added to each observation.\nThe diagram below shows the overall gradient calculation process - as before we calculate the partial derivatives of each function at the outputs, and using the chain rule, multiply them together to get the final gradient.\nThe following table shows what each partial derivative is. The logic here is already covered previously in chapter 1, with the sigmoid function being explained earlier in this post.\nIn the following table, we use the function ones(X) which is terminology from the Python API and simply takes a matrix X and produces a matrix where each element has the value 1, with the same shape as X (same number of rows and columns).\nNote that in the table the function ^ is actually written as A because I couldn\u0026rsquo;t manage to get the carat symbol to escape properly in LaTeX.\n   Derivative Calculation     $$\\frac {\\partial A} {\\partial P}(P, y)$$ $$-(y - P)$$   $$\\frac {\\partial \\alpha} {\\partial M_2}(M_2, B_2)$$ $$ones(M_2)$$   $$\\frac {\\partial \\alpha} {\\partial B_2}(M_2, B_2)$$ $$ones(B_2)$$   $$\\frac {\\partial v} {\\partial W_2}(O_1, W_2)$$ $${O_1}^T$$   $$\\frac {\\partial v} {\\partial O_1}(O_1, W_2)$$ $${W_2}^T$$   $$\\frac {\\partial \\sigma} {\\partial N_1}(N_1)$$ $$\\sigma(N_1) \\times (1 - \\sigma(N_1))$$   $$\\frac {\\partial \\alpha} {\\partial M_1}(M_1, B_1)$$ $$ones(M_1)$$   $$\\frac {\\partial \\alpha} {\\partial B_1}(M_1, B_1)$$ $$ones(B_1)$$   $$\\frac {\\partial v} {\\partial W_1}(X, W_1)$$ $$X^T$$    Code The code is located HERE and is a modification of the previous post\u0026rsquo;s code to extend in the ways described here.\nThe data set is the same so the errors that we get after training can be compared to the errors we got with a simple linear regression.\nIf the neural network is able to fit the data better then the final total error should be lower.\nConclusion Surprisingly after implementation, it looks like for this data set the neural network is actually worse. As I get the following output:\nFinal error (training): 0.20948163078668652\rFinal error (testing): 0.2119911310559618\rThis shows that in fact the training error turns out worse than the linear regression method (though very slightly).\nWhen tested on the testing data, it looks like there\u0026rsquo;s a noticable gap in the error between training and testing.\nI\u0026rsquo;m not sure what\u0026rsquo;s causing it however I can speculate on two things:\n We get around 0.2 error in all cases both for the neural network and the linear regression methods. The neural network has possibly overfit the training data and so doesn\u0026rsquo;t perform so well on the testing data.  This kind of variance is to be expected on certain data sets and neural networks will really shine with more data.\nI have no reason to believe the implementation is incorrect at this point so won\u0026rsquo;t dwell on this too much and instead will continue on with the abstractions portion of the book.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/neuralnetworks/","summary":"The previous post showed the most basic type of machine learning and how to train it, but the concepts learnt there (forward output calculations, backward gradient calculations) will apply equally here too.\nWhat was the problem with linear regression? The main problem with linear regression that we have is that we are training to find a set of weights, which are essentially the coefficients of an N-dimensional line. Therefore we\u0026rsquo;re trying to fit a straight line to a set of points.","title":"Neural Networks"},{"content":"Introduction As mentioned previously, we can represent a training set as a matrix where each row of the matrix is a record/sample, and each column is a single feature of the record.\nAdditionally we can have a weighting assigned to each feature so some features are \u0026ldquo;worth\u0026rdquo; more than others.\nHowever we also require a baseline value for when every numeric feature is a 0 (we would always give 0, but we may not want this).\nThe formula for calculating the target/output value for a given record is then given as the following which is known as a linear regression.\n$$ y_i = {\\beta}_0 + {\\beta}_1 \\times x_1 + \u0026hellip; + {\\beta}_n \\times x_k + \\epsilon $$\nIn this, we have a vector of weights for features and a vector of the actual values for a record that we can add together to get the final target value for the record.\nHowever we also include a base term (beta 0) at the beginning which is the value we use when all features of the record are 0. This lets us shift the line of the regression to better fit the samples.\nCalculating predictions without the intercept The first step is to think of how we can generate our \u0026ldquo;predictions\u0026rdquo; for a given set of feature weights, and a given batch of samples. At first it helps to look at this calculation without the additional \u0026ldquo;intercept\u0026rdquo; (the base term we add to the weighted sum).\nWithout the intercept, the calculation is simply a dot product, or matrix multiplication between the vector of feature weights, and the matrix batch of samples. This will produce a vector of predictions.\nEach element of the prediction vector will be a dot product between the feature weight vector and the corresponding row of the batch.\nThis is represented as:\n$$ P_{batch} = X_{batch} \\times W = \\begin{bmatrix} X_{11} \u0026amp; X_{12} \u0026amp; X_{13} \u0026amp; \u0026hellip; \u0026amp; X_{1k} \\\\ X_{21} \u0026amp; X_{22} \u0026amp; X_{23} \u0026amp; \u0026hellip; \u0026amp; X_{2k} \\\\ X_{31} \u0026amp; X_{32} \u0026amp; X_{33} \u0026amp; \u0026hellip; \u0026amp; X_{3k} \\end{bmatrix} \\times \\begin{bmatrix} W_1 \\\\ W_2 \\\\ W_3 \\\\ . \\\\ . \\\\ . \\\\ W_k \\end{bmatrix} = \\begin{bmatrix} P_1 \\\\ P_2 \\\\ P_3 \\end{bmatrix} $$\nCalculating error Once we have a way to calculate our predictions, we do need a way to know how \u0026ldquo;good\u0026rdquo; those predictions are. This is just a measure of how close they are to the target/measured numbers that we already have.\nA good measure of this is the \u0026ldquo;Mean Squared Error\u0026rdquo; which is simply that we take the sum of the squared differences between the predictions and our target numbers, and divide by the number of entries to get the average difference.\nA \u0026ldquo;perfect\u0026rdquo; set of weights would give us a difference of precisely 0, so the closer we get to 0 the better.\nThis \u0026ldquo;Mean Squared Error\u0026rdquo; function takes a vector of predictions and a vector of targets and produces a single number as output and is given by the formula:\n$$ MSE(P_{batch}, Y_{batch}) = MSE(\\begin{bmatrix}P_1 \\\\ P_2 \\\\ P_3\\end{bmatrix}, \\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ Y_3\\end{bmatrix}) = \\frac {(Y_1 - P_1)^2 + (Y_2 - P_2)^2 + (Y_3 - P_3)^2} 3 $$\nA diagram We can visualise the above with a little diagram. In this diagram we take the batch matrix which we call X, and the weights which we call W.\nWe apply the matrix multiplication in an operation we call V to get the prediction vector P.\nAfter we have the prediction vector P, we take in our target/result vector Y and perform the \u0026ldquo;Mean Squared Error\u0026rdquo; operation.\nWe represent this function with the ^ character, and the output is a single error value known as L.\nAdding the intercept In order to calculate the error with the intercept (bias) included. We simply perform a scalar addition of the bias value to each element of the prediction vector just before we calculate the error.\nThis operation is represented with the alpha symbol in the below diagram, and the bias term represented with the character \u0026lsquo;B\u0026rsquo; (a scalar not a vector here).\nCalculating the partial derivatives for W and B Now that we have a way to go from the input parameters to a single value indicating our total deviation from the actual values, we need to somehow use that value to update our weights and bias terms such as to minimize L and bring it towards 0.\nWe can do this using everything from chapter 1 that we learned about partial derivatives.\nWe can figure out how much a change in each of the elements of the weighting vector, or the intercept term will cause a change in L and use that to modify those terms appropriately.\nWe will work backwards when calculating the partial derivatives here (using MSE instead of ^ to indicate the mean squared error function due to LaTeX issues).\nPartial derivative of L (with respect to P) For each element in Y and P (they\u0026rsquo;re the same size) then the partial derivative of the associated input element with respect to that pair is defined as the squared difference.\nSumming the values and dividing by the number of elements is an operation that occurs afterwards and isn\u0026rsquo;t affected by the changing of any of the input values by any amount, so the partial derivative can eliminate the sum and the division.\nSince:\n$$ MSE(P, Y) = (Y - P)^2 $$\nWe can extend this by FOIL to give us:\n$$ Y_2 - (2 \\times Y \\times P) + P_2 $$\nWith this, we can see that for each element in P, if we are to increase it by a value of 1, then it has no effect on the squared Y term. For the squared P term we can use the power rule of differentiation and see that it will become (2*P).\nFor the central term (-2 * Y * P), raising P by a value of 1 will raise the total by -2 * Y.\nTherefore after this, we get:\n$$ \\frac {\\partial L} {\\partial P}(P, Y) = (-2 \\times Y) + (2 \\times P) = (-2 \\times Y) - (-2 \\times P) = -2 \\times (Y - P) $$\nDerivative of intercept addition The intercept addition operation is simply defined as\n$$ \\alpha = N + B $$\nThen clearly increasing the value of each element of N by one unit will increase the value of each output by 1 unit and so the derivative is just a vector of 1\u0026rsquo;s with the same shape as N.\nSince it\u0026rsquo;s simply an addition, then this is also the partial derivative with respect to B. Increasing the bias term by 1 unit also increases the elements of N by the same difference.\nTherefore both partial derivatives for intercept addition are just a vector of 1\u0026rsquo;s of the same shape.\nDerivative of matrix multiplication (with respect to W) As described in chapter 1, when we have a matrix multiplication and want to calculate the derivative with respect to one of the operands, that the result is just the transpose of the other operand.\nTherefore:\n$$ \\frac {\\partial v} {\\partial W}(X, W) = X^T$$\nFinal products Now that we can calculate the partial derivatives, we can calculate the total derivative using the chain rule with respect to both W and B.\nOne thing to note, however is that B is a single float value, whereas the partial derivative of L with respect to P is a vector (since each element of P contributes to a change in L). We need to take into account that the bias, B is applied to each element of N to produce P. To account for the bias being applied to each element we sum to get the final derivative of L with respect to B\nThe total derivatives can be calculated therefore by the following formulas:\n$$ \\frac {\\delta L} {\\delta W} = {\\frac {\\partial N} {\\partial W}} \\times {{\\frac {\\partial P} {\\partial N}} \\times {\\frac {\\partial L} {\\partial P}}} $$ $$ \\frac {\\delta L} {\\delta B} = sum({\\frac {\\partial L} {\\partial P}} \\times {\\frac {\\partial P} {\\partial B}}) $$\nUsing the derivatives to train Now that we\u0026rsquo;ve passed the data through the formula to get the mean squared error (L), and then done a backwards pass to get the derivatives of L with respect to W and B, we need to update W and B themselves in order to bring L towards 0.\nThis is a simple process and just involves us subtracting some proportion of the derivative to W and B. This proportion/multiplier is known as the learning rate and makes it so we don\u0026rsquo;t make too large a jump each time in the weights.\nFor training, we simply repeat the forward pass, backward derivative calculation, and weight updating over and over in a loop for a number of iterations known as epochs\nTraining set and testing set There is a problem when it comes to training linear regressions and neural networks of overfitting. This is the problem where a set of weights have been determined that work very well in producing the target output for a given entry in the sample, but which doesn\u0026rsquo;t then generalise and correctly predict for entries that haven\u0026rsquo;t been seen yet.\nIn order to solve this issue we break up the data set into two sets.\nThe training set is a data set that is used to train the weights - it\u0026rsquo;s the data that is passed through the formula, and for which we calculate derivatives to update the weights.\nThe testing set is a data set that is not used to update the weights, and is data that the network/regression hasn\u0026rsquo;t had passed through it at all yet. This lets us test whether the trained network will generalise well enough to entries that it hasn\u0026rsquo;t seen yet.\nCode Example The coded example of the above linear regression concepts can be found in the zip file HERE. As with most of the code I write nowadays, it is in Rust and will require installation of Cargo to run. The easiest way to install Rust and Cargo is through Rustup\nThe data set I am using for this linear regression can be found HERE and is a set of features about people applying for credit, along with whether they are deemed \u0026ldquo;good\u0026rdquo; or \u0026ldquo;bad\u0026rdquo; for the purposes of credit (the target feature).\nIt should be fairly easy to understand and is just a codification of the above method with a few points to note:\n Instead of training for a number of epochs, I\u0026rsquo;m training until the delta between the previous and new mean squared error values drops to within a given threshold (it\u0026rsquo;s settling around the final position). I am dividing the input data by the maximum value in the array to bring all values within 0 and 1 since the initial large values cause the squared error calculation to go off to infinity. Since the target values are 0 (bad) and 1 (good) we want all the values and weights to be close to this too. I split the input data into 3/4 training data and 1/4 testing data. Once training has determined the final weights and intercept, I apply to the testing data to see if there\u0026rsquo;s a similar error when applied to the data the system hadn\u0026rsquo;t seen yet.  After running this code, I got these values (on this particular run):\nFinal error (training): 0.20486995882901385\rFinal error (testing): 0.20561283878025513\rThis shows that the linear regression at least seems to work with this data set as there\u0026rsquo;s a very small difference in error between the training batch and the testing batch.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/linearregression/","summary":"Introduction As mentioned previously, we can represent a training set as a matrix where each row of the matrix is a record/sample, and each column is a single feature of the record.\nAdditionally we can have a weighting assigned to each feature so some features are \u0026ldquo;worth\u0026rdquo; more than others.\nHowever we also require a baseline value for when every numeric feature is a 0 (we would always give 0, but we may not want this).","title":"Linear Regression"},{"content":"Overview Supervised learning is a branch of machine learning that can be described as finding patterns or relationships in data that has already been measured. The \u0026ldquo;supervised\u0026rdquo; part of the phrase comes from the fact that we need to monitor the output that the system gives for a given input and to tell the system how close or far away it is from the measured result.\nAs a short example, suppose we looked at 100 houses and measured both their square footage and market value. We know both the input (square footage) and the output (market value), however we want to use machine learning to discover the relationship between square footage and market value in a way that can be applied to houses not in this training set.\nWe give the system one of the measured square footage values, it gives us an output that it calculates as the market value. Since we already measured the market value in the sample, we can tell the system how close it is (or far away) from the answer we expected and it can correct.\nThis will go around in cycles through all samples in a process called training, until the system reliably gives us an answer for each input that is within a certain tolerance of the correct answer. At this stage we can consider the system trained and can apply it to new inputs.\nRepresentation Once we have the set of features that we have measured (a feature is simply one measurement about a sample that we care about. These must be numerical and represent anything, such as for example, the square footage of a house, or it\u0026rsquo;s price).\nWe can represent our sample set as a matrix of numbers, with each column being a single feature of a sample, and each row in the matrix being a single record.\nSince we can represent the sample set as a 2D matrix of numbers, we can then apply the mathematics of calculating the derivitives of functions, and any other mathematical operations on the set in a homogenous way, regardless of what those numbers actually mean. It\u0026rsquo;s just data and the interpretation we apply to the matrix makes it information.\nGoal The goal of supervised learning is to discover relationships in input data that map it to some output data. In practice, the output will be a single feature of the measurements called the target.\nIn the case of the set of data about houses, the target may be selected to be the price of the house. This will let us train the system so that given the other input data (the features about the house), we can predict the price of the house. Thus, uncovering the relationships between features like square footage or number of bedrooms, and the price.\nA More Accurate Definition After the above descriptions, we can now define a more accurate definition of supervised learning.\nThe goal of supervised learning is to find a mathematical function that takes an input matrix (set of samples) and produces a vector as output that is close enough to a target vector.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/supervisedlearning/","summary":"Overview Supervised learning is a branch of machine learning that can be described as finding patterns or relationships in data that has already been measured. The \u0026ldquo;supervised\u0026rdquo; part of the phrase comes from the fact that we need to monitor the output that the system gives for a given input and to tell the system how close or far away it is from the measured result.\nAs a short example, suppose we looked at 100 houses and measured both their square footage and market value.","title":"Supervised Learning"},{"content":"Since setting up my Github workflows as detailed HERE, I have noticed that the code coverage report generation tool that was being used was generating a lot of false negatives and certain code (mainly .except calls) were causing a \u0026ldquo;Not covered\u0026rdquo; status to be reported.\nTarpaulin is an alternative tool that has better integration with Rust, however the downside of it is that due to the instrumentation required it\u0026rsquo;s only available on x86_64 processors and only on the Linux OS.\nLuckily however, Github workflows allow running on exactly that architecture. This is a very simple change to make, and uploading the codecov report to CodeCov is still the same, the only difference is how the report has been generated.\nThe new workflow is as follows:\nname: Test Coverage on: [push] jobs: test: name: coverage runs-on: ubuntu-latest container: image: xd009642/tarpaulin:develop-nightly options: --security-opt seccomp=unconfined steps: - name: Checkout repository uses: actions/checkout@v2 - name: Generate code coverage run: | cargo +nightly tarpaulin --verbose --all-features --workspace --lib --timeout 120 --out Xml - name: Upload to codecov.io uses: codecov/codecov-action@v2 with: # token: ${{secrets.CODECOV_TOKEN}} # not required for public repos fail_ci_if_error: true The main difference to previous is that we use a container image for running Tarpaulin on nightly, on a specific snapshot, and that to generate the code coverage, we use the one liner:\ncargo +nightly tarpaulin --verbose --all-features --workspace --lib --timeout 120 --out Xml\rOnce the report is output to the XML file, the upload to Codecov remains the same!\nThe result is that the code coverage report seems to be a lot more reliable, and also the setup and running of Tarpaulin is so much easier too.\nFor more information, Tarpaulin can be found HERE\n","permalink":"https://forgottenmaster.github.io/posts/rust/infrastructure/tarpaulin/","summary":"Since setting up my Github workflows as detailed HERE, I have noticed that the code coverage report generation tool that was being used was generating a lot of false negatives and certain code (mainly .except calls) were causing a \u0026ldquo;Not covered\u0026rdquo; status to be reported.\nTarpaulin is an alternative tool that has better integration with Rust, however the downside of it is that due to the instrumentation required it\u0026rsquo;s only available on x86_64 processors and only on the Linux OS.","title":"Tarpaulin"},{"content":"I recently found out how to add appropriate GitHub action workflows to a Rust project in a GitHub repository. It was surprisingly easy to do with GitHub actions, and even lets you put a badge in the README.md file.\nTemplate Repository Luckily I stumbled upon this template repository containing exactly the GitHub workflows that I needed. In order to add the appropriate workflows to a Cargo project, clone this repository. The repository is located at: https://github.com/BamPeers/rust-ci-github-actions-workflow\nClick \u0026ldquo;Use this template\u0026rdquo; to create a new repository from it:\nAfter this you\u0026rsquo;ll be redirected to a page to setup the new repository as usual:\nCreating a CodeCov account CodeCov is a fantastic site that hosts code coverage reports generated by tools (or in this case, by the GitHub workflows supplied by the template repository we used). It\u0026rsquo;s free to use if either there are less than 5 users, or it\u0026rsquo;s considered an open source repository.\nCreating an account linked to your GitHub account is easy, just log in using your GitHub credentials, and authorize the app:\nSeeing your reports Once your CodeCov account is linked with GitHub, and the GitHub workflows run to produce your code coverage report, you can see those reports on the main dashboard:\nClicking on one of the repositories here will take you to a report overview page for that repository, showing the current/last run of code coverage in various formats:\nThe Sunburst diagram on the left hand side is interactable and lets you see which files have good or bad coverage at a glance.\nAt the bottom there is a directory navigation panel which also displays the code coverage percentage, and lets you drill down into each individual file. Clicking on an individual file will take you to a source code display and show exactly which lines are covered and which are not:\nAnd you can see from this view, that only 1 line is red (uncovered by tests).\nBadges! Adding badges to the README.md file of your repository that reflect the current state of the workflows and/or CodeCov is easy. For the workflows, navigate to the workflows panel again, and click on an individual workflow, then the hamburger menu, and finally the \u0026ldquo;Create Status Badge\u0026rdquo; option:\nYou\u0026rsquo;ll then be presented with a popup with the markup that you can copy/paste into the README.md file\nFor CodeCov, the badge is just as easy to find. First click on one of the repositories, then Settings, and finally Badge. See the below screenshot as an example:\nPop in the README.md Finally pop these markup codes into the README.md file of your repository and now everybody can see the status of your workflows and code coverage!\n","permalink":"https://forgottenmaster.github.io/posts/rust/infrastructure/githubworkflow/","summary":"I recently found out how to add appropriate GitHub action workflows to a Rust project in a GitHub repository. It was surprisingly easy to do with GitHub actions, and even lets you put a badge in the README.md file.\nTemplate Repository Luckily I stumbled upon this template repository containing exactly the GitHub workflows that I needed. In order to add the appropriate workflows to a Cargo project, clone this repository. The repository is located at: https://github.","title":"Github Workflows"},{"content":"What is constant evaluation? Constant evaluation is basically just performing calculations at compile time rather than runtime. There are some constructs that we would like to represent with a high level programming language such as Rust or C++, but for which, if we know the parameters to the function at compile time, then we can also calculate the output at compile time as well.\nOne thing to note is that it\u0026rsquo;s not possible to do everything at compile time since we generally require I/O, and whenever I/O is involved it can no longer be performed or baked in at compile time. However we can isolate sections of the program that are able to be executed by the compiler and ensure they can be executed in a constant evaluation environment.\nWhy is it needed? Most of the time, source code is compiled by a compiler into machine code which then executes on the target machine. The compiler performs checks such as type checking, and when it outputs the machine code has additional duties such as filling in function pointers to virtual functions, etc. The source code provides a nice high level way to code using types, interfaces, and other such concepts, and the compiler has the job of turning that into CPU instructions.\nHowever, the more CPU instructions are required, the slower the execution will be. Usually we are trying to target a high frame rate and are therefore limited in the time we can spend on each frame. Every operation performed by the CPU will eat up this budget.\nThe more calculations we can move from runtime to compile time, the better the execution will be. Additionally since the program is compiled generally much less than it\u0026rsquo;s executed, it would be a better trade-off of time to have a longer compile time but shorter run time.\nAnother point is that if calculations are done at compile time, with the results stored in the binary itself in the data section, older hardware will be able to run it due to the lower load on the CPU required.\nHow do we do this in Rust? Compared to a language like C++, Rust has comparatively weaker compile time capabilities, but these capabilities are being expanded all the time due to the open source nature of the language. However, compared to other languages that have no compile time/const evaluation capabilities at all, the features we have access to are decent enough.\nThere are four main building blocks for constant evaluation in Rust. The most common method of metaprogramming is the use of macros.\nMacros A macro in rust looks like a function, except that instead of taking values as parameters, and producing a value as an output, it takes code as input and produces replacement code as output.\nIn languages without a deep macro system such as C++, macros are often implemented as simple textual substitutions with no knowledge of the types of syntax accepted.\nIn Rust there are actually 2 different types of macros:\n Declarative macros - these look a lot like functions in Rust, but are different, and identified as a macro by the little \u0026ldquo;!\u0026rdquo; that Rust requires be placed before the opening parenthesis. These are more powerful than macros in C++ for example, but more limited than the second form. Procedural macros - these are compiled in their own separate library and imported to be used, but these have access to the entire Abstract Syntax Tree (AST) that they\u0026rsquo;re decorating. Whereas declarative macros can only add/expand syntax and can\u0026rsquo;t remove syntax, proc macros can rewrite the entire item they\u0026rsquo;re attached to. These are more similar to function decorators in Python.  There\u0026rsquo;s too much to macros to explain here, so i\u0026rsquo;ll do a separate post on them which will let me explore them deeper myself too. However, for a quick example of a macro, here\u0026rsquo;s one that can create a HashMap for us:\nmacro_rules! hashmap { // This case handles the logic and is used when the parameter list does NOT  // have a trailing comma (e.g. hashmap!(22 =\u0026gt; \u0026#34;twenty-two\u0026#34;, 35 =\u0026gt; \u0026#34;thirty-five\u0026#34;))  ($($key:expr =\u0026gt; $value:expr),*) =\u0026gt; { { let mut hm = ::std::collections::HashMap::new(); $(hm.insert($key, $value);)* hm } }; } fn main() { let hm = hashmap!(\u0026#34;one\u0026#34; =\u0026gt; 1, \u0026#34;two\u0026#34; =\u0026gt; 2, \u0026#34;three\u0026#34; =\u0026gt; 3); } Generics Generics allow for complex composite types to be defined that are known in their entirety at compile time, but which the compiler might be able to compile down to a no-op in certain cases. We call these zero-cost abstractions and they\u0026rsquo;re quite common in Rust. It\u0026rsquo;s common to define a transparent wrapper around another type, for example:\n#[repr(transparent)] struct NewType\u0026lt;T\u0026gt;(T); // NewType is a wrapper around T  In the above snippet we can make a wrapper type, and we can compose it with other generic types as needed to be able to give information to the compiler for reasoning, and then have that entirely compiled away at runtime so there\u0026rsquo;s no overhead.\nThe next tool in our compile time toolbox in Rust is the const function\nConst function Whereas macros allow us to expand or replace syntax, const functions let us write familiar looking (albeit very restricted) functions in a way that let them be executed at compile time if the parameters are constant.\nIn a macro, the syntax expansion is performed at compile time but the resulting syntax is still only executed at runtime, which allows us to still make use of heap-allocated structures and such.\nIn a const function, the function itself is executed at compile time (if the parameters are constant), but can still be run at runtime (if the parameters are non-constant).\nHowever because we must write the function with the possibility that it\u0026rsquo;s executing at compile time, we have a whole host of things we can\u0026rsquo;t make use of. In addition to not being able to heap allocate structures, we can\u0026rsquo;t for example, make use of trait bounds or traits in a const function.\nDue to these limitations, we have to code in a more low level and possibly unsafe way than we have before, but this is something I\u0026rsquo;ll be exploring in the upcoming posts.\nAs a simple example, the following is a const function that takes two integers and adds them together:\nconst fn add_together(a: u32, b: u32) -\u0026gt; u32 { a + b } Const keyword The const keyword identifies to the compiler that this is a const context. The left-hand side of a const declaration must explicitly state the type (no type inference). The right-hand side of a const declaration must be entirely calculable at compile time.\nA function written as a const function (as the example add_together function above) can also be invoked at runtime with parameters deduced at runtime, however we can force it to be executed at compile time by introducing a const context as such:\nconst ADDED: u32 = add_together(add_together(1, 2), add_together(3, 4)); The compiler will execute this at compile time and the resulting binary will simply have the result (10) embedded.\nLimitations The limitations of writing code to be executable within a constant context are detailed HERE\nHowever the main limitations are:\n No heap allocations (no allocator present in the compiler) No floating point operations (floats aren\u0026rsquo;t reliable enough for use in the compiler) No bounds on generics in const functions No comparing raw pointers (pointers don\u0026rsquo;t exist in the same way in the compiler)  The other big limitation will be that large parts of the standard library aren\u0026rsquo;t executable in constant contexts and that we will have to find other means to build the same functionality.\nThere are crates that have been developed to add this functionality into a constant context but for the sake of education, we\u0026rsquo;ll be developing this kind of functionality ourselves into a package called consteval\n","permalink":"https://forgottenmaster.github.io/posts/rust/consteval/intro/","summary":"What is constant evaluation? Constant evaluation is basically just performing calculations at compile time rather than runtime. There are some constructs that we would like to represent with a high level programming language such as Rust or C++, but for which, if we know the parameters to the function at compile time, then we can also calculate the output at compile time as well.\nOne thing to note is that it\u0026rsquo;s not possible to do everything at compile time since we generally require I/O, and whenever I/O is involved it can no longer be performed or baked in at compile time.","title":"Introduction"},{"content":"Overview We often find ourselves wanting to find a smaller sequence, inside of a larger sequence. This often manifests as searching for a substring in a larger text, or determining if a list of numbers is a sublist of another.\nWe can achieve this by looking at a sequence of characters at the beginning of the larger sequence (known as the haystack) that is the same length as the smaller sequence (known as the needle). This sliding motion can be viewed as the following animation\nHowever, with the Naiive approach, when we compare the needle against a specific window in the haystack, we always start by comparing the first elements, then the second elements, and so on until we either hit a mismatch, or have compared all elements as equal.\nThis is obviously a problem for larger sequences as, every time the window slides one position to the right in the haystack, we can end up checking every character in the needle.\nHow is Rabin-Karp different? The basic movement of the \u0026ldquo;window\u0026rdquo; that we\u0026rsquo;re checking in the haystack is still the same - it slides right by one position each step. However where Rabin-Karp differs is by avoiding checking each character to find a mismatch if it knows a mismatch occurs. The way it achieves this is by using a single numeric hash code that represents the sequence of elements we are looking for (known as the fingerprint).\nThe fingerprint of the initial window can be calculated as usual by hashing all the elements in the window. However, where the efficiency comes from with Rabin-Karp is that the method used to calculate the fingerprint allows us to \u0026ldquo;roll\u0026rdquo; that fingerprint each time we move the window to the right by one step.\nWhat this means is, when we move the window to the right by one step, we drop whatever contribution to the overall hash was made by the leftmost element, and then incorporate the new element that\u0026rsquo;s coming in on the right hand side of the window - without having to recalculate all the elements in between.\nHow can it do this? We can do this by multiplying the hash code of each element by a particular, unique number and adding all the values together for a unique fingerprint.\nWe need to first choose a number as a base which we will raise to consecutive powers to determine these multipliers. It doesn\u0026rsquo;t matter too much what this number is, except that it must be greater than 1 (because raising 1 to any power gives 1 which isn\u0026rsquo;t a unique multiplier).\nThe initial fingerprint Given that B is the arbitrary numerical base we will raise, W is the window we\u0026rsquo;re calculating the fingerprint for, and L is the length of that sequence, then the formula is given as follows:\n$$ Fingerprint(W) = \\sum_{n=0}^{L-1} Hash(W_n) \\times B^{L-1-n} $$\nOr, written another way:\n$$ Fingerprint(W) = (Hash(W_0) \\times B^{L-1}) + (Hash(W_1) \\times B^{L-1-1}) + (Hash(W_2) \\times B^{L-1-2}) $$ $$ + \u0026hellip; + (Hash(W_n) \\times B^0 ) $$\n A concrete example It\u0026rsquo;s always difficult to interpret mathematical formulas sometimes, so here is a small and concrete example of calculating the hash code. Given the sequence of integers W=\\([17, 23, 49, 51]\\) we can see that L=4, and if we pick as our base B=2 then in order to calculate the fingerprint, we simply do the following:\n$$ Fingerprint(W) = (Hash(17) \\times 2^3) + (Hash(23) \\times 2^2) + (Hash(49) \\times 2) + Hash(51) $$\nIf we furthermore take Hash(X) to be X itself (as they\u0026rsquo;re already integers), then we can easily compute the final fingerprint as:\n$$ (17 \\times 8) + (23 \\times 4) + (49 \\times 2) + 51 $$ $$ = 136 + 92 + 98 + 51 $$ $$ = 377 $$\n Rolling In order to update the fingerprint after the initial calculation, we will need to do the following in sequence:\n Subtract the highest power term on the left Multiply the fingerprint by B - this has the effect of raising the remaining powers by 1 Add the new term to the fingerprint  Let\u0026rsquo;s look at these in turn\u0026hellip;\nSubtract the highest power term When we slide the window to the right, the elements on the left of the window (and the left of the above formula) will \u0026ldquo;drop off\u0026rdquo;. As we have raised each element to successive powers, we know how to calculate the multiplier added to the hash of element 0. The multiplier is simply \\(B^{L-1}\\), and the value we will subtract from the fingerprint is \\(Hash(W_0) \\times B^{L-1})\\)\nIn the concrete example shown above, we subtract \\((Hash(17) \\times 2^3)\\) and are left with:\n$$ Fingerprint(W) = (Hash(23) \\times 2^2) + (Hash(49) \\times 2^1) + (Hash(51) \\times 2^0) $$\nRaise the powers We now multiply by B (in the concrete example this is 2). Since the following is true:\n$$ (X + Y) \\times Z \\equiv (X \\times Z) + (Y \\times Z) $$\nThis has the effect of multiplying each term in the formula by B, which furthermore, has the effect of raising the power by 1 of each term.\nAfter multiplying by 2, the concrete example will be given as:\n$$ Fingerprint(W) = (Hash(23) \\times 2^3) + (Hash(49) \\times 2^2) + (Hash(51) \\times 2^1) $$\nAdd the new term This is the easiest part. Since the powers of the other terms have been raised, we only need to add the new term into the \u0026ldquo;zeroeth\u0026rdquo; power position, which is simply adding the hashcode of the element.\nIn our concrete example, say that we are rotating in a new element, 101, on the right as the window slides. The new fingerprint is represented as:\n$$ Fingerprint(W) = (Hash(23) \\times 2^3) + (Hash(49) \\times 2^2) + (Hash(51) \\times 2) + Hash(101) $$\nWhich, again assuming Hash(X) is X, gives the new fingerprint of:\n$$ (23 \\times 8) + (49 \\times 4) + (51 \\times 2) + 101 $$ $$ = 184 + 196 + 102 + 101 $$ $$ = 583 $$\nBut what about overflow? As you can see however, from this formula that for large sequence lengths, even for the smallest viable base of 2, will result in an integer growing too large to fit in any sensible datatype.\nAfter all \\(2^{500000}\\) is\u0026hellip;..huge\nWhat can we do to keep these numbers small?\u0026hellip;.Modular Arithmetic\nModular Arithmetic Using modular arithmetic involves using integer division of the number at each step by some known value (known as the modulus) and retaining the remainder. In this way all the numbers \u0026ldquo;wrap around\u0026rdquo; but the formula is still valid, it\u0026rsquo;s just being done on a number circle rather than a number line.\nAn illustration of this is with clocks. Clocks use a modulus of 12 before they wrap around, but you can still add, for example, 4 hours, or subtract 4 hours when that wraps around.\nAdapting the formulas is easy enough, we just apply the modulo operator when calculating the hash code of an element, and at every step of the calculation.\nThe tricky part of this however, is when we are calculating the powers - we can\u0026rsquo;t calculate the power and then apply the modulus as the power calculation overflows. Instead we need to recognise that:\n$$ pow(X, 3) \\equiv (X \\times X \\times X) $$\nAnd this lets us apply modulo at every step, as in (assuming modulus of M):\n$$ ((((X \\mod M) \\times X) \\mod M) \\times X) \\mod M $$\nOne thing we need to accumulate while doing this is the multiplier we have applied to the left most element (\\(B^L-1\\)) which we then can just use later when we\u0026rsquo;re subtracting the left hand term.\nCoding it up Following is a walkthrough of how to code this algorithm up in Rust, the complete source can be found at the playground link HERE\nI\u0026rsquo;ll run through the steps one by one below, explaining the code for those who may be unfamiliar with Rust syntax.\nStep 1 - HashCode trait In Rust we are able to get the hash of a type that implements the Hash trait for use in a HashMap. However this method requires passing the type through a Hasher and incurring a performance cost to generate a hash code for even basic types such as integers.\nSince an integer is its own hash code, and a character can be converted to an integer as a hash code we don\u0026rsquo;t need to do anything too tricky.\nFor the element hashing therefore, I\u0026rsquo;ve gone with creating a new trait:\ntrait HashCode { fn hash_code(\u0026amp;self) -\u0026gt; u64; } That is, a simple trait (interface in other languages) with a single function that operates on an const reference to the element (\u0026amp;self) and returns an integer hash code for it (u64).\nStep 2 - Implementing HashCode for types we want to use The next step is to actually implement the trait for the types we want to be able to use in the sequences for the algorithm. Rust allows us to implement custom traits for existing types in order to extend their functionality - even primitive types. For this, we will implement it for the following primitive types:\n i32 - A signed 32-bit integer. For this, if it\u0026rsquo;s a positive value we can simply keep the value and cast it to a wider, 64 bit integer with no loss. If it\u0026rsquo;s negative, because we know that the range of a 64-bit integer covers the range of the 32-bit integer then we will map the negative portion of i32 by negating it - and then subtracting from u64::MAX. u32 - This involves a simple widening cast which is safe since we know all unsigned 32-bit integer values will fit into an unsigned 64-bit variable. u64 - For this, the hash code is the value itself so it simply returns it. char - Characters in rust are 4 bytes large representing a unicode codepoint. For the same reason as u32 will be safely castable into u64, so are characters.  The implementation of the HashCode trait then is as follows:\nimpl HashCode for i32 { fn hash_code(\u0026amp;self) -\u0026gt; u64 { if *self \u0026gt;= 0 { *self as u64 // if we\u0026#39;re positive, just keep our value and cast as a u64  } else { std::u64::MAX - self.abs() as u64 // if we\u0026#39;re negative, just negate it and then subtract from u64 MAX to use the upper end  } } } impl HashCode for u32 { fn hash_code(\u0026amp;self) -\u0026gt; u64 { *self as u64 } } impl HashCode for u64 { fn hash_code(\u0026amp;self) -\u0026gt; u64 { *self } } impl HashCode for char { fn hash_code(\u0026amp;self) -\u0026gt; u64 { (*self).into() } } Step 3 - Generating a fingerprint In order to be able to roll a fingerprint as described previously, we first need to construct one from scratch for a given range. Given a sequence of elements (that implement our HashCode trait), along with a base to use, and a modulus to keep the numbers small as described in the section on modular arithmetic - we would like a function that will generate the fingerprint for the sequence. Additionally, we want to return the calculated multiplier for that left-most term so that we don\u0026rsquo;t need to calculate it later.\nTo allow us to keep the numbers small, we don\u0026rsquo;t use the pow function, but use an imperative loop to accumulate the fingerprint and base offset.\nThe code for this function looks like the following (note that this assumes the list isn\u0026rsquo;t empty and will panic if it is due to the [0] access):\nfn generate_fingerprint\u0026lt;T: HashCode\u0026gt;(list: \u0026amp;[T], base: u64, modulus: u64) -\u0026gt; (u64, u64) { let mut fingerprint = list[0].hash_code() % modulus; let mut base_offset = 1; for elem in \u0026amp;list[1..] { let elem_hash = elem.hash_code() % modulus; fingerprint = (((fingerprint * base) % modulus) + elem_hash) % modulus; base_offset = (base_offset * base) % modulus; } (fingerprint, base_offset) } Step 4 - Rolling the fingerprint In order to \u0026ldquo;roll\u0026rdquo; the fingerprint and generate the next one from the previous one, we need the following pieces of information:\n The previous fingerprint - for obvious reasons The calculated multiplier (called the base offset) for the left-most element - we use this to calculate the final value to subtract from the fingerprint The old element we\u0026rsquo;re rotating out - we need this to get the hash code from, which is combined with the base offset to get the value to subtract The new element we\u0026rsquo;re rotating in - we need this again to get the hash code which will be added to the fingerprint The base - we need this to allow us to \u0026ldquo;raise the powers\u0026rdquo; of all the remaining terms in the fingerprint after removing the left most term The modulus - we need this for the same reason as we needed it in generate_fingerprint. It lets us perform modular arithmetic and keep the values small  One additional thing to note which could be missed is that before removing the term we want to be rid of, we must first add the modulus to the fingerprint. This is because we\u0026rsquo;re using modular arithmetic - it might wrap around so that the current fingerprint value is less than the value we want to subtract. Adding the modulus essentially adds one full rotation to the fingerprint - allowing us to subtract the term safely.\nThe function looks like the following:\nfn roll_fingerprint\u0026lt;T: HashCode\u0026gt;( mut fingerprint: u64, base_offset: u64, old_term: \u0026amp;T, new_term: \u0026amp;T, base: u64, modulus: u64, ) -\u0026gt; u64 { let old_term_hash = old_term.hash_code() % modulus; let new_term_hash = new_term.hash_code() % modulus; let term_to_subtract = (old_term_hash * base_offset) % modulus; fingerprint = fingerprint + modulus - term_to_subtract; // remove the old term after adding the modulus on to protect against underflow.  fingerprint *= base; // power shift all other terms up by 1  (fingerprint + new_term_hash) % modulus // return new fingerprint after adding in new term and modding } Step 5 - Putting it together Now we can write the actual Rabin-Karp implementation. This function will take a sequence known as the needle, and an equal or larger sequence known as the Haystack, along with the standard base and modulus to use.\nIt will return a boolean value indicating whether the needle was found in the haystack or not. This could be extended if necessary in the future to return the index of the match.\nFor Rabin-Karp, the steps are as follows:\n Calculate the fingerprint of the needle - this will never change and is the fingerprint we\u0026rsquo;re trying to match Handle the trivial case where the needle is empty - an empty needle is always present in any list Generate the initial fingerprint for the slice of the haystack of the same length as the needle - This is the starting fingerprint to match, and also gives us the base offset to use If they match already, then we return a match Otherwise we slide the window along by 1 each time (until the right side of the window hits the end of the haystack), rolling the hash each time to remove the old left hand element and add the incoming element Each time, check if there\u0026rsquo;s a match If we\u0026rsquo;ve checked all windows and not found a match, it\u0026rsquo;s not there  One thing to note though is that it is sufficient to detect a mismatch by mismatching fingerprints however it\u0026rsquo;s not sufficient to detect a match with matching fingerprints. This is because we\u0026rsquo;re using modular arithmetic and a mathematical formula for calculating the final fingerprint - it\u0026rsquo;s possible different sequences end up with the same fingerprint.\nThe efficiency of this algorithm is in the fact we don\u0026rsquo;t have to check elementwise when we know there\u0026rsquo;s a mismatch, and only have to check elementwise in the cases that the fingerprints match.\nBecause of this requirement that elements need to be compared as equal, our Rabin-Karp function requires an additional bound on it\u0026rsquo;s generic type, that of PartialEq (which lets us use the == operator). The final function is as follows:\nfn rabin_karp\u0026lt;T: HashCode + PartialEq\u0026gt;( needle: \u0026amp;[T], haystack: \u0026amp;[T], base: u64, modulus: u64, ) -\u0026gt; bool { if needle.len() == 0 { true // we can always find the empty list inside any list  } else { // get the initial fingerprints and window  let (needle_fingerprint, _) = generate_fingerprint(needle, base, modulus); let needle_len = needle.len(); let haystack_len = haystack.len(); let mut window = \u0026amp;haystack[0..needle_len]; let (mut window_fingerprint, base_offset) = generate_fingerprint(window, base, modulus); // check the initial fingerprints/window for match  if needle_fingerprint == window_fingerprint \u0026amp;\u0026amp; needle == window { return true; } // otherwise run a starting index for the window from 1 up to and including haystack_len-needle_len.  for window_index in 1..=(haystack_len - needle_len) { let new_window = \u0026amp;haystack[window_index..window_index + needle_len]; let roll_out = \u0026amp;window[0]; let roll_in = \u0026amp;new_window[needle_len - 1]; window_fingerprint = roll_fingerprint( window_fingerprint, base_offset, roll_out, roll_in, base, modulus, ); window = new_window; if needle_fingerprint == window_fingerprint \u0026amp;\u0026amp; needle == window { return true; } } // wasn\u0026#39;t found or we\u0026#39;d have returned true on a match.  false } } Testing the implementation The following code snippet is used to test the implementation of Rabin-Karp finds a match or not in the correct cases:\nfn main() { const BASE: u64 = 253; const MODULUS: u64 = 101; println!( \u0026#34;[2, 4, 1] in [7, 8, 2, 4, 1, 5] =\u0026gt; {}\u0026#34;, rabin_karp(\u0026amp;[2, 4, 1], \u0026amp;[7, 8, 2, 4, 1, 5], BASE, MODULUS) ); println!( \u0026#34;[2, 4, 1] in [7, 8, 2, 4, 3, 5] =\u0026gt; {}\u0026#34;, rabin_karp(\u0026amp;[2, 4, 1], \u0026amp;[7, 8, 2, 4, 3, 5], BASE, MODULUS) ); } We\u0026rsquo;re using relatively small sequences, and small modulus but this algorithm is pretty efficient when scaled up at large sizes. In my experiments, a haystack of length 1,000,000 and a needle of length 500,000 resulted in a benchmark time on the naiive algorithm of over 1 minute, whereas with the Rabin-Karp implementation benchmarked the same problem at around 13 milliseconds\nThe output from this is:\n[2, 4, 1] in [7, 8, 2, 4, 1, 5] =\u0026gt; true\r[2, 4, 1] in [7, 8, 2, 4, 3, 5] =\u0026gt; false\r","permalink":"https://forgottenmaster.github.io/posts/algorithms/searching/rabinkarp/","summary":"Overview We often find ourselves wanting to find a smaller sequence, inside of a larger sequence. This often manifests as searching for a substring in a larger text, or determining if a list of numbers is a sublist of another.\nWe can achieve this by looking at a sequence of characters at the beginning of the larger sequence (known as the haystack) that is the same length as the smaller sequence (known as the needle).","title":"Rabin-Karp Pattern Search"},{"content":"Repository A Git repository has 2 main storages:\n  The object store This storage is used to store the actual data, commits, etc. for the repository and is a permanent record of all the changes to the repository. This is efficiently copied across when cloning an existing repository to provide an exact duplicate of not only the data but the whole history.\n  The index This is a temporary store which records changes that have yet to be made to the repository but have been staged.\n  We\u0026rsquo;ll look at these in more detail now\nObject Store The object store portion of the repository stores the 4 main atomic types of objects that Git uses. These are:\n  Blobs - The contents of a single version of a file is represented as a Blob (a contraction of \u0026ldquo;binary large object\u0026rdquo;). A blob is simply the bytes of the file, along with a hash of the contents. Git doesn\u0026rsquo;t store any metadata along with the blob here, but that\u0026rsquo;s stored in other structures within the object store.\n  Trees - A tree is the information about 1 level of the directory structure. It contains references to all the blobs for the files in one directory, along with their path names and some metadata. It may also refer to child trees in order to build up a complete hierarchy by following the root level tree.\n  Commits - A commit object contains the author name/email, along with the commit data and the message that was provided to the log. Each commit points to a single tree object which is the root tree that allows the whole state of the repository to be reproduced for a given commit. Additionally, every commit except for the initial commit will have one (or more - in the case of branch merging) parent commits which allows the software to traverse backwards through the commit chain.\n  Tags - A tag object is an arbitrary string, usually human readable which is assigned to a specific object to allow for it to be referenced by a more readable string rather than the hash that\u0026rsquo;s generated for the object. This is usually used to give certain commits descriptive names, such as for assigning releases.\n  The Index The second storage in a repository is the index, which is a temporary binary file that captures a version of the project\u0026rsquo;s structure at some moment in time. This could be a reference to a given commit (from which we can traverse the tree objects and the blob objects to recreate the directory structure of the whole repository).\nThe developer is allowed to add and remove changes to the index in incremental steps, before finally applying the contents of the index to the object store.\nWhen one runs commands such as add, rm, or mv the changes are staged in the index in a quick and efficient way.\nThe more expensive persistence of the index is done only when the developer commits those changes to the repository.\nTherefore the index is a cache of sorts that allows adding/removing/restoring files as needed in a staging environment before finally committing the index to the repository, resulting in the more heavy processing required.\nThe index also plays an important part in merges, since it can store multiple copies of the same file for merging purposes.\nHashing Every object in the object store in Git is referenced by a unique identifier that represents it, and it only.\nIn the case of Git, this identifier is the SHA-1 hash of the contents of the blob.\nTherefore the object store is basically a hash map that allows efficient retrieval of an object from the store given its unique hash of the contents.\nThis is different to the way that most source control software will store and retrieve files. In others, the files are indexed by file path derived from the directory structure on disk. However Git indexes by hashing the content of the files, meaning if you have two hashes that are the same, you can guarantee that the contents of the blob are the same.\nAdditionally, and more powerfully, since Git stores the entire history as referenced from a head (most recent) commit, and this commit is an object in the store, just like the others, the hash of a commit or tree is derived from any child trees or commits.\nThis is a very powerful statement because it means that if you have two identifiers of two commits, and they are the same - not only do you know the same changes were made in those commits, but you know that the entire history of the repository was the same in both cases too.\nPack Files Git stores the complete contents of each version of every file, since it\u0026rsquo;s deriving the identifier of the file version from the contents of the data itself, it can only operate on entire and complete file contents.\nBut how can it store many different instances of the same file when they only differ by a few characters?\nGit solves this by using pack files, and it achieves it by storing the full contents of a certain blob, and other blobs that are close to it in content are stored alongside that as a diff.\nThus if you have version A of a file, and version B of a file which only has a single character changed, then the pack file will have the full contents of A, along with a diff with the changed character.\nHowever, because Git indexes data by the content itself, and doesn\u0026rsquo;t care about directory structure, it can store different blobs that are in no way related together in a pack file, as long as they\u0026rsquo;re close to each other in nature.\nIn order to be able to locate an object within a pack, Git will store the original SHA-1 hash for each alongside either the complete content or the delta for that content.\nObject Store Diagram A picture is worth a thousand words, as the saying goes, so below is a diagram of a sample object store - showing how blobs, trees, commits, and tags all reference one another.\nIn the diagram, the blobs, being the raw data, are located at the bottom of the diagram. Blobs are standalone objects and don\u0026rsquo;t reference anything, but are referenced by tree objects. These are represented as a rectangle.\nTree objects can reference blobs, and other child trees, along with metadata identifying the path on the user\u0026rsquo;s disk so that it can be reconstructed in the same way on disk. These are represented as a triangle.\nA circle represents a commit here, which may have one or more parent commits, allowing us to traverse backward through history (the root/initial commit has no parent), and points to a single tree, that represents the entire state of the repository at that commit.\nTags point to a commit, and are represented as a parallelogram.\nAs you can see in the diagram, Git stores references from child commits, to their parents. This makes it super easy to add a new commit, as you make the commit, and point it to the previous one - then the new commit becomes the head.\nFrom the diagram, we can see that commit #11235 has information about the author, the commit message, and the timestamp. Additionally has a reference to the parent (initial) commit.\nThe commit object references a tree that in turn, references two blobs, and a child tree referencing a blob.\nThe initial commit references a tree that references two of the same blobs (dead23 and feeb1e) as the new one, but doesn\u0026rsquo;t have access to tree #1010220.\nTherefore we can see from the diagram that the new commit added a file into a subfolder, but all the other files remain the same. Because Git indexes files by their content only, which is immutable, it\u0026rsquo;s able to refer to the same instances of the blobs.\nAccessing the object store You\u0026rsquo;re able to access any given object from the object store by it\u0026rsquo;s SHA-1 hash, which as we\u0026rsquo;re now aware, is the ID of the object (and the hash of the contents).\nWe can do this with the cat-file subcommand. The following is an example of using it to retrieve a data blob from the object store:\ngit cat-file -p 2d10\rText inside the file\rNote that we don\u0026rsquo;t need the full identifier of the blob, just enough of the prefix to allow Git to locate it for display.\nWe can access any object from the store this way, for example accessing a commit\ngit cat-file -p 7a97\rtree 17efabebf0f96829e91a5b9dc358b2e2c2eec05e\rparent 8bddb3945ff1f5978c57fc298c2ab3ba88945a83\rauthor Robin Firth \u0026lt;theforgottenmaster@googlemail.com\u0026gt; 1636230599 +0000\rcommitter Robin Firth \u0026lt;theforgottenmaster@googlemail.com\u0026gt; 1636230599 +0000\rAdded file only on branch\rAnd we can see that the commit has a reference to its tree object, parent commit, and the author metadata.\n","permalink":"https://forgottenmaster.github.io/posts/git/basic_concepts/","summary":"Repository A Git repository has 2 main storages:\n  The object store This storage is used to store the actual data, commits, etc. for the repository and is a permanent record of all the changes to the repository. This is efficiently copied across when cloning an existing repository to provide an exact duplicate of not only the data but the whole history.\n  The index This is a temporary store which records changes that have yet to be made to the repository but have been staged.","title":"Basic Concepts"},{"content":"Creating a repository There are two different ways to establish a repository in Git in contrast to other version control systems where there is only creating the initial repository.\nIn Git we are also able to clone a repository to create copy containing all of the objects and history from an existing repository.\nInitialising a new repository This is how we can create a brand new Git repository which may either be empty, or contain data/files already.\nIn order to initialise a new repository we simply use the init subcommand while inside the root folder we want to turn into a repository. For example if we want to make a brand new one.\nmkdir repo\rcd repo\rgit init\rOf course, the repo folder need not be empty, and the steps would be the same to initialise an existing folder as a repository.\nCloning an existing repository Git is also able to easily make a clone of an existing repository which will copy all of the objects and history across with it, but allows branching and commits to this new repository independant of the source one.\nIn open source development, this is the most common way, as a developer will make a clone of the central repository in order to make their own fixes that they can then request to be merged back into the central/online repository.\nIn order to clone a repository we can do\ngit clone \u0026lt;path-to-repository\u0026gt; repo\rWhere path-to-repository can either be a relative or absolute path to another local repository on the machine or something more exotic such as a URL.\nThe most popular way to clone is using the GitHub URL to the repository.\nAdding and removing files In order to add and remove files to the repository, one can use the add and rm subcommands, along with a set of files to add or remove.\nWhen a file is added or removed however, it is not immediately added to the repository, or removed from the repository but only staged as a change to make to the repository.\nIn order to add a file that is either currently untracked by the repository, or has been edited and needs updating in the repository, we can do the following\ngit add file_to_add.txt\rAnd we can remove a file that is currently tracked by the repository also\ngit rm file_to_remove.txt\rThis will add or remove the file from the staged change as mentioned previously\nViewing staged changes We can use the subcommand status to display the list of files that have been staged to be added or modified, the list of files staged for removal, and the list of files that Git has found in the folder that aren\u0026rsquo;t tracked yet.\nThis last type is the state that a file will be in when you first create it and add it to the folder (or a subfolder), but before it\u0026rsquo;s been staged for addition with the add subcommand.\nFor example, after the following sequence of commands where we add a file, and then remove a file:\ngit add file_to_add.txt\rgit rm file_to_remove.txt\rWe will see the following when we ask for the status:\ngit status\rOn branch master\rChanges to be committed:\r(use \u0026quot;git restore --staged \u0026lt;file\u0026gt;...\u0026quot; to unstage)\rnew file: file_to_add.txt\rdeleted: file_to_remove.txt\rUntracked files:\r(use \u0026quot;git add \u0026lt;file\u0026gt;...\u0026quot; to include in what will be committed)\runtracked_file.txt\rCommitting staged changes Once we\u0026rsquo;ve added and removed files to the stage, and we are ready for the repository to be updated to reflect the changes, we will need to perform a commit operation.\nWhen we commit to the repository, we are required to give a message identifying the changes that were made.\nWe can either do this by specifying it on the command line to the commit subcommand as follows:\ngit commit -m \u0026quot;Committing some changes\u0026quot;\rOr, for more detailed messages, they can be composed in a text editor. To do this, just use the commit command without specifying the message:\ngit commit\rWhich will then open the default text editor where you can type a more detailed message, save, and exit - the commit will then be applied.\nOnce staged changes are committed, using the status subcommand again will show that the stage is upto date with the repository, this state is referred to as clean.\nConfiguration The subcommand config can be used to specify various parameters affecting the operation of Git. Before you can make a commit, Git needs to know your name and email address as a minimum.\nThese can be set as such:\ngit config user.name \u0026quot;Name Here\u0026quot;\rgit config user.email \u0026quot;myemailaddress@example.com\u0026quot;\rViewing commits The log subcommand will allow us to see the history of commits done to the repository. They will be displayed from most recent to the oldest, since Git always keeps the head of the commit chain and accesses the others from there.\nEach entry will show the commit hash, along with the author name/email, the date of the commit, and the message.\nFor example after doing two commits to a repository, we may see something like the following on the command line:\ngit log\rcommit 2f988a4223baa3d8bd1db16f8cc777181eb390cd (HEAD -\u0026gt; master)\rAuthor: Robin Firth \u0026lt;theforgottenmaster@googlemail.com\u0026gt;\rDate: Sat Nov 6 19:34:57 2021 +0000\rChanged the text from \u0026quot;Some text\u0026quot; to \u0026quot;Changed text\u0026quot;\rcommit 18dd164cf53fdb05bb757d3e0b452c4bfcf0b38a\rAuthor: Robin Firth \u0026lt;theforgottenmaster@googlemail.com\u0026gt;\rDate: Sat Nov 6 19:33:55 2021 +0000\rInitial commit\rAn individual commit can be viewed with the show subcommand. This is useful if you are given a commit hash in conversation with someone and you want to see what was changed in that commit.\nFor example, to see the initial commit above we can use show and see the following:\ngit show 18dd164cf53fdb05bb757d3e0b452c4bfcf0b38a\rcommit 18dd164cf53fdb05bb757d3e0b452c4bfcf0b38a\rAuthor: Robin Firth \u0026lt;theforgottenmaster@googlemail.com\u0026gt;\rDate: Sat Nov 6 19:33:55 2021 +0000\rInitial commit\rdiff --git a/file_to_add.txt b/file_to_add.txt\rnew file mode 100644\rindex 0000000..9dbfeee\r--- /dev/null\r+++ b/file_to_add.txt\r@@ -0,0 +1 @@\r+Some text\r\\ No newline at end of file\rThis shows the same information as in the log, however also shows the file differences that were part of that commit.\nDiffing commits If you have any 2 commit hashes, you can easily determine the diff that is applied to the first commit, in order to reach the second commit.\nFor example, in order to see the diff that was applied between the initial commit, and the second commit above, we will use the command and see something like:\ngit diff 18dd164cf53fdb05bb757d3e0b452c4bfcf0b38a 2f988a4223baa3d8bd1db16f8cc777181eb390cd\rdiff --git a/file_to_add.txt b/file_to_add.txt\rindex 9dbfeee..41c4a21 100644\r--- a/file_to_add.txt\r+++ b/file_to_add.txt\r@@ -1 +1 @@\r-Some text\r\\ No newline at end of file\r+Changed text\r\\ No newline at end of file\rAnd we can see fromm the diff that \u0026ldquo;Some text\u0026rdquo; was removed, and \u0026ldquo;Changed text\u0026rdquo; was added in its place.\nRenaming files We could achieve the same result as a rename by removing the file, renaming it, and adding it back again, however there is a Git command for renaming or moving a file. For example if we want to rename file_to_add.txt to renamed_file.txt, we can do so as follows:\ngit mv file_to_add.txt renamed_file.txt\rA move can be achieved by keeping the destination filename the same, but specifying the folder that it should be placed under.\nAfter the above rename, running git status shows:\nOn branch master\rChanges to be committed:\r(use \u0026quot;git restore --staged \u0026lt;file\u0026gt;...\u0026quot; to unstage)\rrenamed: file_to_add.txt -\u0026gt; renamed_file.txt\rAdvanced configuration There are actually 3 levels of configuration for the Git config subcommand\n Repository-specific. These are the default level if you use the config subcommand without specifying one of the others. These will take precedence if set and override any system or user level configuration settings. User-specific. These are next lowest in the hierarchy and if not overridden will be applied for all repositories for that user only. These are set by using the \u0026ndash;global option. System-wide. These are the lowest priority and if not overridden will apply to all repositories, of all users on that system. This is specified with the \u0026ndash;system option.  Example: setting name For an example, we\u0026rsquo;ll show setting the user.name configuration setting we showed before, at the 3 levels of configuration.\n Repository-specific  git config user.name \u0026quot;User Name For This Repo Only\u0026quot;\rUser-specific  git config --global user.name \u0026quot;User Name For All Repositories For This User\u0026quot;\rSystem-wide  git config --system user.name \u0026quot;User Name For Everybody\u0026quot;\rViewing configuration settings We can view all the set configuration settings by passing the \u0026ldquo;-l\u0026rdquo; flag to the config subcommand. Which will display something like the following:\ngit config -l\rdiff.astextplain.textconv=astextplain\rfilter.lfs.clean=git-lfs clean -- %f\rfilter.lfs.smudge=git-lfs smudge -- %f\rfilter.lfs.process=git-lfs filter-process\rfilter.lfs.required=true\rhttp.sslbackend=openssl\rhttp.sslcainfo=C:/Program Files/Git/mingw64/ssl/certs/ca-bundle.crt\rcore.autocrlf=true\rcore.fscache=true\rcore.symlinks=false\rcore.editor=\u0026quot;C:\\\\Program Files\\\\Microsoft VS Code\\\\bin\\\\code.cmd\u0026quot; --wait\rpull.rebase=false\rcredential.helper=manager-core\rcredential.https://dev.azure.com.usehttppath=true\rinit.defaultbranch=master\ruser.email=theforgottenmaster@googlemail.com\ruser.name=Robin Firth\rcore.repositoryformatversion=0\rcore.filemode=false\rcore.bare=false\rcore.logallrefupdates=true\rcore.symlinks=false\rcore.ignorecase=true\rUnsetting a configuration setting The \u0026ndash;unset flag can be passed to unset a specific configuration setting at any of the three levels (by omitting, or including either \u0026ndash;global or \u0026ndash;system flags). For unsetting the user.name setting for example at all levels, we could do:\ngit config --unset user.name\rgit config --unset --global user.name\rgit config --unset --system user.name\rAliases You can set your own aliases for more complex commands in Git. This allows you to run complex commands with a shorter subcommand.\nFor example we can make an alias which will commit with a default message, and run it:\ngit config alias.default-commit -- \u0026quot;commit -m 'Default commit message'\u0026quot;\rgit default-commit\r","permalink":"https://forgottenmaster.github.io/posts/git/basic_usage/","summary":"Creating a repository There are two different ways to establish a repository in Git in contrast to other version control systems where there is only creating the initial repository.\nIn Git we are also able to clone a repository to create copy containing all of the objects and history from an existing repository.\nInitialising a new repository This is how we can create a brand new Git repository which may either be empty, or contain data/files already.","title":"Basic Usage"},{"content":"What are traits? Traits in Rust can basically be thought of as interfaces in C#. They can do everything that a C# interface can do except with a few more capabilities. We will start off by equating the common functionality of traits in Rust with C# interfaces, and then explore the additional capabilities we get with Rust traits.\nDescribes capabilities In C# interfaces, we can describe a set of function signatures which will tell the user the capabilities of that interface, such that they know when they call something what data to pass in, and what they should get back. The caller need not know how the function is implemented by a specific type, just that it does what is described on the tin.\nFor example, a C# interface which provides the capability to get an identifier through a GetIdentifier function which takes no arguments, and returns the ID of the instance as an integer can look as follows:\ninterface IGetIdentifier { int GetIdentifier(); } In Rust we can describe the exact same functionality. Note however a few changes in order to follow naming conventions or required by the language syntax:\n interface keyword is renamed to trait In Rust, the naming convention for a trait that has a single method is just the name of the method itself Rust has sized integer types so int is replaced with i32 (can be any of the other integer types too) Rust requires each member function to take explicitly the object we\u0026rsquo;re calling on. This allows us to tell the compiler/caller whether we\u0026rsquo;re taking an immutable reference, mutable reference, or taking ownership. In this case we only need an immutable reference to get an identifier Rust naming convention for functions and variables is snake_case. Therefore GetIdentifier method is renamed get_identifer.  The equivalent Rust definition is therefore:\ntrait GetIdentifier { fn get_identifier(\u0026amp;self) -\u0026gt; i32; } Generics In C# we are also able to make an interface generic. This is useful if we need to implement an interface multiple times for a given type for different situations. We can\u0026rsquo;t implement the same interface multiple times, but each different set of generic parameters is essentially a different interface.\nLet\u0026rsquo;s say we extend the above interface to work for any identifier type and not just integers. The C# snippet would be extended to look as follows:\ninterface IGetIdentifier\u0026lt;T\u0026gt; { T GetIdentifier(); } Likewise, the Rust snippet is extended with the exact same syntax:\ntrait GetIdentifier\u0026lt;T\u0026gt; { fn get_identifier(\u0026amp;self) -\u0026gt; T; } Bounded generics This is the ability to constrain generic parameter types to ones that only implement certain interfaces. In C# we have the ability to constrain based on interface or base class type, however in Rust we don\u0026rsquo;t have struct inheritance but do have trait inheritance. As a result, in Rust we can only constrain on interface. But this is good practice anyways as inheritance of behaviour is better than inheritance of state.\nIn C# if we want to bound the above interface to only allow T\u0026rsquo;s that implement an \u0026ldquo;IIdentifier\u0026rdquo; interface, that is an interface that allows the type to be used as an identifier, would look like this:\ninterface IGetIdentfier\u0026lt;T\u0026gt; where T : IIdentifier { T GetIdentifier(); } In Rust, we have two options for defining trait bounds, we can do it as above with a \u0026ldquo;where\u0026rdquo; syntax. This is useful when there are lots of bounds for a type and we can break them up over multiple lines which is more readable in most cases:\ntrait GetIdentifier\u0026lt;T\u0026gt; where T: Identifier { fn get_identifier(\u0026amp;self) -\u0026gt; T; } However we also have the ability to define these trait bounds inline, which for fewer trait bounds could be neater:\ntrait GetIdentifier\u0026lt;T: Identifier\u0026gt; { fn get_identifier(\u0026amp;self) -\u0026gt; T; } Default implementations Since C#8 we have the ability to provide default implementations for methods, to be used if the implementor doesn\u0026rsquo;t provide their own implementation. This lets us define some required methods that must be implemented, by not defining a default implementation, and provided methods (which can still be overridden, but don\u0026rsquo;t need to be) by giving a default.\nFor example in C#, if we assume that the definition of the \u0026ldquo;IIdentifier\u0026rdquo; interface is as follows:\ninterface Identifier { string GetString(); } Then we can add a provided method to the GetIdentifier interface which will print out the string identifier (note that we still require the GetIdentifier function to be implemented since there\u0026rsquo;s no possible way we can know how to get one). We get the resulting code:\ninterface IGetIdentifier\u0026lt;T\u0026gt; where T: IIdentifier { T GetIdentifier(); void PrintIdentifier() { System.Console.WriteLine($\u0026#34;{GetIdentifier().GetString()}\u0026#34;); } } We can do the same in Rust too in the same way as the following code snippet shows:\ntrait GetIdentifier\u0026lt;T: Identifier\u0026gt; { fn get_identifier(\u0026amp;self) -\u0026gt; T; fn print_identifier(\u0026amp;self) { println!(\u0026#34;{}\u0026#34;, self.get_identifier().get_string()); } } Implementing We can of course implement an interface or trait in both C# and Rust (what use would an interface be if we couldn\u0026rsquo;t!?). However the syntax is different, and this is where we start to see C# and Rust diverge quite dramatically.\nIn C# we have to provide the implementation of the interface at the same point as we define the implementing class/struct itself. For example, implementing the IIdentifier interface for a struct Foo:\nstruct Foo : IIdentifier { public int identifier; public string GetString() =\u0026gt; $\u0026#34;{identifier}\u0026#34;; } And implementing the IGetIdentifiertrait for a second struct FooGetter:\nstruct FooGetter : IGetIdentifier\u0026lt;Foo\u0026gt; { public Foo identifier; public Foo GetIdentifier() =\u0026gt; identifier; } In Rust however, the difference is that we define implementation blocks separate to the variables inside the struct itself. This allows us to break up behaviour/functions from the pure data contained in the structure. Each trait has its own implementation block so the implementation of the above structures and trait implementations will look as follows:\n#[derive(Clone)] struct Foo { pub i32 identifier; } impl Identifier for Foo { pub fn get_string(\u0026amp;self) -\u0026gt; String { format!(\u0026#34;{}\u0026#34;, self.identifier) } } struct FooGetter { pub Foo identifier; } impl GetIdentifier\u0026lt;Foo\u0026gt; for FooGetter { pub fn get_identifier(\u0026amp;self) -\u0026gt; Foo { self.identifier.clone() } } Note the Clone implementation required on Foo, and the clone function call in get_identifier. This is required because in Rust every type is movable, and clones are explicit. Adding the #[derive(Clone)] attribute to the struct allows us to automatically derive a deep clone implementation as long as all the struct fields implement Clone.\nNote that this is not idiomatic Rust, clones are rarely used as we can pass references around safely and the compiler will check the ownership and lifetime rules for us.\nIs that it? So you might look at the previous content and think that Rust traits are just C# interfaces with a different syntax. They can do everything C# interfaces can do right?. Well, yes, except that there are more capabilities that Rust gives us that just aren\u0026rsquo;t possible in C#\nThe following few sections then will only contain Rust snippets as there\u0026rsquo;s no valid way to represent them in C# (or C++, or most languages I\u0026rsquo;ve used - except Haskell, which makes traits similar to typeclasses).\nAssociated Types In the above code snippets, we had a generic trait GetIdentifier which was implemented on FooGetter with the generic parameter Foo. However this opens the way for us to have multiple implementations on FooGetter with different types. However what if we want to force the user to define a maximum of 1 implementation of GetIdentifier?\nWell, we have to remove the generics, and we end up with a trait as follows:\ntrait GetIdentifier { fn get_identifier(\u0026amp;self) -\u0026gt; ??? // what type goes here? } However, we have a problem here since we don\u0026rsquo;t know what the actual identifier type is now for any given implementation. We\u0026rsquo;ve removed the ability to specify it in a generic parameter, so in C# the only way to do this would be to fix the concrete type we return. That means all implementors of GetIdentifier returns the same type.\nTechnically in C# we can do it by returning the IIdenfier interface itself:\ninterface IGetIdentifier { IIdentifier GetIdentifier(); } Which indeed will allow each implementor to determine what the actual type they\u0026rsquo;re returning is, as long as it implements the IIdenfier interface.\nThe downside here is that we are forced to box the result which means a heap allocation, which means garbage collector tracking overhead.\nWe can do the same thing in Rust, we have to be explicit about returning a dynamic trait object in a box though:\ntrait GetIdentifier { fn get_identifier(\u0026amp;self) -\u0026gt; Box\u0026lt;dyn Identifier\u0026gt;; } However this still requires allocating heap storage and returning. An additional downside is type erasure. We\u0026rsquo;ve lost all information about the actual concrete type, all we know is the Box has an Identifier in it, so can only access the methods of the Identifier trait and nothing more.\nThere must be a better way!?. Enter associated types:\ntrait GetIdentifier { type IdentifierType: Identifier; fn get_identifier(\u0026amp;self) -\u0026gt; Self::IdentifierType; fn print_identifier(\u0026amp;self) { println!(\u0026#34;{}\u0026#34;, self.get_identifier().get_string()); } } Problems solved!. There\u0026rsquo;s a bit of new syntax here, but the main points are:\n We define an associated type on the trait with the type T syntax, allowing each implementor to specify a different concrete type We place a trait bound on it so that only concrete types implementing the Identifier trait can be used We return Self::IdentifierType from the get_identifier function. Importantly this is the concrete type, meaning no heap allocations and no type erasure  One further thing with associated types is that we can add trait bounds that force them to a specific concrete type. For example, say that we want to create a function that will take any GetIdentifier and print it, but only if the identifier type is an i32. We can do this as a trait bound with the following syntax:\nfn call_print_identifier_if_i32\u0026lt;T: GetIdentifier\u0026lt;IdentifierType = i32\u0026gt;\u0026gt;(t: \u0026amp;T) { t.print_identifier(); } We can also even place trait bounds on associated types within trait bounds!. For example if we want a function that will accept any GetIdentifier, but only if the IdentifierType implements Clone, we can do so:\nfn do_something\u0026lt;T\u0026gt;(t: \u0026amp;T) where T: GetIdentifier, \u0026lt;T as GetIdentifier\u0026gt;::IdentifierType: Clone { let c = t.get_identifier().clone(); // do something with cloned instance } Associated methods Unlike C# (and most other languages) interfaces which can only define methods tied to the specific instance of the implementing type, due to requiring dynamic dispatch and using a vtable, in Rust we can also define methods at the type level. These would be called static methods in other languages but in Rust, they are known as associated methods.\nA simple example of a trait making use of this functionality is the Default trait in the standard library. If we were to define it ourselves, we can do it like this:\ntrait Default { fn default() -\u0026gt; Self; } Self here is the implementing type, and notice how an associated method is indicated not by a keyword, but by the lack of self, \u0026amp;self, or \u0026amp;mut self in the first argument position.\nThese associated methods can be called with :: syntax, for example if our type Foo implements Default, we can call it as:\nlet default_foo = Foo::default(); Extension traits The next feature Rust provides us with respect to traits is as a side effect of having to implement them separate to the type we\u0026rsquo;re implementing on. This implies we can implement traits for types that we don\u0026rsquo;t actually own. This isn\u0026rsquo;t possible in C# or C++ where the implementation of a type is defined at the same time as the type itself.\nThis means that we are able to add functionality to existing types, even standard library types or primitive types by creating a trait and implementing it.\nFor example, say that we want to add a AsBytes trait which specifies that the type has a method called as_bytes which returns a vector of u8\u0026rsquo;s representing the bytes of the type.\nSuch a trait can be defined like:\ntrait AsBytes { fn as_bytes(\u0026amp;self) -\u0026gt; Vec\u0026lt;u8\u0026gt;; } And we can implement that for our own types, however, unlike in most other languages, we can implement this for existing types even primitives. For example on a u32:\nimpl AsBytes for u32 { fn as_bytes(\u0026amp;self) -\u0026gt; Vec\u0026lt;u8\u0026gt; { self.to_le_bytes().to_vec() } } to_le_bytes is a function that the standard library provides for us, that will give us an array of u8\u0026rsquo;s of length 4 with the bytes of the u32 in it. We can call to_vec on this to turn it into a dynamically sized vector instead to return.\nBlanket implementations The final feature for traits that we have with Rust is the ability to implement a trait for all types, optionally bounded with trait bounds.\nLet\u0026rsquo;s say we want to add a method to all iterators which will result in a new iterator that prints out the item (for all items that are displayable) as it iterates them.\nNote that we can do this with a map call to decorate a function, taking the input, printing it and returning it again to make a new iterator. This would look something like this:\nlet iter = (0..10).map(|elem| { println!(\u0026#34;{}\u0026#34;, elem); elem }); However this requires the user to roll the function themselves to print and return, and is a bit unwieldy. What we\u0026rsquo;d like is:\nlet iter = (0..).print(); We can do this with a blanket implementation that adds this print function to all iterators. First we need a structure to wrap the iterator that will step through and do the printing:\nstruct Print\u0026lt;T\u0026gt; { iterator: T } We will need to implement the Iterator trait here for the new structure, so that we can step over and print the elements. However we can only do this if the elements implement the Display trait. We can use trait bounds to ensure that:\n T is an Iterator The elements from T implement Display  This will look as follows:\nimpl\u0026lt;T: Iterator\u0026gt; Iterator for Print\u0026lt;T\u0026gt; where \u0026lt;T as Iterator\u0026gt;::Item: Display { type Item = \u0026lt;T as Iterator\u0026gt;::Item; // just passing the items through  fn next(\u0026amp;mut self) -\u0026gt; Option\u0026lt;Self::Item\u0026gt; { let item = self.iterator.next()?; println!(\u0026#34;{}\u0026#34;, item); Some(item) } } The little ? syntax when we call the next function of the iterator we\u0026rsquo;re wrapping is a little outside of the scope of the article, but the easiest way to think of it is if that call returns None, then we return None immediately. Otherwise item is set to the value inside the Some (which we then print and return).\nFinally we need to actually add the convenience function to all iterators. We can do this by creating an extension trait called IteratorPrint with the print function we want:\ntrait IteratorPrint where Self: Sized { fn print(self) -\u0026gt; Print\u0026lt;Self\u0026gt; { Print { iterator: self } } } We need to specify Self: Sized because traits in Rust can be implemented even for dynamically sized types which can\u0026rsquo;t exist on their own and must be boxed or put behind a reference.\nSince we need to put self into the Print structure, it will need to have a compile time known size, so we specify we can only use it with such types.\nNow we have all the boilerplate setup, we can actually do the blanket implementation. We\u0026rsquo;ll add it only to compatible iterators, otherwise we end up with a more cryptic compile error. The actual final code for this is as follows:\nimpl\u0026lt;T: Iterator\u0026gt; IteratorPrint for T where \u0026lt;T as Iterator\u0026gt;::Item: Display { } Then we can take any iterator that has elements that are displayable, and use this print function on it, even if we didn\u0026rsquo;t write the iterator type ourselves!. For example the following is valid:\n(0..10).print().for_each(|_| {}); The for_each call will just apply the closure to each element, in our case we just want to do nothing, but it will trigger the prints as it iterates.\n","permalink":"https://forgottenmaster.github.io/posts/rust/whyrust/traits/","summary":"What are traits? Traits in Rust can basically be thought of as interfaces in C#. They can do everything that a C# interface can do except with a few more capabilities. We will start off by equating the common functionality of traits in Rust with C# interfaces, and then explore the additional capabilities we get with Rust traits.\nDescribes capabilities In C# interfaces, we can describe a set of function signatures which will tell the user the capabilities of that interface, such that they know when they call something what data to pass in, and what they should get back.","title":"Traits"},{"content":"Move By Default In most programming languages, there is no one true \u0026ldquo;owner\u0026rdquo; of any given piece of data. Data can be allocated on the stack or the heap but references to it can be passed around as needed. There isn\u0026rsquo;t a way of the compiler to track ownership of a piece of data in most languages such as C++.\nAdditionally expensive copies of structures may be done without us realising, or being able to opt out of it. For example in C++ and C# the following code will result in making a copy of SomeLargeStruct\nSomeExpensiveStruct s1; SomeExpensiveStruct s2 = s1; // this will make a copy of s1  In Rust, each piece of data has a single owner at any one time. If the owning binding or function goes out of scope, the data is dropped and this gives an opportunity for the type to do any resources allocated. This is similar to a destructor in C++. Due to the single ownership model, Rust moves data by default on assigment, or when passing into and out of functions.\nAn example of the flow of ownership in Rust is the following\nlet v1 = vec![1, 2, 3]; // v1 is a binding that owns a vector with the integers 1, 2, and 3 let v2 = v1; // v1 is *moved* to binding v2. v2 is now the owner of the vector, v1 has relinquished control println!(\u0026#34;{}\u0026#34;, v1); // here we are trying to print out v1, but v1 isn\u0026#39;t the owner of any data and so can\u0026#39;t give out references, etc.  In the above code snippet, the binding v1 transfers ownership of the data to the binding v2, trying to use v1 afterwards results in the following compiler output\nerror[E0382]: use of moved value: `v1`\r--\u0026gt; src/main.rs:4:10\r|\r2 | let v1 = vec![1, 2, 3];\r| -- move occurs because `v1` has type `Vec\u0026lt;i32\u0026gt;`, which does not implement the `Copy` trait\r3 | let v2 = v1;\r| -- value moved here\r4 | dbg!(v1);\r| ^^ value used here after move\r Rust won\u0026rsquo;t let us compile a program which violates the ownership rules like this.\nThe above example is of a move that occurs when assigning from one binding to another, but the same will happen if we passed into a function, for example\nfn foo(v: Vec\u0026lt;i32\u0026gt;) { // v is a vector of integers, ownership is transferred into the function from outside  // the function is the owner of the data, so at the end of the function, the data is dropped } fn main() { let v = vec![1, 2, 3]; foo(v); // ownership is transferred into the function here  dbg!(v); } Will result in a similar compiler error as before:\nerror[E0382]: use of moved value: `v`\r--\u0026gt; src/main.rs:7:10\r|\r5 | let v = vec![1, 2, 3];\r| - move occurs because `v` has type `Vec\u0026lt;i32\u0026gt;`, which does not implement the `Copy` trait\r6 | foo(v);\r| - value moved here\r7 | dbg!(v);\r| ^ value used here after move\r Use After Free In languages such as C++, there are no mechanisms which prevent a use after free error. For example the following code is completely valid\nstruct SomeStruct { public: const char* name; }; SomeStruct\u0026amp; allocAndReturn() { SomeStruct s; s.name = \u0026#34;Hello, World!\u0026#34;; return s; } int main() { SomeStruct\u0026amp; s = allocAndReturn(); std::cout \u0026lt;\u0026lt; s.name \u0026lt;\u0026lt; std::endl; // Boom!, we\u0026#39;ve used an invalid reference  return 0; } As the C++ compiler doesn\u0026rsquo;t track ownership or lifetimes, it doesn\u0026rsquo;t prevent the function from returning a reference to an object that then goes out of scope, then when it tries to access s.name the program segfaults.\nContrast this in Rust, where the compiler tracks lifetimes, it is able to detect a use after free and will fail to compile. If we attempt to return a reference to an object created in a function as in the above program, we won\u0026rsquo;t be able to compile. As it\u0026rsquo;s difficult to do the above function in Rust without delving into lifetime annotations, a smaller program demonstrates the same issue\nfn main() { let v = { let v = vec![4, 5, 6]; \u0026amp;v }; dbg!(v); } As Rust is an expression based language, we can create an arbitrary scope or code block that ends in an expression, and use that to assign to a binding, or anywhere else an expression is accepted.\nIn the code snippet, we are creating a vector v inside the block, and then trying to return a reference to it, out of the block and bind it to the outer v binding.\nHowever Rust detects that we are trying to keep a reference for longer than the object is in scope and gives the following compiler error\nerror[E0597]: `v` does not live long enough\r--\u0026gt; src/main.rs:4:9\r|\r2 | let v = {\r| - borrow later stored here\r3 | let v = vec![4, 5, 6];\r4 | \u0026amp;v\r| ^^ borrowed value does not live long enough\r5 | };\r| - `v` dropped here while still borrowed\r This is the Rust\u0026rsquo;s borrow checker and lifetime rules at work.\nModification While Reading In other languages, it\u0026rsquo;s common for a mutable and immutable reference to exist at the same time. For example in C++ we may do the following:\nSomeStruct s; const SomeStruct \u0026amp;s1 = s; SomeStruct\u0026amp; s2 = s; The implications of this is that someone that is holding an immutable reference to an object will observe changes made to the object (through a mutable reference). This is contrary to what the definition should be of an immutable reference, the holder shouldn\u0026rsquo;t see any changes as it\u0026rsquo;s immutable.\nIf we attempt to do this in Rust, it will fail to compile as we cannot take a mutable and immutable reference at the same time (though we may take as many immutable references as we\u0026rsquo;d like since none will observe any changes).\nfn main() { let mut v = vec![1, 2, 3]; let v1 = \u0026amp;v; let v2 = \u0026amp;mut v; dbg!(v1); } We are greeted with the following compile error\nerror[E0502]: cannot borrow `v` as mutable because it is also borrowed as immutable\r--\u0026gt; src/main.rs:4:14\r|\r3 | let v1 = \u0026amp;v;\r| -- immutable borrow occurs here\r4 | let v2 = \u0026amp;mut v;\r| ^^^^^^ mutable borrow occurs here\r5 | dbg!(v1);\r| -- immutable borrow later used here\r Rust guarantees memory safety here by allowing either 0 or more immutable references OR 0 or 1 mutable reference. It is forbidden to have a mutable reference while ANY references are taken, and only 1 mutable reference can be taken at a time.\nThis means if you hold a mutable reference to an object, the compiler guarantees that only you can make changes to the object. It also guarantees that if you hold an immutable reference you will never see any state changes through that.\nThis isn\u0026rsquo;t strictly true\u0026hellip;.there are types in the standard library that allow for interior mutability, that is, mutating through an immutable reference. However, they are also perfectly safe to use because they perform the check at runtime instead. When using those types, you are only allowed to mutate a value if nothing else currently holds a borrow to it.\nIn conclusion, most programming languages have no real protections against multiple sources writing to an object, even when other sources have read-only access. Rust has a policy of only 1 writer at a time, and only when there are no readers in existence.\nThis does require some code restructuring to separate the reading from the writing, but results in much safer code and prevents common problems such as modifying a collection while iterating over it, which is impossible in Rust due to the borrow checker.\nThread Safety The last topic on memory safety in Rust will be a few words on how the safety rules also apply in a multithreaded context.\nMost languages don\u0026rsquo;t have any guarantees about whether an object is safe to be accessed from multiple threads at a time, and if an object is not safe, but is accessed concurrently from 2 threads, this is called a data race, and is not a good thing.\nRust provides a system by which the compiler will fail to compile a program if we try to use a non thread-safe type in a multithreaded context.\nThe way it achieves this is by using two core traits called Send and Sync\nThese are implemented (or not implemented) by types automatically based on the data inside the types. If all the members of a custom type are Send, or Sync then the type itself is Send or Sync.\nSend means that instances of this type can be transferred over to a different thread. This is transfer of ownership across a thread boundary.\nSync means that instances of this type can be accessed from multiple threads at the same time, that is references to the instance are Send.\nThe developer can implement Send and Sync manually on types if those types aren\u0026rsquo;t automatically deemed Send/Sync, however to do so is an unsafe operation as it requires that the developer has checked that the type is indeed thread-safe.\n","permalink":"https://forgottenmaster.github.io/posts/rust/whyrust/memorysafety/","summary":"Move By Default In most programming languages, there is no one true \u0026ldquo;owner\u0026rdquo; of any given piece of data. Data can be allocated on the stack or the heap but references to it can be passed around as needed. There isn\u0026rsquo;t a way of the compiler to track ownership of a piece of data in most languages such as C++.\nAdditionally expensive copies of structures may be done without us realising, or being able to opt out of it.","title":"Memory Safety"},{"content":"The final piece of the puzzle in the mathematical foundations needed is to determine how we can trace back a derivative with respect to matrix inputs.\nIn the previous post we saw that calculating the derivative of two vectors of equal length when passing through the dot product operation results in the transpose of the other.\nMatrix Multiplication Refresher The first step is to recall how we can multiply two matrices together. In deep learning typically this will be performed with a matrix of samples, and a matrix of weights to be applied. The result being a weighted sum of the samples with weights.\nIn order for the product of two matrices to be defined, the number of columns in the first matrix must equal the number of rows in the second. The resulting matrix will be of the outer dimensions.\nThat is, given a matrix X, of dimensions m x n and a matrix W, of dimensions n x p then the multiplication is defined, and the resulting matrix is of dimensions m x p.\nIn the resulting matrix, an element at row i and column j is the result of the dot product of the ith row of the first matrix, and the jth row of the second matrix.\nAs a concrete example, suppose we have\n$$ X = \\begin{bmatrix} X_{11} \u0026amp; X_{12} \u0026amp; X_{13} \\\\ X_{21} \u0026amp; X_{22} \u0026amp; X_{23} \\\\ X_{31} \u0026amp; X_{32} \u0026amp; X_{33} \\end{bmatrix} $$ $$ W = \\begin{bmatrix} W_{11} \u0026amp; W_{12} \\\\ W_{21} \u0026amp; W_{22} \\\\ W_{31} \u0026amp; W_{32} \\end{bmatrix} $$\nThen the resulting matrix can be represented as\n$$ X \\cdot W = \\begin{bmatrix} XW_{11} \u0026amp; XW_{12} \\\\ XW_{21} \u0026amp; XW_{22} \\\\ XW_{31} \u0026amp; XW_{32} \\end{bmatrix} $$\nWhere \\(XW_{ij}\\) is the dot product of the ith row of X and the jth row of W\nDerivative Of Matrices In order for us to be able to calculate the rate of change in the output of a function with respect to the elements of one of the input matrices we will need to have the output be either:\n A matrix of the same size as an input matrix - derivative can be calculated elementwise in that case between input and output matrices A scalar value - derivative can be calculated elementwise for each element in the input matrix to determine how this affects the single output  For deep learning, we want to be able to first multiply a matrix of samples (X) by a matrix of weights (W) in order to get the weighted sum of the records in a resulting matrix, then we would like to pass the resulting matrix to a function that produces a single value that could be used to activate a neuron for example.\nTherefore what we have is a function F which can be defined as\n$$ F = F(Y) $$\nThat is it takes some matrix Y and performs an operation on it (in our case likely summing the elements). The matrix Y will be produced by multiplying the two input matrices, that is:\n$$ Y = X \\cdot W $$\nIn order to calculate the total derivative with respect to the input matrices, we can apply the chain rule. The first step of which is easy, that is to take Y and calculate how a change in each element would affect the resulting single value. The derivative of F therefore is a matrix representing these partial derivatives. We will call this G, that is \\(G = F'(Y)\\)\nThe derivative of Y depends on two variables X and W which are multiplied together. The formula for the total derivative therefore is\n$$ dY = dX \\cdot W + X \\cdot dW $$\nThat is, changes in the elements of X would end up being multiplied by W, and also changes in the elements of W would end up being multiplied by X.\nThen, the derivative of the total function can be given (using the chain rule) as:\n$$ dF = G:dY $$\nAnd expanding out dY with the above formula gives\n$$ dF = G:dX \\cdot W + G:X \\cdot dW $$\nFor notation, X:Y means the elementwise multiplication of matrices X and Y, that must be the same dimensions (also known as frobenius inner product).\nIsolating dX and dY In the above formula, we have G being multiplied elementwise by a matrix that is the result of matrix multiplication between dX and W or X and dW.\nWhat we want to do is to isolate dX and dY so that they are multiplied elementwise with the other matrix. How can we get W and X onto the other side?\nWell,\n$$ G : (dX \\cdot W) = (G \\cdot W^T) : dX $$\nAnd\n$$ G : (X \\cdot dW) = (X^T \\cdot G) : dW $$\nThat is, the total derivative can be calculated as\n$$ dF = (G \\cdot W^T) : dX + (X^T \\cdot G) : dW $$\nPartial Derivatives Now from the above formula, we can easily calculate the partial derivatives of F by holding either X or W as constant, meaning dX or dW respectively is 0.\nDoing this will eliminate the term from the formula leaving only the other one. This shows us how the partial derivatives are\n$$ \\frac{\\partial F}{\\partial X} = G \\cdot W^T $$ $$ \\frac{\\partial F}{\\partial W} = X^T \\cdot W $$\nAdditional Notes The book at this point actually didn\u0026rsquo;t explain how we get to using the transpose matrices for derivatives, instead the book says in a handwavey fashion that \u0026ldquo;the way the mathematics works out\u0026rdquo;.\nI found the above explanation at This Page and above tried to break it down as best I could.\nI will likely need to revisit matrix multiplication in the future but for now understand it enough to move on to learning more about the structure of a neural network and applying these rules in practice.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/matrixinputs/","summary":"The final piece of the puzzle in the mathematical foundations needed is to determine how we can trace back a derivative with respect to matrix inputs.\nIn the previous post we saw that calculating the derivative of two vectors of equal length when passing through the dot product operation results in the transpose of the other.\nMatrix Multiplication Refresher The first step is to recall how we can multiply two matrices together.","title":"Matrix Inputs"},{"content":"After learning what a derivative of a function is, and how to apply the chain rule to a composite function, we then learned how to calculate the derivative of a function with multiple inputs by tracing back through the chain of functions following the route of one of the parameters while holding the others constant.\nThe next step in the prerequisite mathematics we need to build the foundations of a neural network is to determine what the derivative of a function means when one or more of the inputs is a vector.\nGiven the following block diagram for a chain of functions\nThat is, a function, f which takes two inputs both of which are vectors (denoted by the bar above the name), and a function, g which operates on the resulting value.\nFurthermore, let\u0026rsquo;s say that f is actually the dot product function. This function is written as follows, if we assume that X and W are both vectors of length 3:\n$$ f(X, W) = X \\cdot W = X_1 \\times W_1 + X_2 \\times W_2 + X_3 \\times W_3 $$\nAs a reminder, the derivative of a function is the ratio of the change in the output given a change in the input, at a specific input value (point on the graph of the function). This is easy to determine when the inputs and outputs are plain values as it\u0026rsquo;s just a ratio of the two deltas.\nWhen an input is a vector though, what does a change in the input mean?. In fact, this is easily interpreted as a change in one of the components of the input.\nAs seen in the dot product formula (though the function can be any function on vectors), a vector can be decomposed into a list of individual numbers/components. Therefore we can calculate the derivative of the function with respect to an individual component by holding the others to be constant and varying the one we\u0026rsquo;re interested in.\nAfter applying this first function though, the second is operating on a single value and so the chain rule applies as before. In the case of the dot product, this gives us 6 partial derivatives: 3 with respect to the 3 components in the input vector X and 3 with respect to the components of W.\nThe formulas for calculating the derivatives then for the composite function will be as follows in accordance with the chain rule (where n is a valid index for the input vector):\n$$ (g \\circ f)_{X_n} = f_{X_n}(X, W) \\times g'(f(X, W)) $$ $$ (g \\circ f)_{W_n} = f_{W_n}(X, W) \\times g'(f(X, W)) $$\nLooking back at the formula for calculating the dot product we can see that holding all values constant except for a single component we are interested in causes a change in the output proportional to the matching component of the other vector. For example if we let \\(X_1\\) increase by 1 while holding all other components constant, we can see that the only part of the formula containing \\(X_1\\) is\n$$ X_1 \\times W_1 $$\nIncreasing \\(X_1\\) by 1 then increases the output only by \\(W_1\\)\nWhen we calculate the derivative for the other components then, we find out that elementwise, the derivatives of the dot product with respect to X can be represented by:\n$$ \\begin{bmatrix} W_1 \u0026amp; W_2 \u0026amp; W_3 \\end{bmatrix} $$\nand by the same token, the component wise derivatives of the dot product with respect to W is:\n$$ \\begin{bmatrix} X_1 \u0026amp; X_2 \u0026amp; X_3 \\end{bmatrix} $$\nWhen we write the dot product as row and column vectors in order to have the correct shapes for performing the dot product:\n$$ \\begin{bmatrix} X_1 \u0026amp; X_2 \u0026amp; X_3 \\end{bmatrix} \\cdot \\begin{bmatrix} W_1 \\\\ W_2 \\\\ W_3 \\end{bmatrix} $$\nWe can see that the component wise derivative with respect to X will be \\( \\begin{bmatrix} W_1 \u0026amp; W_2 \u0026amp; W_3 \\end{bmatrix} \\), that is \\(W^T\\) and the component wise derivative with respect to W will be \\( \\begin{bmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{bmatrix} \\), that is \\(X^T\\)\nConclusion In conclusion, calculating the derivative of a function with vector inputs is just calculating the derivative of the function with respect to each component in the vector. Additionally, and most importantly for deep learning, the derivative of the dot product of a row vector X with a column vector W turns out to be \\(W^T\\) and \\(X^T\\) respectively.\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/vectorinputs/","summary":"After learning what a derivative of a function is, and how to apply the chain rule to a composite function, we then learned how to calculate the derivative of a function with multiple inputs by tracing back through the chain of functions following the route of one of the parameters while holding the others constant.\nThe next step in the prerequisite mathematics we need to build the foundations of a neural network is to determine what the derivative of a function means when one or more of the inputs is a vector.","title":"Vector Inputs"},{"content":"So far, we\u0026rsquo;ve learned how to chain functions together, and how to calculate the derivative of a function at a specific input value. We\u0026rsquo;ve also learned how to apply the chain rule when we are chaining multiple functions together. However, how do we calculate the derivative of a function, or chain of functions when a function has multiple inputs?.\nAs it turns out, we can calculate the derivative in the same way!, we just need to, in the case where there are multiple inputs, calculate the partial derivative with respect to the given parameter.\nLets start off with a single function, one that takes two inputs and returns a single output\nIn such a function, the change in the output can be affected by a change in either input (x or y). Because of this, calculating the derivative with respect to x at a given value might result in a different value to what is given when calculating the derivative for input y. These are the partial derivatives of the function, where each partial derivative is how the output changes with a given change of the input (while keeping the other inputs constant).\nThe formulas for these partial derivatives can be given as:\n$$ \\frac {\\partial f} {\\partial x} = \\lim_{\\Delta \\to 0} \\frac {f(x + \\Delta, y) - f(x - \\Delta, y)} {2 \\times \\Delta} $$ $$ \\frac {\\partial f} {\\partial y} = \\lim_{\\Delta \\to 0} \\frac {f(x, y + \\Delta) - f(x, y - \\Delta)} {2 \\times \\Delta} $$\nApplying the chain rule We can apply the chain rule in the same way when calculating the partial derivative of a composite function with respect to one of the inputs. Since the change in input causes a change in the input to the next function, which then causes a change in the input to the next, and so on, we will still do a backward pass the same way as we would with a single input. We just follow it back to the point where the input is given. For example with the following\nWe can see that the first function has two inputs whereas g and h have only 1 input. So there will be two partial derivatives for this composite function. These can be given as follows (applying the chain rule):\n$$ (f \\circ g \\circ h)_x = f_x(x, y) \\times g'(f(x, y)) \\times h'(g(f(x, y))) $$ $$ (f \\circ g \\circ h)_y = f_y(x, y) \\times g'(f(x, y)) \\times h'(g(f(x, y))) $$\nNote that here we are using Legrange\u0026rsquo;s Notation as it can be cleaner than the Leibniz notation. In this notation we can use \\(f'(x)\\) to denote the derivative of function f taking a single input, at value x.\nWith multiple inputs, the input we\u0026rsquo;re describing the partial derivative of can be given as a subscript. e.g. \\(f_x(x, y)\\) would be saying we are interested in how much a change in x will cause a change in the output of the function, at the point defined by the given values of x and y.\nOverall the chain rule works the same way as a single input, except we are defining the input we\u0026rsquo;re interested in when back propagating to work out the derivative.\nLet\u0026rsquo;s assume the following functions to calculate the derivative of a given 2 input function with respect to each of the inputs:\nfn derivative_x(f: impl Fn(f64, f64) -\u0026gt; f64, x: f64, y: f64, delta: f64) -\u0026gt; f64 { (f(x + delta, y) - f(x - delta, y)) / (2.0 * delta) } fn derivative_y(f: impl Fn(f64, f64) -\u0026gt; f64, x: f64, y: f64, delta: f64) -\u0026gt; f64 { (f(x, y + delta) - f(x, y - delta)) / (2.0 * delta) } Along with the existence of the previous function, \u0026ldquo;derivative\u0026rdquo; which performs the same calculation but on a single input function. We can then calculate the derivatives of the composite function as shown above with the following functions:\nfn derivative_x_chain(f: impl Fn(f64, f64) -\u0026gt; f64, g: impl Fn(f64) -\u0026gt; f64, h: impl Fn(f64) -\u0026gt; f64, x: f64, y: f64, delta: f64) -\u0026gt; f64 { // forward pass to calculate inputs  let f_of_xy = f(x, y); let g_of_f_of_xy = g(f_of_xy); // backward pass to calculate partial derivatives  let derivative_f = derivative_x(f, x, y, delta); let derivative_g = derivative(g, f_of_xy, delta); let derivative_h = derivative(h, g_of_f_of_xy, delta); // total derivative with respect to x of compound function  derivative_f * derivative_g * derivative_h } fn derivative_y_chain(f: impl Fn(f64, f64) -\u0026gt; f64, g: impl Fn(f64) -\u0026gt; f64, h: impl Fn(f64) -\u0026gt; f64, x: f64, y: f64, delta: f64) -\u0026gt; f64 { // forward pass to calculate inputs  let f_of_xy = f(x, y); let g_of_f_of_xy = g(f_of_xy); // backward pass to calculate partial derivatives  let derivative_f = derivative_y(f, x, y, delta); let derivative_g = derivative(g, f_of_xy, delta); let derivative_h = derivative(h, g_of_f_of_xy, delta); // total derivative with respect to x of compound function  derivative_f * derivative_g * derivative_h } For a runnable example of comparing calculating the derivative of a composite function both directly and via the chain rule, I have written a simple example on the Rust Playground which shows that the derivative calculated for a composite function directly results in the same value as a derivative calculated using the chain rule (both single and multiple input functions are tested).\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/multipleinputs/","summary":"So far, we\u0026rsquo;ve learned how to chain functions together, and how to calculate the derivative of a function at a specific input value. We\u0026rsquo;ve also learned how to apply the chain rule when we are chaining multiple functions together. However, how do we calculate the derivative of a function, or chain of functions when a function has multiple inputs?.\nAs it turns out, we can calculate the derivative in the same way!","title":"Multiple Inputs"},{"content":"C Style Enums In C#, C++, and a lot of other popular programming languages, we have access to a type called an \u0026ldquo;enumeration\u0026rdquo; (or enum for short). This is simply a type safe collection of named constant values.\nFor example in C++, making an enum whose variants represent a set of allowed colors for a hypothetical UI framework could be written as (with the values of the variants explicitly typed out for transparency):\nenum Color { Red = 1, Green = 2, Blue = 3 }; A function can then go ahead and accept a \u0026ldquo;Color\u0026rdquo; and the user will be able to pass only the named colors. Except that in C++, this is not true. Nothing stops the caller from casting an arbitrary integer as a \u0026ldquo;Color\u0026rdquo;. For example calling a SetColor function that takes a Color, the caller can do:\nwidget.SetColor(static_cast\u0026lt;Color\u0026gt;(10)); // what even is color with value 10???. It hasn\u0026#39;t been defined so likely won\u0026#39;t be correctly handled.  This is not desired as SetColor can\u0026rsquo;t assume that the given Color is only one that was specified in the enumeration. If someone can arbitrarily cast an integer to Color, what is the function supposed to do with it?.\nIn Rust, we can have value type enums the same way:\nenum Color { Red = 1, Green = 2, Blue = 3 } And casting such an enum value into an integer is totally fine, the following snippet will print \u0026ldquo;Selected color is: 3\u0026rdquo;:\n// enum variant to integral value is safely supported because it\u0026#39;s a total function - all enum variants in this style of enumeration // can be cast to the respective integer value. println!(\u0026#34;Selected color is: {}\u0026#34;, Color::Blue as u8); This is totally fine and allowed by Rust because all variants of this C-style enumeration can be casted safely to an integer. We say it\u0026rsquo;s infallible\nHowever, casting an integer to an enumeration type is not infallible because not all possible integral values have a variant in the enumeration, we can\u0026rsquo;t do the following - it simply does not compile:\n// does not compile as integral to enum conversion is not implemented since it can fail for certain values of integer. let int_as_color = 10 as Blue; Rust is a safe and cautious language and the compiler will just not allow operations that could fail, therefore converting from enum to integral isn\u0026rsquo;t supported by default, however the enum creator can implement the TryFromtrait for any integral types.\nHowever this is boilerplate that\u0026rsquo;s already been done and available in a crate. Therefore to allow for C-style enums in Rust with safe conversions in both directions, it\u0026rsquo;s best to use https://crates.io/crates/num_enum\nTuple Enums In C++, the above is all you get, loosely typed integers that aren\u0026rsquo;t even that safe. With Rust, enums become more powerful with the ability to store different data inside of each variant.\nThese are similar to an algebraic data type such as Haskell has. It could be thought of similar to a union in C++ in that an element of the enum type takes up the amount of space required for the biggest variant (allowing it to store in an array), but strongly typed so you can\u0026rsquo;t access data you shouldn\u0026rsquo;t.\nAs an example, suppose we want to have an \u0026ldquo;Angle\u0026rdquo; enumeration. An angle could be stored in either Degrees or Radians, but both are stored as floats. In this case the tuple enum would look as follows:\nenum Angle { Degrees(f32), Radians(f32) } In both of these variants, we store an f32, but behind the scenes each instance of Angle is tagged with its discriminant (Degrees or Radians) and the only way to access the data inside is through pattern matching. This means we literally can\u0026rsquo;t access data that we shouldn\u0026rsquo;t for the variant we have. An example of implementing the Intotrait for this would be:\nimpl Into\u0026lt;f32\u0026gt; for Angle { fn into(self) -\u0026gt; f32 { match self { Self::Degrees(val) =\u0026gt; val, Self::Radians(val) =\u0026gt; val } } } An example of an enumeration with differing types could be a Color, where we can choose between different color formats:\nenum Color { RGBF32(f32, f32, f32), RGBAF32(f32, f32, f32, f32), RGBU8(u8, u8, u8), RGBAU8(u8, u8, u8, u8) } That is, we can choose between colors with RGB or RGBA components, and can choose the type of the components we\u0026rsquo;re storing. However because this is essentially a strongly typed union, and Rust requires all types to have a defined size to be stored, the largest size would still be picked, in this case each Color would be 16 bytes large (corresponding to the size of RGBAF32 which is largest).\nPattern matching works the exact same way. For example, a method on Color which can return whether the color supports transparency could be written as follows:\nimpl Color { fn supports_transparency(\u0026amp;self) -\u0026gt; bool { match self { Self::RGBF32(..) =\u0026gt; false, Self::RGBAF32(..) =\u0026gt; true, Self::RGBU8(..) =\u0026gt; false, Self::RGBAU8(..) =\u0026gt; true } } } Named Field Enums A variant in an enum can use the tuple syntax for defining the types it contains, and pattern matching as described above, however we can also store values associated with an enum variant by name in a record/struct like syntax. We can freely mix and match these on a per-variant basis. For example an enumeration which represents an Error. We might support storing an ErrorMessage, ErrorCode, or both. This might look as follows:\nenum Error { Message(String), Code(i32), Both { message: String, code: i32 } } When pattern matching on tuple types, we need to use the tuple patterns. When matching on record types, we need to use that syntax. An example of a function to try to get an error code from an Error would be:\nimpl Error { fn try_get_code(\u0026amp;self) -\u0026gt; Option\u0026lt;i32\u0026gt; { match self { Self::Message(..) =\u0026gt; None, Self::Code(c) =\u0026gt; Some(*c), Self::Both{code: c, ..} =\u0026gt; Some(*c) } } } Empty Enums!? Empty enums can\u0026rsquo;t be constructed. This may sound kind of pointless, what does\nenum Void { } Even mean if it can\u0026rsquo;t be constructed?\nAs it turns out this can be very useful for statically proving that we can\u0026rsquo;t ever take a particular branch of code in some cases, and is sometimes seen in generic code.\nFor example, in Rust we have the Result type which has two type parameters. One is the success type, and one is the error type. Say that a trait requires a return type of Result\u0026lt;SuccessType, ErrorType\u0026gt; from a function\ntrait TryOperation { type SuccessType; type ErrorType; fn try_operation(\u0026amp;mut self) -\u0026gt; Result\u0026lt;Self::SuccessType, Self::ErrorType\u0026gt; } In order to implement such a trait, we must provide an ErrorType to satisfy the signature of the trait, but for an infallible operation which is guaranteed to not fail, what do we choose for an ErrorType?. Any type is as good as any other type if we never need to construct it and we never return it:\nstruct InfallibleOperation { } impl TryOperation for InfallibleOperation { type SuccessType = (); // Unit type is like \u0026#34;void\u0026#34; in other languages, it\u0026#39;s a type we can use when we don\u0026#39;t need to return any information.  type ErrorType = ???; // What do we put here if this operation never fails?  fn try_operation(\u0026amp;mut self) -\u0026gt; Result\u0026lt;Self::SuccessType, Self::ErrorType\u0026gt; { println!(\u0026#34;Hello, World!\u0026#34;); } } The implementation of this function simply prints to the console and never fails, we\u0026rsquo;re \u0026ldquo;trying\u0026rdquo; to perform the operation but it will always succeed. It turns out in these cases we can communicate this at the type level to the caller by using the empty enum as the ErrorType (e.g. Void). If the caller sees that the signature for this is:\nfn try_operation(\u0026amp;mut self) -\u0026gt; Result\u0026lt;(), Void\u0026gt; Then they can easily see that the error case can never possibly happen (because an instance of Void physically cannot be constructed). In this case, the caller knows it\u0026rsquo;s safe to not even handle the possibility of error as this is ensured by the compiler in the types.\n","permalink":"https://forgottenmaster.github.io/posts/rust/whyrust/enums/","summary":"C Style Enums In C#, C++, and a lot of other popular programming languages, we have access to a type called an \u0026ldquo;enumeration\u0026rdquo; (or enum for short). This is simply a type safe collection of named constant values.\nFor example in C++, making an enum whose variants represent a set of allowed colors for a hypothetical UI framework could be written as (with the values of the variants explicitly typed out for transparency):","title":"Enums"},{"content":"The chain rule allows us to determine the derivative of a composite function as a product of the derivatives of each of the individual functions.\nThe reason this is a product is because, say you had a function \u0026ldquo;f\u0026rdquo; which has a derivative of 3 at a particular value of x. That means, when you increase the input by 1, the output increases by 3.\nThen say you have a second function \u0026ldquo;g\u0026rdquo; which has a derivative of 5 at the value output by \u0026ldquo;f\u0026rdquo; for x. This again means that when you increase the input by 1, then the output increases proportionally, this time by 5.\nHowever, the compound function \\( g \\circ f \\) will chain these two together in such a way that increasing the input to f increases the output of f by 3, and because they were chained, this is the same as increasing the input to g by 3.\nSince the gradient of g is 5 meaning that the output change is 5 times the change of the input, then raising the input to that function by 3 must result in a raise of the output by 15.\nSince derivatives are calculated as the gradient at a particular value, we need to know the inputs and outputs of both functions in order to calculate the partial derivatives.\nOnce we have these, we can just multiply them together.\nThe formula for the chain rule in this situation will be:\n$$ (g \\circ f)'(x) = g'(f(x)) \\times f'(x) $$\nThat is, calculating the rate of change (gradient) of the initial function f at the given value of x. After this, calculating the gradient of the second function, g, at the value output by f.\nTherefore in order to calculate these compound derivatives, we need to run two passes.\nThe first pass, called the forward pass, is where we simply are obtaining the input and output values for all functions involved. The second pass, called the backward pass, is where we use the inputs and outputs of the functions in order to calculate their gradients at that point. The final step is then just to multiply together to get the gradient of the composite function.\nThis two-pass sequence can be visualised with the following diagram. In it, we have three functions and an x value that\u0026rsquo;s passed in. we run the x value through the functions in order to get \\( f(x) \\), then \\( g(f(x)) \\), and finally \\( h(g(f(x))) \\).\nThe dashed lines represents the backward pass where we take the calculated values to determine the derivatives of f, g, and h at the appropriate input values.\n We can code up the calculation of the derivative of such a chain of functions as displayed in the diagram. For this section of code, we\u0026rsquo;ll assume the existence of a helper function called \u0026ldquo;derivative\u0026rdquo; which, given a function and a value at which the calculate the derivative, will approximate the derivative using the logic described in the previous post.\n// forward pass, calculating all inputs/outputs let x = 42.0; // initial input let f_of_x = f(x); // output from f (also input to g) let g_of_f_of_x = g(f_of_x); // output from g (also input to h) let h_of_g_of_f_of_x = h(g_of_f_of_x); output from h which is the final output // backward pass, calculating the derivatives of the functions at the calculated inputs let deriv_h = derivative(h, g_of_f_of_x); // calculate the derivative of h at the value that was output from g let deriv_g = derivative(g, f_of_x); // calculate derivative of g at the value that was output from f let deriv_f = derivative(f, x); // calculate derivative of f at the value of x  // multiply together to get the final rate of change of the output with change to x deriv_h * deriv_g * deriv_f ","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/chainrule/","summary":"The chain rule allows us to determine the derivative of a composite function as a product of the derivatives of each of the individual functions.\nThe reason this is a product is because, say you had a function \u0026ldquo;f\u0026rdquo; which has a derivative of 3 at a particular value of x. That means, when you increase the input by 1, the output increases by 3.\nThen say you have a second function \u0026ldquo;g\u0026rdquo; which has a derivative of 5 at the value output by \u0026ldquo;f\u0026rdquo; for x.","title":"Chain Rule"},{"content":"Maths The derivative of a function is the rate at which the output changes with respect to a change in the input at a specific value for the input. This last part is important as the derivative of a function is essentially the gradient, or tangent of the graph of that function at a specific point which can of course change depending on where you are on the number line.\nThe formula for calculating the derivative of a given function \u0026ldquo;f\u0026rdquo; with respect to it\u0026rsquo;s input parameter \u0026ldquo;x\u0026rdquo;, at a given value of x which we call \u0026ldquo;a\u0026rdquo; here, can be written as:\n$$ \\frac{\\partial f}{\\partial x}(a) = \\lim_{\\Delta \\to 0} \\frac{f(a + \\Delta) - f(a - \\Delta)}{2 \\times \\Delta} $$\nLet\u0026rsquo;s break this down a little.\nWe\u0026rsquo;ll take the left hand side of the formula first, that is \\( \\frac{\\partial f}{\\partial x}(a) \\) which is the partial derivative of f with respect to x, at the given value of a. If we wanted to take another partial derivative of f, for example if the function took multiple parameters, the parameter we\u0026rsquo;re taking the partial derivative with respect to will appear as the denominator. For example, in a function such as:\n$$ f(x, y, z) = x + y + z $$\nThen the partial derivatives could be written as: \\( \\frac{\\partial f}{\\partial x} \\), \\( \\frac{\\partial f}{\\partial y} \\), and \\( \\frac{\\partial f}{\\partial z} \\)\nWhat\u0026rsquo;s a limit?\nThe first symbol in the right hand side is this \\( \\lim_{\\Delta \\to 0} \\) which means \u0026ldquo;as \\(\\Delta\\) approaches 0\u0026rdquo;. Limits are a way of talking about the way the function reacts to bringing certain parameters closer to a given limit. Essentially, the closer the parameter is to the limit, the more accurate the approximation is. In our case we talk about the function being as \\(\\Delta\\) approaches 0, the right hand side becomes more accurately the correct value for the derivative.\nOf course, \\(\\Delta\\) can never be 0, as this would result in a divide by zero operation which is undefined, but we can get arbitrarily close.\nWhat\u0026rsquo;s delta?\nThe symbol \\(\\Delta\\) is the greek symbol \u0026ldquo;Delta\u0026rdquo; which we use here to mean a change. Since to calculate the derivative we\u0026rsquo;re applying a small change to the value, and measuring how the output is affected by this small change.\nHow is the derivative calculated?\nThe rest of the right hand side is simply calculating the actual derivative at the value a, with the given \\(\\Delta\\) value. For a pretty close approximation of the true derivative, we can choose a very small value of delta, for example 0.001. We first add the small value to the value at which we\u0026rsquo;re calculating the derivative, this is the \\( f(a + \\Delta) \\)\nThis gives us the output of the function at a point ever so slightly in front of the test value (a). In order to measure a rate of change, two samples are required. We could have simply applied the function to a itself, however in order to account for asymmetry in the graph, we can get a better test by applying the function to the value located slightly behind the test value, this is \\( f(a - \\Delta) \\).\nFinally we take the difference of these two sample points, that is the difference in their outputs. This gives us the difference in the function output \\( f(a + \\Delta) - f(a - \\Delta) \\).\nIn order to get the gradient, once we have the change in the output of the function, we must divide by the change in the input. Since we both added and subtracted \\( \\Delta \\) to get our sample input points, the range (change in input) is \\( 2 \\times \\Delta \\)\n Code We can code up a function that can calculate the derivative of another function at a given input value, and given the value of the small delta we\u0026rsquo;re applying.\nThe function we write here will take a float in and output a float for simplicity. We could of course use generics to allow this to work for various types. With floats being the data type used, it would look as follows\nfn derivative(f: impl Fn(f64) -\u0026gt; f64, a: f64, delta: f64) -\u0026gt; f64 { let front_result = f(a + delta); let back_result = f(a - delta); let input_change = delta * 2.0; let output_change = front_result - back_result; output_change / input_change } An example call of finding the derivative of a function:\n$$ f(x) = x^2 $$\nat the input value 42 might look as follows:\nlet f = |x| x * x; let a = 42.0; let delta = 0.001; derivative(f, a, delta)  Diagram A derivative of a function being the rate of change of the output with change in the input can be visualised by the following diagram. This diagram shows a function f, which maps \u0026ldquo;x\u0026rdquo; to \u0026ldquo;y\u0026rdquo;, and shows a change in the input (dx) producing a change in the output (dy). The ratio of \\( \\frac{dy}{dx} \\) gives us the derivative\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/derivatives/","summary":"Maths The derivative of a function is the rate at which the output changes with respect to a change in the input at a specific value for the input. This last part is important as the derivative of a function is essentially the gradient, or tangent of the graph of that function at a specific point which can of course change depending on where you are on the number line.","title":"Derivatives"},{"content":"There isn\u0026rsquo;t too much to say for what a function actually is, since being a programmer, we use them every day. However for completeness I\u0026rsquo;ll include a post here.\nFor our purposes it will help to think of a function as a black box, that takes one or more inputs and returns an output. We can then chain these functions together by taking the output of function 1, and passing it to the next function and so on.\nThe advantage of thinking of them as a chain like this rather than a stack as programmers are used to thinking of them, is that this chain will easily map to the concept of the forward pass of a neural network later on.\nAssuming the existence of the following two functions, each of which takes a single parameter:\n$$ f(x) = x^2 $$ $$ g(x) = x + 10 $$\nWe can code these functions as below, along with how they\u0026rsquo;re chained together to act as one big composite function that takes input into the first, and extracts output from the last:\nlet f = |x: f64| x * x; let g = |x: f64| x + 10.0; let input = 42.0; // input into the first function in the chain let intermediate = f(input); // after invoking function \u0026#34;f\u0026#34;, the output is an intermediate result let output = g(intermediate); // to get the final input, we can pass this intermediate result through \u0026#34;g\u0026#34;  A diagram showing function omposition would look something like this, where the chain of two functions results in the same as running through a single function that is the composite of the two\n","permalink":"https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter1/functions/","summary":"There isn\u0026rsquo;t too much to say for what a function actually is, since being a programmer, we use them every day. However for completeness I\u0026rsquo;ll include a post here.\nFor our purposes it will help to think of a function as a black box, that takes one or more inputs and returns an output. We can then chain these functions together by taking the output of function 1, and passing it to the next function and so on.","title":"Functions"},{"content":" Ubisoft Reflections\nSenior Gameplay Programmer - Assassin\u0026rsquo;s Creed VR\nApril 2020 - Present\nAs a senior gameplay programmer I have been responsible for architecting and implementing large systems in Assassin\u0026rsquo;s Creed VR. Where the product statement isn\u0026rsquo;t clear or is incomplete, I have worked with other disciplines to ensure the final design does what is required. During this time I have gained experience in Unity3D including DOTS and best practices for efficient programs. Additionally, I attempt to always share my knowledge as I develop it myself so as to allow others to also build their skills. My main epiphany or learning from this project has been the now popular \u0026ldquo;Entity Component System\u0026rdquo; (ECS) architecture which I have put into practice in a large feature of the project.\nUbisoft Reflections\nGameplay Programmer - Tom Clancy\u0026rsquo;s The Division 2\nMarch 2018 - April 2020\nAs a gameplay programmer I have helped design and implement some fairly large systems for \u0026ldquo;Tom Clancy\u0026rsquo;s The Division 2\u0026rdquo; with the main feature being clans. As part of the clans feature I worked with designers, as well as the online team in order to develop clan missions and rewards, and clan spaces.\nUbisoft Reflections\nJunior Gameplay Programmer - Tom Clancy\u0026rsquo;s The Division\nNovember 2014 - March 2018\nIn this position I worked on \u0026ldquo;Tom Clancy\u0026rsquo;s The Division\u0026rdquo; as well as the associated expansion packs and DLC. I gained industry experience using C++ in a large codebase, and also with the in-house engine used to develop these (Snowdrop). I gained experience in using visual scripting and node graph systems.\n Eutechnyx Ltd.\nProgrammer - Auto Club Revolution\nJuly 2010 - November 2014\nWhile working at Eutechnyx I was part of the team developing \u0026ldquo;Auto Club Revolution\u0026rdquo;, a social network and gaming site revolving around car enthusiasts. I gained experience with front and back end web technologies using HTML/CSS/JavaScript for the frontend, and Python for the backend. Additional technologies used were MongoDB for the database management software, along with Redis for caching. Being a dynamic website, I gained experience interfacing with popular APIs including Facebook, and payment providers.\n Newcastle University\nMSc, Computer Games Engineering\n2008-2009\nDissertation titled \u0026ldquo;Games with a purpose\u0026rdquo;\nGraduated with merit\nNewcastle University\nBSc, Computing Science\n2005-2008\nDissertation titled \u0026ldquo;Video records of everyday life\u0026rdquo;\nGraduated with first class degree\n","permalink":"https://forgottenmaster.github.io/resume/","summary":"Ubisoft Reflections\nSenior Gameplay Programmer - Assassin\u0026rsquo;s Creed VR\nApril 2020 - Present\nAs a senior gameplay programmer I have been responsible for architecting and implementing large systems in Assassin\u0026rsquo;s Creed VR. Where the product statement isn\u0026rsquo;t clear or is incomplete, I have worked with other disciplines to ensure the final design does what is required. During this time I have gained experience in Unity3D including DOTS and best practices for efficient programs.","title":"Résumé"}]