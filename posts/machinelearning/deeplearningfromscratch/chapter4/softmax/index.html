<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>4.3 - Softmax | Robin Firth</title>
<meta name="keywords" content="" />
<meta name="description" content="This post will describe a new loss function called &ldquo;cross entropy loss&rdquo;. Due to often being coupled with the &ldquo;softmax&rdquo; function, this is often just called the &ldquo;softmax cross entropy loss&rdquo; function. There are a couple of components needed to support that.
Softmax Thus far we used the mean squared error as a loss function which has a nice property that the further the prediction is from the target, the steeper the gradient that&rsquo;s sent in the backpropagation pass is which enables it to converge quite fast.">
<meta name="author" content="">
<link rel="canonical" href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/softmax/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css" integrity="sha256-b2AFbUTT9&#43;tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://forgottenmaster.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://forgottenmaster.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://forgottenmaster.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://forgottenmaster.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://forgottenmaster.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.96.0" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="4.3 - Softmax" />
<meta property="og:description" content="This post will describe a new loss function called &ldquo;cross entropy loss&rdquo;. Due to often being coupled with the &ldquo;softmax&rdquo; function, this is often just called the &ldquo;softmax cross entropy loss&rdquo; function. There are a couple of components needed to support that.
Softmax Thus far we used the mean squared error as a loss function which has a nice property that the further the prediction is from the target, the steeper the gradient that&rsquo;s sent in the backpropagation pass is which enables it to converge quite fast." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/softmax/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-02T19:33:53&#43;01:00" />
<meta property="article:modified_time" content="2022-05-02T19:33:53&#43;01:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="4.3 - Softmax"/>
<meta name="twitter:description" content="This post will describe a new loss function called &ldquo;cross entropy loss&rdquo;. Due to often being coupled with the &ldquo;softmax&rdquo; function, this is often just called the &ldquo;softmax cross entropy loss&rdquo; function. There are a couple of components needed to support that.
Softmax Thus far we used the mean squared error as a loss function which has a nice property that the further the prediction is from the target, the steeper the gradient that&rsquo;s sent in the backpropagation pass is which enables it to converge quite fast."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://forgottenmaster.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Machine Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Deep Learning From Scratch",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Chapter 4 - Extensions",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/"
    }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "4.3 - Softmax",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/softmax/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "4.3 - Softmax",
  "name": "4.3 - Softmax",
  "description": "This post will describe a new loss function called \u0026ldquo;cross entropy loss\u0026rdquo;. Due to often being coupled with the \u0026ldquo;softmax\u0026rdquo; function, this is often just called the \u0026ldquo;softmax cross entropy loss\u0026rdquo; function. There are a couple of components needed to support that.\nSoftmax Thus far we used the mean squared error as a loss function which has a nice property that the further the prediction is from the target, the steeper the gradient that\u0026rsquo;s sent in the backpropagation pass is which enables it to converge quite fast.",
  "keywords": [
    
  ],
  "articleBody": "This post will describe a new loss function called “cross entropy loss”. Due to often being coupled with the “softmax” function, this is often just called the “softmax cross entropy loss” function. There are a couple of components needed to support that.\nSoftmax Thus far we used the mean squared error as a loss function which has a nice property that the further the prediction is from the target, the steeper the gradient that’s sent in the backpropagation pass is which enables it to converge quite fast.\nHowever, if the problem is a classification problem where the output is identifying one of a set of classes which the observation evaluates to, then we know that the predictions that are output for those classes are the probabilities of the observation being in that category.\nDue to being a set of probabilities, we know:\n Each individual probability should be between 0 and 1 All probabilities summed up should total 1  The softmax function will exploit these properties in order to produce even steeper gradients to converge faster for classification problems.\nFor a problem with 3 classes, the network will output a vector of 3 elements per observation as predictions, however the network will provide these as raw floating point values which aren’t necessarily between 0 and 1.\nFor a vector such as \\(\\begin{bmatrix}5 \u0026 3 \u0026 2\\end{bmatrix}\\) we want to convert this into a vector of probabilities. The simplest way to do this would be to normalize the vector, that is dividing each element by the total sum. For this specific vector, it would look like follows\n$$ Normalize(\\begin{bmatrix}5 \\\\ 3 \\\\ 2\\end{bmatrix}) = \\begin{bmatrix}\\frac 5 {5 + 3 + 2} \\\\ \\frac 3 {5 + 3 + 2} \\\\ \\frac 2 {5 + 3 + 2}\\end{bmatrix} = \\begin{bmatrix}\\frac 5 {10} \\\\ \\frac 3 {10} \\\\ \\frac 2 {10}\\end{bmatrix} = \\begin{bmatrix}0.5 \\\\ 0.3 \\\\ 0.2\\end{bmatrix} $$\nHowever it turns out that the softmax function instead will provide steeper gradients which is to say that it will be able to bias the probabilities towards that which it thinks is correct.\nThe formula for softmax is much the same as regular normalization except that we’re using the values as powers of the mathematical constant, e.\nFor that same example then, the softmax function look like follows\n$$ Softmax(\\begin{bmatrix}5 \\\\ 3 \\\\ 2\\end{bmatrix}) = \\begin{bmatrix}\\frac {e^5} {{e^5} + {e^3} + {e^2}} \\\\ \\frac {e^3} {{e^5} + {e^3} + {e^2}} \\\\ \\frac {e^2} {{e^5} + {e^3} + {e^2}}\\end{bmatrix} = \\begin{bmatrix}\\frac {148.41315910257660342111558004055} {175.88775212469492138927453715571} \\\\ \\frac {20.085536923187667740928529654582} {175.88775212469492138927453715571} \\\\ \\frac {7.389056098930650227230427460575} {175.88775212469492138927453715571}\\end{bmatrix} = \\begin{bmatrix}0.84379473448133947005179288509305 \\\\ 0.11419519938459447893014170254169 \\\\ 0.04201006613406605101806541236525\\end{bmatrix} $$\nShortening these results to 2 decimal places gives us \\(\\begin{bmatrix}0.84 \u0026\u0026 0.11 \u0026\u0026 0.04\\end{bmatrix}\\)\nAs we can see the calculation is now less neutral to the answer already closest to 1 and so this function biases it more toward 1 by reducing the value of the less favourable results.\nIf we were to apply a max function which sets the probability of the highest prediciton to 1 and the others to 0, then we’d end up with \\(\\begin{bmatrix}1, 0, 0\\end{bmatrix}\\) instead.\nAs we can see, softmax generates results halfway between a standard normalization, and the max function, hence the name softmax.\nCross entropy loss The second portion is the actual loss calculation which is called cross entropy loss and again is specific to classification problems where the predictions and targets represent the probabilities of being in a particular class.\nThe cross entropy loss function which is represented by the following dual case formula\n$$ CE(p_i, y_i) = \\begin{cases} -log(1 - p_i), \u0026 \\text{if } y_i = 0 \\\\ -log(p_i), \u0026 \\text{if } y_i = 1 \\end{cases} $$\nWhere \\(p_i\\) is an entry in the predictions probabilities output by the softmax function at index \\(i\\), and \\(y_i\\) is the target at the same index which is guaranteed to be either a 0 or a 1 (since this is a classification problem).\nThis can actually be represented in a single formula due to the target being either a 0 or a 1\n$$ CE(p_i, y_i) = -y_i \\times log(p_i) - (1 - y_i) \\times log(1 - p_i) $$\nThis works because, if \\(y_i\\) is a 0 then the first term will be eliminated, and if it’s a 1 then the second term is eliminated (due to 1 - 1 being 0).\nIf we look at a graph comparing the loss values generated by the cross entropy loss, compared to those generated by the mean squared error function we can see that the penalties are not only higher from the beginning, but that they get higher at a much steeper rate\nIn fact, as can be seen from the graph, as the difference between our prediction and target approaches 1, the loss approaches infinity.\nThis produces greater penalties for loss, and the softmax function produces steeper gradients, so the two combined together will enable us to move weights by a larger amount and train the network much faster. These two are often used together, and so is usually seen as the softmax cross entropy loss\nThe formula for this can be written as follows (assuming 3 classes in the output)\n$$ SCE_1 = -y_1 \\times log(\\frac {e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3}}) - (1 - y_1) \\times log(1 - \\frac {e^{x_1}} {e^{x_1} + e^{x_2} + e^{x_3}}) $$\nThis more complex formula might seem to have an equally complex derivative calculation, however it’s a very simple formula in fact\n$$ \\frac {\\partial {SCE_1}} {\\partial x_1} = {\\frac {e^{x_1}} {e^{x_1} + e^{x_2} + e^{x_3}}} - y_1 $$\nWhich means that the derivative is simply\n$$ softmax(\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}) - \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} $$\nCoding Now we’re ready to implement this as a loss function which as it turns out will be fairly simple. Each row of the input data to the loss function will be a prediction with X columns (for X classes) that we pass through the softmax function to normalize them into a set of probabilities.\nAfter the softmax, we will do the cross entropy loss calculation, and this is where we must be careful. Cross entropy loss as mentioned before is asymptotic and will tend towards infinity as the difference approaches 1, so to avoid this we clip the output of the softmax function so that the prediction is neither 0 nor 1, but we’ll clamp to some small epsilon value.\nSoftmaxCrossEntropy struct For the struct, as with all of our other “vtable” style types, it will be an empty type - though we need to make sure it’s Clonable due to the cloning of the neural network required in the training process\n#[derive(Clone)] pub struct SoftmaxCrossEntropy; The only function we’ll have associated with this type is one that can create a new boxed instance for convenience. For consistency we call this new_boxed\npub fn new_boxed() - BoxSelf {  Box::new(Self) } LossImpl trait bounds For implementing the LossImpl trait, we require that we are implementing it only for those types that support the necessary functionality. These are mostly arithmetic, but in particular the num_traits::Float trait will provide us a few functions we’re going to make use of.\nWe need the following traits placed as bounds on the type T\n Float - This is from num_traits and provides us functions such as epsilon() and exp() ScalarOperand - This is from ndarray and is required because there are a few points where we are dividing or otherwise operating on the whole array with a value of type T  Therefore the declaration that we are implementing LossImplfor our SoftmaxCrossEntropy type will look as follows\nimplT: Float + ScalarOperand LossImplT for SoftmaxCrossEntropy LossImpl::calculate_output In order to calculate the output from our given set of predictions and targets, we firstly will need to apply the softmax function to turn the predictions into probabilities, and as mentioned previously need to also clamp the output to within +/- some epsilon value.\nThere is however something else we need to consider which is that if our predictions and targets have only a single class that a value either can be in (1) or not at all (0), then applying the softmax function would always produce a 1 since any number divided by itself is 1.\nIn order to ensure that the softmax can work on a single-class problem, we will need to modify it to be a dual class problem.\nWe can do this by treating the existing column as the probability that the observation is in the class, and therefore we can add a new column that is the probability of it not being in the class. We can calculate this by knowing that probabilities sum to 1, so the value of this column will be 1-x where x is the value in the existing one.\nTherefore the vector of predictions in a single class problem \\(\\begin{bmatrix} 0.4 \\\\ 0.1 \\\\ 0.9 \\end{bmatrix}\\) would be mapped onto the dual class problem matrix of \\(\\begin{bmatrix} 0.4 \u0026 0.6 \\\\ 0.1 \u0026 0.9 \\\\ 0.9 \u0026 0.1 \\end{bmatrix}\\)\nHowever, since we’re turning the single class problem into a dual class one, we need to also do the same for the target vector so that the cross entropy can be applied correctly.\nWe will wrap all this up into a helper function that we’ll detail shortly called calculate_softmax_predictions_and_targets which we call as such in our calculate_output method\nlet (predictions, targets) =  calculate_softmax_predictions_and_targets((*predictions).clone(), (*targets).clone()); Next we can apply the cross loss to the predictions and targets. Recall from the formula of the cross entropy loss that we can calculate it as a subtraction sum.\nThe left hand side of the subtraction (called the minuend) is \\(-y \\times log(p) \\) which means that we need to get a version of the targets array that is negated, and a version of the predictions array that is the natural log of the elements.\nIn code this looks as follows\nlet minuend = targets.mapv(|elem| -elem) * predictions.mapv(|elem| elem.ln()); The right hand side of the formula (called the subtrahend) is \\((1 - y) \\times log(1 - p)\\) so the subtrahend looks as follows\nlet subtrahend = targets.mapv(|elem| T::one() - elem) * predictions.mapv(|elem| (T::one() - elem).ln()); Then we can perform the subtraction to get the loss Array\nlet result = minuend - subtrahend; Finally, the loss function is expected to return a single value representing the total loss over all predictions, so we need to sum the results to get the final loss value\nresult.sum() Putting all of this together we get the entire calculate_output function\nfn calculate_output(\u0026self, predictions: \u0026ArrayT, Ix2, targets: \u0026ArrayT, Ix2) - T {  let (predictions, targets) =  calculate_softmax_predictions_and_targets((*predictions).clone(), (*targets).clone());  let minuend = targets.mapv(|elem| -elem) * predictions.mapv(|elem| elem.ln());  let subtrahend =  targets.mapv(|elem| T::one() - elem) * predictions.mapv(|elem| (T::one() - elem).ln());  let result = minuend - subtrahend;  result.sum() } LossImpl::calculate_input_gradient The second required function is to be able to calculate the input gradients during the backward pass. For this we firstly need to know whether this is a single class problem or not, this is because when we calculate the softmax predictions on a single class problem, we are converting it to a dual class problem.\nBecause of this, the gradients produced will also be of a dual class nature, so before propagating back through the network, this extra dummy column we added needs to be stripped off.\nWe can cache whether it’s a single class Array by simply checking the number of columns in it is 1\nlet is_single_class = predictions.ncols() == 1; We need to go ahead and recalculate the predictions because we aren’t retaining any state in our vtable trait objects, but won’t show that here.\nRecalling that calculating the gradient for the cross entropy loss function is simply the result of subtracting the targets Array from the calculated predictions, we can implement it as such\nlet gradient = predictions - targets; The last step is to strip away that dummy column we added if the problem is a single class problem, which we can do through a helper function\nif is_single_class {  dual_class_to_single(gradient) } else {  gradient } Putting all of this together gives us the complete implementation for our gradient calculation function\nfn calculate_input_gradient(  \u0026self,  predictions: \u0026ArrayT, Ix2,  targets: \u0026ArrayT, Ix2, ) - ArrayT, Ix2 {  let is_single_class = predictions.ncols() == 1;  let (predictions, targets) =  calculate_softmax_predictions_and_targets((*predictions).clone(), (*targets).clone());  let gradient = predictions - targets;  if is_single_class {  dual_class_to_single(gradient)  } else {  gradient  } } calculate_softmax_predictions_and_targets (helper) As a reminder, we want this helper function to prepare the predictions and targets for our cross entropy loss calculation, by doing the following:\n Adding a dummy column if it’s a single class Array so that softmax can work with a dual class Array Run the predictions through the softmax function to turn them into probabilities Clamp the probabilities so that they don’t ever hit 0 or 1 due to asymptotic calculations when we use log during the cross entropy loss calculation  The first step to this is easy enough, but we will delegate this to a helper function that we will cover later. For calling it, we will simply check if the number of columns in the predictions is 1, and if it is then we will update both predictions and targets to have the additional column\nif predictions.ncols() == 1 {  predictions = single_class_to_dual(predictions);  targets = single_class_to_dual(targets); } Then we apply the softmax function on the predictions which is done with a simple call, again we’ll defer covering the softmax function until later\nlet mut predictions = softmax(predictions) For the third point, clamping the probabilities, we can do this by using a method called “clamp” from the num_traits library. This function works with any T where T implements the Float trait. Conveniently, Float also gives us an associated method “epsilon” which gives us a small epsilon value to clamp with.\nWe can map the predictions in place, and for each element of the predictions array, perform the clamp. This looks as follows\npredictions.mapv_inplace(|elem| num_traits::clamp(elem, T::epsilon(), T::one() - T::epsilon())); Finally we’re able to return the predictions and targets. As an entire function it looks as follows\nfn calculate_softmax_predictions_and_targetsT: Float(  mut predictions: ArrayT, Ix2,  mut targets: ArrayT, Ix2, ) - (ArrayT, Ix2, ArrayT, Ix2) {  if predictions.ncols() == 1 {  predictions = single_class_to_dual(predictions);  targets = single_class_to_dual(targets);  }  let mut predictions = softmax(predictions);  predictions.mapv_inplace(|elem| num_traits::clamp(elem, T::epsilon(), T::one() - T::epsilon()));  (predictions, targets) } softmax This function will be responsible for applying softmax to the Array of predictions. We actually will need to apply this independently on each row\nWe know that in softmax calculation, we are working with values of \\(e^x\\) and don’t need to keep the original values around. Therefore the first step should be to prepare the input array by calculating each element to be e raised to that power as such\narr.map_inplace(|elem| *elem = elem.exp()); ndarray::Array contains a convenient function called map_axis which takes an axis to map over, and a function. The function will take an entire slice of that axis at a particular index, and should return a single value. This results in an Array that is one dimension less.\nFor our purposes we will iterate in the direction of the columns which is Axis(1), and the function will therefore accept one row at a time. We just sum the row to get the total of all features in that row.\nThis will give us a 1 dimensional array, but for the division we will be doing, to ensure that the broadcasting works correctly, we need to reshape this into a 2 dimensional array, with a single column. Therefore each entry in the column vector will be a total for the associated row in the input matrix.\nWe can use the into_shape function to reshape the totals, but we need to unwrap the Result as if the number of elements in the input don’t match the requested dimensions it will give an error. In this case we know it has to be correct so we can unwrap it safely.\nThis looks as follows\nlet totals = arr  .map_axis(Axis(1), |row| row.sum())  .into_shape((arr.nrows(), 1))  .unwrap(); Finally we can divide the array by the totals. Due to the way operator broadcasting works, this will broadcast the division of a multi-column vector by a single column one such that each element in a given row is divided by the associated element in the divisor.\nFor clarity, this means that\n$$ \\begin{bmatrix} x_1 \u0026 x_2 \u0026 x_3 \\\\ y_1 \u0026 y_2 \u0026 y_3 \\end{bmatrix} / \\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\end{bmatrix} = \\begin{bmatrix} x_1/z_1 \u0026 x_2/z_1 \u0026 x_3/z_1 \\\\ y_1/z_2 \u0026 y_2/z_2 \u0026 y_3 /z_2 \\end{bmatrix} $$\nThe total function is then as follows (note the Float trait bound so we have access to the exp function and also to arithmetical operators)\nfn softmaxT: Float(mut arr: ArrayT, Ix2) - ArrayT, Ix2 {  arr.map_inplace(|elem| *elem = elem.exp());  let totals = arr  .map_axis(Axis(1), |row| row.sum())  .into_shape((arr.nrows(), 1))  .unwrap();  arr / totals } single_class_to_dual This is the first of the two “mapping” functions we use to convert a single class problem into a dual class one for the purposes of being able to apply softmax and cross entropy appropriately.\nThis one takes the initial predictions array and adds an additional feature/column onto it representing the inverse probabilities, or the probabilities of not being in the single class.\nTo calculate this, we will iterate over the elements of the input array (which we validate is a single column so each element is a prediction for a row), we will then use a flat_map to map this single element onto an iterator that produces both the element, and then the inverse of the element (1 - element). We can then take this iterator and use Array::from_iter to convert it into a 1-D array.\nHowever, all of the code expects the array to be a 2-D array, so we must reshape it to be as such.\nThe function must also have appropriate bounds on “T” to be able to use the various functions. These bounds will be:\n Clone - We need to be able to clone elements in the predictions array so as to be able to emit it, and also use it to calculate the value in the additional column One - We need this trait to be able to get “one” of a “T” as we’ll need it to calculate the value in the second column Sub - This is used also in calculating the value for the new column as we need to be able to subtract the prediction (a “T”) from our “one” (also a “T”)  Overall the function looks as follows\nfn single_class_to_dualT: Clone + One + SubOutput = T(input: ArrayT, Ix2) - ArrayT, Ix2 {  assert_eq!(input.ncols(), 1);  let rows = input.nrows();  Array::from_iter(input.iter().flat_map(|elem| {  std::iter::once((*elem).clone()).chain(std::iter::once(T::one() - (*elem).clone()))  }))  .into_shape((rows, 2))  .unwrap() } dual_class_to_single The final function will go the other way which is to take a 2-D array with 2 columns and strip the last column off, retaining only the first one.\nWe can do this with the select function provided by ndarray::Array which takes the Axis to select from, and the indices to retain.\nIn our case, we are selecting from the columns of the Array which is Axis(1), and we only need select index 0.\nThe total function is then pretty simple\nfn dual_class_to_singleT: Clone(input: ArrayT, Ix2) - ArrayT, Ix2 {  assert_eq!(input.ncols(), 2);  input.select(Axis(1), \u0026[0]) } Performance Now that we’ve implemented the Tanh activation function previously (in a previous post), and now the softmax cross entropy loss function, we can test on the sample data set/problem and see how accurate the various approaches are.\nFirstly we have a linear regression, which is simply a single dense layer with 1 neuron, and a linear activation function. For the loss function we use MeanSquaredError, so the code of the network setup looks as follows\nlet mut network = Network::new(  vec![Layer::new_with_seed(  1,  DenseLayerSetup::new_boxed(Linear::new_boxed()),  SEED,  )],  Loss::new(MeanSquaredError::new_boxed()), ); And running the example shows that the linear regression in fact only has a small percentage chance of correctly guessing the right answer\nAccuracy (training): 16.652% Accuracy (testing): 16.796% Secondly we try out a basic neural network. For this, we will have two dense layers. The first dense layer will use the Tanh activation function and will have multiple neurons (we choose 54 here). The second layer will have a single neuron with a linear activation function and will serve to bring together the outputs from the hidden layer into a single output for use with the loss function.\nAgain, we use a basic MeanSquaredError loss function so the setup code looks as follows\nlet mut network = Network::new(  vec![  Layer::new_with_seed(54, DenseLayerSetup::new_boxed(Tanh::new_boxed()), SEED),  Layer::new_with_seed(1, DenseLayerSetup::new_boxed(Linear::new_boxed()), SEED),  ],  Loss::new(MeanSquaredError::new_boxed()), ); Surprisingly, this function results in exactly the same predictions as the linear regression, which is to say using a basic neural network with a MeanSquaredError loss function is resulting in a very low accuracy. It’s the same figure as shown by the following output.\nAccuracy (training): 16.652% Accuracy (testing): 16.796% This at least indicates that our problem can at least be somewhat solved by a basic line in linear regression, however it’s surprising that a more complex network is not able to get any more accurate. This could be an issue with the MeanSquaredError.\nFinally, we make only one small change to the basic neural network, which is to swap the MeanSquaredError loss function for our new SoftmaxCrossEntropy loss function. This is the only change to the setup as shown by the following code\nlet mut network = Network::new(  vec![  Layer::new_with_seed(54, DenseLayerSetup::new_boxed(Tanh::new_boxed()), SEED),  Layer::new_with_seed(1, DenseLayerSetup::new_boxed(Linear::new_boxed()), SEED),  ],  Loss::new(SoftmaxCrossEntropy::new_boxed()), ); However, the accuracy on our problem data has shot right up!\nAccuracy (training): 91.786% Accuracy (testing): 91.742% This indicates that the steeper gradients and harsher loss penalties given by softmax and cross entropy loss respectively are resulting in a much better fit after training the network. Thus we can conclude that this loss function is much better for classification type problems.\n",
  "wordCount" : "3654",
  "inLanguage": "en",
  "datePublished": "2022-05-02T19:33:53+01:00",
  "dateModified": "2022-05-02T19:33:53+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/softmax/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Robin Firth",
    "logo": {
      "@type": "ImageObject",
      "url": "https://forgottenmaster.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://forgottenmaster.github.io/" accesskey="h" title="Robin Firth (Alt + H)">Robin Firth</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://forgottenmaster.github.io/resume/" title="Résumé">
                    <span>Résumé</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://forgottenmaster.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/">Posts</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/">Machine Learning</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/">Deep Learning From Scratch</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/">Chapter 4 - Extensions</a></div>
    <h1 class="post-title">
      4.3 - Softmax
    </h1>
    <div class="post-meta"><span title='2022-05-02 19:33:53 +0100 BST'>May 2, 2022</span>&nbsp;·&nbsp;18 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#softmax" aria-label="Softmax">Softmax</a></li>
                <li>
                    <a href="#cross-entropy-loss" aria-label="Cross entropy loss">Cross entropy loss</a></li>
                <li>
                    <a href="#coding" aria-label="Coding">Coding</a><ul>
                        
                <li>
                    <a href="#softmaxcrossentropy-struct" aria-label="SoftmaxCrossEntropy struct">SoftmaxCrossEntropy struct</a></li>
                <li>
                    <a href="#lossimpl-trait-bounds" aria-label="LossImpl trait bounds">LossImpl trait bounds</a></li>
                <li>
                    <a href="#lossimplcalculate_output" aria-label="LossImpl::calculate_output">LossImpl::calculate_output</a></li>
                <li>
                    <a href="#lossimplcalculate_input_gradient" aria-label="LossImpl::calculate_input_gradient">LossImpl::calculate_input_gradient</a></li>
                <li>
                    <a href="#calculate_softmax_predictions_and_targets-helper" aria-label="calculate_softmax_predictions_and_targets (helper)">calculate_softmax_predictions_and_targets (helper)</a></li>
                <li>
                    <a href="#softmax-1" aria-label="softmax">softmax</a></li>
                <li>
                    <a href="#single_class_to_dual" aria-label="single_class_to_dual">single_class_to_dual</a></li>
                <li>
                    <a href="#dual_class_to_single" aria-label="dual_class_to_single">dual_class_to_single</a></li></ul>
                </li>
                <li>
                    <a href="#performance" aria-label="Performance">Performance</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>This post will describe a new loss function called &ldquo;cross entropy loss&rdquo;. Due to often being coupled with the &ldquo;softmax&rdquo; function, this is often just called the &ldquo;softmax cross entropy loss&rdquo; function. There are a couple of components needed to support that.</p>
<h3 id="softmax">Softmax<a hidden class="anchor" aria-hidden="true" href="#softmax">#</a></h3>
<p>Thus far we used the mean squared error as a loss function which has a nice property that the further the prediction is from the target, the steeper the gradient that&rsquo;s sent in the backpropagation pass is which enables it to converge quite fast.</p>
<p>However, if the problem is a classification problem where the output is identifying one of a set of classes which the observation evaluates to, then we know that the predictions that are output for those classes are the <strong>probabilities</strong> of the observation being in that category.</p>
<p>Due to being a set of probabilities, we know:</p>
<ol>
<li>Each individual probability should be between 0 and 1</li>
<li>All probabilities summed up should total 1</li>
</ol>
<p>The softmax function will exploit these properties in order to produce even steeper gradients to converge faster for classification problems.</p>
<p>For a problem with 3 classes, the network will output a vector of 3 elements per observation as predictions, however the network will provide these as raw floating point values which aren&rsquo;t necessarily between 0 and 1.</p>
<p>For a vector such as \(\begin{bmatrix}5 &amp; 3 &amp; 2\end{bmatrix}\) we want to convert this into a vector of probabilities. The simplest way to do this would be to <strong>normalize</strong> the vector, that is dividing each element by the total sum. For this specific vector, it would look like follows</p>
<p>$$ Normalize(\begin{bmatrix}5 \\ 3 \\ 2\end{bmatrix}) = \begin{bmatrix}\frac 5 {5 + 3 + 2} \\ \frac 3 {5 + 3 + 2} \\ \frac 2 {5 + 3 + 2}\end{bmatrix} = \begin{bmatrix}\frac 5 {10} \\ \frac 3 {10} \\ \frac 2 {10}\end{bmatrix} = \begin{bmatrix}0.5 \\ 0.3 \\ 0.2\end{bmatrix} $$</p>
<p>However it turns out that the softmax function instead will provide steeper gradients which is to say that it will be able to bias the probabilities towards that which it thinks is correct.</p>
<p>The formula for softmax is much the same as regular normalization except that we&rsquo;re using the values as powers of the mathematical constant, <em>e</em>.</p>
<p>For that same example then, the softmax function look like follows</p>
<p>$$ Softmax(\begin{bmatrix}5 \\ 3 \\ 2\end{bmatrix}) = \begin{bmatrix}\frac {e^5} {{e^5} + {e^3} + {e^2}} \\ \frac {e^3} {{e^5} + {e^3} + {e^2}} \\ \frac {e^2} {{e^5} + {e^3} + {e^2}}\end{bmatrix} = \begin{bmatrix}\frac {148.41315910257660342111558004055} {175.88775212469492138927453715571} \\ \frac {20.085536923187667740928529654582} {175.88775212469492138927453715571} \\ \frac {7.389056098930650227230427460575} {175.88775212469492138927453715571}\end{bmatrix} = \begin{bmatrix}0.84379473448133947005179288509305 \\ 0.11419519938459447893014170254169 \\ 0.04201006613406605101806541236525\end{bmatrix} $$</p>
<p>Shortening these results to 2 decimal places gives us \(\begin{bmatrix}0.84 &amp;&amp; 0.11 &amp;&amp; 0.04\end{bmatrix}\)</p>
<p>As we can see the calculation is now <strong>less neutral</strong> to the answer already closest to 1 and so this function biases it more toward 1 by reducing the value of the less favourable results.</p>
<p>If we were to apply a <strong>max</strong> function which sets the probability of the highest prediciton to 1 and the others to 0, then we&rsquo;d end up with \(\begin{bmatrix}1, 0, 0\end{bmatrix}\) instead.</p>
<p>As we can see, softmax generates results halfway between a standard normalization, and the max function, hence the name <strong>soft</strong>max.</p>
<h3 id="cross-entropy-loss">Cross entropy loss<a hidden class="anchor" aria-hidden="true" href="#cross-entropy-loss">#</a></h3>
<p>The second portion is the actual loss calculation which is called cross entropy loss and again is specific to classification problems where the predictions and targets represent the probabilities of being in a particular class.</p>
<p>The cross entropy loss function which is represented by the following dual case formula</p>
<p>$$
CE(p_i, y_i) =
\begin{cases}
-log(1 - p_i), &amp; \text{if } y_i = 0 \\
-log(p_i), &amp; \text{if } y_i = 1
\end{cases}
$$</p>
<p>Where \(p_i\) is an entry in the predictions probabilities output by the softmax function at index \(i\), and \(y_i\) is the target at the same index which is guaranteed to be either a 0 or a 1 (since this is a classification problem).</p>
<p>This can actually be represented in a single formula due to the target being either a 0 or a 1</p>
<p>$$
CE(p_i, y_i) = -y_i \times log(p_i) - (1 - y_i) \times log(1 - p_i)
$$</p>
<p>This works because, if \(y_i\) is a 0 then the first term will be eliminated, and if it&rsquo;s a 1 then the second term is eliminated (due to 1 - 1 being 0).</p>
<p>If we look at a graph comparing the loss values generated by the cross entropy loss, compared to those generated by the mean squared error function we can see that the penalties are not only higher from the beginning, but that they get higher at a much steeper rate</p>
<p><img loading="lazy" src="loss_graph.PNG" alt="Loss Graph"  />
</p>
<p>In fact, as can be seen from the graph, as the difference between our prediction and target approaches 1, the loss approaches infinity.</p>
<p>This produces greater penalties for loss, and the softmax function produces steeper gradients, so the two combined together will enable us to move weights by a larger amount and train the network much faster. These two are often used together, and so is usually seen as the <strong>softmax cross entropy loss</strong></p>
<p>The formula for this can be written as follows (assuming 3 classes in the output)</p>
<p>$$ SCE_1 = -y_1 \times log(\frac {e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3}}) - (1 - y_1) \times log(1 - \frac {e^{x_1}} {e^{x_1} + e^{x_2} + e^{x_3}}) $$</p>
<p>This more complex formula might seem to have an equally complex derivative calculation, however it&rsquo;s a very simple formula in fact</p>
<p>$$ \frac {\partial {SCE_1}} {\partial x_1} = {\frac {e^{x_1}} {e^{x_1} + e^{x_2} + e^{x_3}}} - y_1 $$</p>
<p>Which means that the derivative is simply</p>
<p>$$ softmax(\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}) - \begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix} $$</p>
<h3 id="coding">Coding<a hidden class="anchor" aria-hidden="true" href="#coding">#</a></h3>
<p>Now we&rsquo;re ready to implement this as a loss function which as it turns out will be fairly simple. Each row of the input data to the loss function will be a prediction with X columns (for X classes) that we pass through the softmax function to normalize them into a set of probabilities.</p>
<p>After the softmax, we will do the cross entropy loss calculation, and this is where we must be careful. Cross entropy loss as mentioned before is asymptotic and will tend towards infinity as the difference approaches 1, so to avoid this we clip the output of the softmax function so that the prediction is neither 0 nor 1, but we&rsquo;ll clamp to some small epsilon value.</p>
<h4 id="softmaxcrossentropy-struct">SoftmaxCrossEntropy struct<a hidden class="anchor" aria-hidden="true" href="#softmaxcrossentropy-struct">#</a></h4>
<p>For the struct, as with all of our other &ldquo;vtable&rdquo; style types, it will be an empty type - though we need to make sure it&rsquo;s Clonable due to the cloning of the neural network required in the training process</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#75715e">#[derive(Clone)]</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">SoftmaxCrossEntropy</span>;</span></span></code></pre></div>
<p>The only function we&rsquo;ll have associated with this type is one that can create a new boxed instance for convenience. For consistency we call this new_boxed</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">new_boxed</span>() -&gt; Box<span style="color:#f92672">&lt;</span>Self<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    Box::new(Self)
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h4 id="lossimpl-trait-bounds">LossImpl trait bounds<a hidden class="anchor" aria-hidden="true" href="#lossimpl-trait-bounds">#</a></h4>
<p>For implementing the LossImpl trait, we require that we are implementing it only for those types that support the necessary functionality. These are mostly arithmetic, but in particular the <strong>num_traits::Float</strong> trait will provide us a few functions we&rsquo;re going to make use of.</p>
<p>We need the following traits placed as bounds on the type T</p>
<ol>
<li>Float - This is from num_traits and provides us functions such as epsilon() and exp()</li>
<li>ScalarOperand - This is from ndarray and is required because there are a few points where we are dividing or otherwise operating on the whole array with a value of type T</li>
</ol>
<p>Therefore the declaration that we are implementing LossImpl<!-- raw HTML omitted --> for our SoftmaxCrossEntropy type will look as follows</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span>T: <span style="color:#a6e22e">Float</span> <span style="color:#f92672">+</span> ScalarOperand<span style="color:#f92672">&gt;</span> LossImpl<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">for</span> SoftmaxCrossEntropy</span></span></code></pre></div>
<h4 id="lossimplcalculate_output">LossImpl::calculate_output<a hidden class="anchor" aria-hidden="true" href="#lossimplcalculate_output">#</a></h4>
<p>In order to calculate the output from our given set of predictions and targets, we firstly will need to apply the softmax function to turn the predictions into probabilities, and as mentioned previously need to also clamp the output to within +/- some epsilon value.</p>
<p>There is however something else we need to consider which is that if our predictions and targets have only a single class that a value either can be in (1) or not at all (0), then applying the softmax function would always produce a 1 since any number divided by itself is 1.</p>
<p>In order to ensure that the softmax can work on a single-class problem, we will need to modify it to be a dual class problem.</p>
<p>We can do this by treating the existing column as the probability that the observation <strong>is</strong> in the class, and therefore we can add a new column that is the probability of it <strong>not</strong> being in the class. We can calculate this by knowing that probabilities sum to 1, so the value of this column will be 1-x where x is the value in the existing one.</p>
<p>Therefore the vector of predictions in a single class problem \(\begin{bmatrix} 0.4 \\ 0.1 \\ 0.9 \end{bmatrix}\) would be mapped onto the dual class problem matrix of \(\begin{bmatrix} 0.4 &amp; 0.6 \\ 0.1 &amp; 0.9 \\ 0.9 &amp; 0.1 \end{bmatrix}\)</p>
<p>However, since we&rsquo;re turning the single class problem into a dual class one, we need to also do the same for the target vector so that the cross entropy can be applied correctly.</p>
<p>We will wrap all this up into a helper function that we&rsquo;ll detail shortly called <strong>calculate_softmax_predictions_and_targets</strong> which we call as such in our calculate_output method</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> (predictions, targets) <span style="color:#f92672">=</span>
</span></span><span style="display:flex;"><span>    calculate_softmax_predictions_and_targets((<span style="color:#f92672">*</span>predictions).clone(), (<span style="color:#f92672">*</span>targets).clone());</span></span></code></pre></div>
<p>Next we can apply the cross loss to the predictions and targets. Recall from the formula of the cross entropy loss that we can calculate it as a subtraction sum.</p>
<p>The left hand side of the subtraction (called the minuend) is \(-y \times log(p) \) which means that we need to get a version of the targets array that is negated, and a version of the predictions array that is the natural log of the elements.</p>
<p>In code this looks as follows</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> minuend <span style="color:#f92672">=</span> targets.mapv(<span style="color:#f92672">|</span>elem<span style="color:#f92672">|</span> <span style="color:#f92672">-</span>elem) <span style="color:#f92672">*</span> predictions.mapv(<span style="color:#f92672">|</span>elem<span style="color:#f92672">|</span> elem.ln());</span></span></code></pre></div>
<p>The right hand side of the formula (called the subtrahend) is \((1 - y) \times log(1 - p)\) so the subtrahend looks as follows</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> subtrahend <span style="color:#f92672">=</span> targets.mapv(<span style="color:#f92672">|</span>elem<span style="color:#f92672">|</span> T::one() <span style="color:#f92672">-</span> elem) <span style="color:#f92672">*</span> predictions.mapv(<span style="color:#f92672">|</span>elem<span style="color:#f92672">|</span> (T::one() <span style="color:#f92672">-</span> elem).ln());</span></span></code></pre></div>
<p>Then we can perform the subtraction to get the loss <em>Array</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> result <span style="color:#f92672">=</span> minuend <span style="color:#f92672">-</span> subtrahend;</span></span></code></pre></div>
<p>Finally, the loss function is expected to return a single value representing the <em>total</em> loss over all predictions, so we need to sum the results to get the final loss value</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>result.sum()</span></span></code></pre></div>
<p>Putting all of this together we get the entire calculate_output function</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_output</span>(<span style="color:#f92672">&amp;</span>self, predictions: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, targets: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">T</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">let</span> (predictions, targets) <span style="color:#f92672">=</span>
</span></span><span style="display:flex;"><span>        calculate_softmax_predictions_and_targets((<span style="color:#f92672">*</span>predictions).clone(), (<span style="color:#f92672">*</span>targets).clone());
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">let</span> minuend <span style="color:#f92672">=</span> targets.mapv(<span style="color:#f92672">|</span>elem<span style="color:#f92672">|</span> <span style="color:#f92672">-</span>elem) <span style="color:#f92672">*</span> predictions.mapv(<span style="color:#f92672">|</span>elem<span style="color:#f92672">|</span> elem.ln());
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">let</span> subtrahend <span style="color:#f92672">=</span>
</span></span><span style="display:flex;"><span>        targets.mapv(<span style="color:#f92672">|</span>elem<span style="color:#f92672">|</span> T::one() <span style="color:#f92672">-</span> elem) <span style="color:#f92672">*</span> predictions.mapv(<span style="color:#f92672">|</span>elem<span style="color:#f92672">|</span> (T::one() <span style="color:#f92672">-</span> elem).ln());
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">let</span> result <span style="color:#f92672">=</span> minuend <span style="color:#f92672">-</span> subtrahend;
</span></span><span style="display:flex;"><span>    result.sum()
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h4 id="lossimplcalculate_input_gradient">LossImpl::calculate_input_gradient<a hidden class="anchor" aria-hidden="true" href="#lossimplcalculate_input_gradient">#</a></h4>
<p>The second required function is to be able to calculate the input gradients during the backward pass. For this we firstly need to know whether this is a single class problem or not, this is because when we calculate the softmax predictions on a single class problem, we are converting it to a dual class problem.</p>
<p>Because of this, the gradients produced will also be of a dual class nature, so before propagating back through the network, this extra dummy column we added needs to be stripped off.</p>
<p>We can cache whether it&rsquo;s a single class Array by simply checking the number of columns in it is 1</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> is_single_class <span style="color:#f92672">=</span> predictions.ncols() <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>;</span></span></code></pre></div>
<p>We need to go ahead and recalculate the predictions because we aren&rsquo;t retaining any state in our vtable trait objects, but won&rsquo;t show that here.</p>
<p>Recalling that calculating the gradient for the cross entropy loss function is simply the result of subtracting the targets Array from the calculated predictions, we can implement it as such</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> gradient <span style="color:#f92672">=</span> predictions <span style="color:#f92672">-</span> targets;</span></span></code></pre></div>
<p>The last step is to strip away that dummy column we added if the problem is a single class problem, which we can do through a helper function</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">if</span> is_single_class {
</span></span><span style="display:flex;"><span>    dual_class_to_single(gradient)
</span></span><span style="display:flex;"><span>} <span style="color:#66d9ef">else</span> {
</span></span><span style="display:flex;"><span>    gradient
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>Putting all of this together gives us the complete implementation for our gradient calculation function</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_input_gradient</span>(
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&amp;</span>self,
</span></span><span style="display:flex;"><span>    predictions: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    targets: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">let</span> is_single_class <span style="color:#f92672">=</span> predictions.ncols() <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">let</span> (predictions, targets) <span style="color:#f92672">=</span>
</span></span><span style="display:flex;"><span>        calculate_softmax_predictions_and_targets((<span style="color:#f92672">*</span>predictions).clone(), (<span style="color:#f92672">*</span>targets).clone());
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">let</span> gradient <span style="color:#f92672">=</span> predictions <span style="color:#f92672">-</span> targets;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> is_single_class {
</span></span><span style="display:flex;"><span>        dual_class_to_single(gradient)
</span></span><span style="display:flex;"><span>    } <span style="color:#66d9ef">else</span> {
</span></span><span style="display:flex;"><span>        gradient
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h4 id="calculate_softmax_predictions_and_targets-helper">calculate_softmax_predictions_and_targets (helper)<a hidden class="anchor" aria-hidden="true" href="#calculate_softmax_predictions_and_targets-helper">#</a></h4>
<p>As a reminder, we want this helper function to prepare the predictions and targets for our cross entropy loss calculation, by doing the following:</p>
<ol>
<li>Adding a dummy column if it&rsquo;s a single class Array so that softmax can work with a dual class Array</li>
<li>Run the predictions through the softmax function to turn them into probabilities</li>
<li>Clamp the probabilities so that they don&rsquo;t ever hit 0 or 1 due to asymptotic calculations when we use log during the cross entropy loss calculation</li>
</ol>
<p>The first step to this is easy enough, but we will delegate this to a helper function that we will cover later. For calling it, we will simply check if the number of columns in the predictions is 1, and if it is then we will update <strong>both</strong> predictions and targets to have the additional column</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">if</span> predictions.ncols() <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span> {
</span></span><span style="display:flex;"><span>    predictions <span style="color:#f92672">=</span> single_class_to_dual(predictions);
</span></span><span style="display:flex;"><span>    targets <span style="color:#f92672">=</span> single_class_to_dual(targets);
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>Then we apply the softmax function on the predictions which is done with a simple call, again we&rsquo;ll defer covering the softmax function until later</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> predictions <span style="color:#f92672">=</span> softmax(predictions)</span></span></code></pre></div>
<p>For the third point, clamping the probabilities, we can do this by using a method called &ldquo;clamp&rdquo;  from the num_traits library. This function works with any T where T implements the Float trait. Conveniently, Float also gives us an associated method &ldquo;epsilon&rdquo; which gives us a small epsilon value to clamp with.</p>
<p>We can map the predictions in place, and for each element of the predictions array, perform the clamp. This looks as follows</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>predictions.mapv_inplace(<span style="color:#f92672">|</span>elem<span style="color:#f92672">|</span> num_traits::clamp(elem, T::epsilon(), T::one() <span style="color:#f92672">-</span> T::epsilon()));</span></span></code></pre></div>
<p>Finally we&rsquo;re able to return the predictions and targets. As an entire function it looks as follows</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_softmax_predictions_and_targets</span><span style="color:#f92672">&lt;</span>T: <span style="color:#a6e22e">Float</span><span style="color:#f92672">&gt;</span>(
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">mut</span> predictions: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">mut</span> targets: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>) -&gt; (Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> predictions.ncols() <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span> {
</span></span><span style="display:flex;"><span>        predictions <span style="color:#f92672">=</span> single_class_to_dual(predictions);
</span></span><span style="display:flex;"><span>        targets <span style="color:#f92672">=</span> single_class_to_dual(targets);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> predictions <span style="color:#f92672">=</span> softmax(predictions);
</span></span><span style="display:flex;"><span>    predictions.mapv_inplace(<span style="color:#f92672">|</span>elem<span style="color:#f92672">|</span> num_traits::clamp(elem, T::epsilon(), T::one() <span style="color:#f92672">-</span> T::epsilon()));
</span></span><span style="display:flex;"><span>    (predictions, targets)
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h4 id="softmax-1">softmax<a hidden class="anchor" aria-hidden="true" href="#softmax-1">#</a></h4>
<p>This function will be responsible for applying softmax to the Array of predictions. We actually will need to apply this independently on each <strong>row</strong></p>
<p>We know that in softmax calculation, we are working with values of \(e^x\) and don&rsquo;t need to keep the original values around. Therefore the first step should be to prepare the input array by calculating each element to be e raised to that power as such</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>arr.map_inplace(<span style="color:#f92672">|</span>elem<span style="color:#f92672">|</span> <span style="color:#f92672">*</span>elem <span style="color:#f92672">=</span> elem.exp());</span></span></code></pre></div>
<p>ndarray::Array contains a convenient function called <strong>map_axis</strong> which takes an axis to map over, and a function. The function will take an entire slice of that axis at a particular index, and should return a single value. This results in an Array that is one dimension less.</p>
<p>For our purposes we will iterate in the direction of the <strong>columns</strong> which is Axis(1), and the function will therefore accept one <strong>row</strong> at a time. We just sum the row to get the total of all features in that row.</p>
<p>This will give us a 1 dimensional array, but for the division we will be doing, to ensure that the broadcasting works correctly, we need to reshape this into a 2 dimensional array, with a single column. Therefore each entry in the column vector will be a total for the associated row in the input matrix.</p>
<p>We can use the into_shape function to reshape the totals, but we need to unwrap the Result as if the number of elements in the input don&rsquo;t match the requested dimensions it will give an error. In this case we know it has to be correct so we can unwrap it safely.</p>
<p>This looks as follows</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> totals <span style="color:#f92672">=</span> arr
</span></span><span style="display:flex;"><span>    .map_axis(Axis(<span style="color:#ae81ff">1</span>), <span style="color:#f92672">|</span>row<span style="color:#f92672">|</span> row.sum())
</span></span><span style="display:flex;"><span>    .into_shape((arr.nrows(), <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>    .unwrap();</span></span></code></pre></div>
<p>Finally we can divide the array by the totals. Due to the way operator broadcasting works, this will broadcast the division of a multi-column vector by a single column one such that each element in a given row is divided by the associated element in the divisor.</p>
<p>For clarity, this means that</p>
<p>$$ \begin{bmatrix} x_1 &amp; x_2 &amp; x_3 \\ y_1 &amp; y_2 &amp; y_3 \end{bmatrix} / \begin{bmatrix} z_1 \\ z_2 \\  \end{bmatrix} = \begin{bmatrix} x_1/z_1 &amp; x_2/z_1 &amp; x_3/z_1 \\ y_1/z_2 &amp; y_2/z_2 &amp; y_3 /z_2  \end{bmatrix} $$</p>
<p>The total function is then as follows (note the Float trait bound so we have access to the exp function and also to arithmetical operators)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">softmax</span><span style="color:#f92672">&lt;</span>T: <span style="color:#a6e22e">Float</span><span style="color:#f92672">&gt;</span>(<span style="color:#66d9ef">mut</span> arr: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    arr.map_inplace(<span style="color:#f92672">|</span>elem<span style="color:#f92672">|</span> <span style="color:#f92672">*</span>elem <span style="color:#f92672">=</span> elem.exp());
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">let</span> totals <span style="color:#f92672">=</span> arr
</span></span><span style="display:flex;"><span>        .map_axis(Axis(<span style="color:#ae81ff">1</span>), <span style="color:#f92672">|</span>row<span style="color:#f92672">|</span> row.sum())
</span></span><span style="display:flex;"><span>        .into_shape((arr.nrows(), <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>        .unwrap();
</span></span><span style="display:flex;"><span>    arr <span style="color:#f92672">/</span> totals
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h4 id="single_class_to_dual">single_class_to_dual<a hidden class="anchor" aria-hidden="true" href="#single_class_to_dual">#</a></h4>
<p>This is the first of the two &ldquo;mapping&rdquo; functions we use to convert a single class problem into a dual class one for the purposes of being able to apply softmax and cross entropy appropriately.</p>
<p>This one takes the initial predictions array and adds an additional feature/column onto it representing the inverse probabilities, or the probabilities of <strong>not</strong> being in the single class.</p>
<p>To calculate this, we will iterate over the elements of the input array (which we validate is a single column so each element is a prediction for a row), we will then use a <strong>flat_map</strong> to map this single element onto an iterator that produces both the element, and then the inverse of the element (1 - element). We can then take this iterator and use Array::from_iter to convert it into a 1-D array.</p>
<p>However, all of the code expects the array to be a 2-D array, so we must reshape it to be as such.</p>
<p>The function must also have appropriate bounds on &ldquo;T&rdquo; to be able to use the various functions. These bounds will be:</p>
<ol>
<li>Clone - We need to be able to clone elements in the predictions array so as to be able to emit it, and also use it to calculate the value in the additional column</li>
<li>One - We need this trait to be able to get &ldquo;one&rdquo; of a &ldquo;T&rdquo; as we&rsquo;ll need it to calculate the value in the second column</li>
<li>Sub&lt;Output=T&gt; - This is used also in calculating the value for the new column as we need to be able to subtract the prediction (a &ldquo;T&rdquo;) from our &ldquo;one&rdquo; (also a &ldquo;T&rdquo;)</li>
</ol>
<p>Overall the function looks as follows</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">single_class_to_dual</span><span style="color:#f92672">&lt;</span>T: Clone <span style="color:#f92672">+</span> One <span style="color:#f92672">+</span> Sub<span style="color:#f92672">&lt;</span>Output <span style="color:#f92672">=</span> T<span style="color:#f92672">&gt;&gt;</span>(input: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    assert_eq!(input.ncols(), <span style="color:#ae81ff">1</span>);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">let</span> rows <span style="color:#f92672">=</span> input.nrows();
</span></span><span style="display:flex;"><span>    Array::from_iter(input.iter().flat_map(<span style="color:#f92672">|</span>elem<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>        std::iter::once((<span style="color:#f92672">*</span>elem).clone()).chain(std::iter::once(T::one() <span style="color:#f92672">-</span> (<span style="color:#f92672">*</span>elem).clone()))
</span></span><span style="display:flex;"><span>    }))
</span></span><span style="display:flex;"><span>    .into_shape((rows, <span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>    .unwrap()
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h4 id="dual_class_to_single">dual_class_to_single<a hidden class="anchor" aria-hidden="true" href="#dual_class_to_single">#</a></h4>
<p>The final function will go the other way which is to take a 2-D array with 2 columns and strip the last column off, retaining only the first one.</p>
<p>We can do this with the <strong>select</strong> function provided by ndarray::Array which takes the Axis to select from, and the indices to retain.</p>
<p>In our case, we are selecting from the columns of the Array which is Axis(1), and we only need select index 0.</p>
<p>The total function is then pretty simple</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">dual_class_to_single</span><span style="color:#f92672">&lt;</span>T: Clone<span style="color:#f92672">&gt;</span>(input: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    assert_eq!(input.ncols(), <span style="color:#ae81ff">2</span>);
</span></span><span style="display:flex;"><span>    input.select(Axis(<span style="color:#ae81ff">1</span>), <span style="color:#f92672">&amp;</span>[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h3 id="performance">Performance<a hidden class="anchor" aria-hidden="true" href="#performance">#</a></h3>
<p>Now that we&rsquo;ve implemented the Tanh activation function previously (in a previous post), and now the softmax cross entropy loss function, we can test on the sample data set/problem and see how accurate the various approaches are.</p>
<p>Firstly we have a linear regression, which is simply a single dense layer with 1 neuron, and a <strong>linear</strong> activation function. For the loss function we use MeanSquaredError, so the code of the network setup looks as follows</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> network <span style="color:#f92672">=</span> Network::new(
</span></span><span style="display:flex;"><span>    vec![Layer::new_with_seed(
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>        DenseLayerSetup::new_boxed(Linear::new_boxed()),
</span></span><span style="display:flex;"><span>        SEED,
</span></span><span style="display:flex;"><span>    )],
</span></span><span style="display:flex;"><span>    Loss::new(MeanSquaredError::new_boxed()),
</span></span><span style="display:flex;"><span>);</span></span></code></pre></div>
<p>And running the example shows that the linear regression in fact only has a small percentage chance of correctly guessing the right answer</p>
<pre tabindex="0"><code>Accuracy (training): 16.652%
Accuracy (testing): 16.796%
</code></pre><p>Secondly we try out a basic neural network. For this, we will have two dense layers. The first dense layer will use the <strong>Tanh</strong> activation function and will have multiple neurons (we choose 54 here). The second layer will have a single neuron with a linear activation function and will serve to bring together the outputs from the hidden layer into a single output for use with the loss function.</p>
<p>Again, we use a basic MeanSquaredError loss function so the setup code looks as follows</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> network <span style="color:#f92672">=</span> Network::new(
</span></span><span style="display:flex;"><span>    vec![
</span></span><span style="display:flex;"><span>        Layer::new_with_seed(<span style="color:#ae81ff">54</span>, DenseLayerSetup::new_boxed(Tanh::new_boxed()), SEED),
</span></span><span style="display:flex;"><span>        Layer::new_with_seed(<span style="color:#ae81ff">1</span>, DenseLayerSetup::new_boxed(Linear::new_boxed()), SEED),
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    Loss::new(MeanSquaredError::new_boxed()),
</span></span><span style="display:flex;"><span>);</span></span></code></pre></div>
<p>Surprisingly, this function results in <strong>exactly</strong> the same predictions as the linear regression, which is to say using a basic neural network with a MeanSquaredError loss function is resulting in a very low accuracy. It&rsquo;s the same figure as shown by the following output.</p>
<pre tabindex="0"><code>Accuracy (training): 16.652%
Accuracy (testing): 16.796%
</code></pre><p>This at least indicates that our problem can at least be <em>somewhat</em> solved by a basic line in linear regression, however it&rsquo;s surprising that a more complex network is not able to get any more accurate. This could be an issue with the MeanSquaredError.</p>
<p>Finally, we make <strong>only one</strong> small change to the basic neural network, which is to swap the MeanSquaredError loss function for our new SoftmaxCrossEntropy loss function. This is the only change to the setup as shown by the following code</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> network <span style="color:#f92672">=</span> Network::new(
</span></span><span style="display:flex;"><span>    vec![
</span></span><span style="display:flex;"><span>        Layer::new_with_seed(<span style="color:#ae81ff">54</span>, DenseLayerSetup::new_boxed(Tanh::new_boxed()), SEED),
</span></span><span style="display:flex;"><span>        Layer::new_with_seed(<span style="color:#ae81ff">1</span>, DenseLayerSetup::new_boxed(Linear::new_boxed()), SEED),
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    Loss::new(SoftmaxCrossEntropy::new_boxed()),
</span></span><span style="display:flex;"><span>);</span></span></code></pre></div>
<p>However, the accuracy on our problem data has shot right up!</p>
<pre tabindex="0"><code>Accuracy (training): 91.786%
Accuracy (testing): 91.742%
</code></pre><p>This indicates that the steeper gradients and harsher loss penalties given by softmax and cross entropy loss respectively are resulting in a <strong>much</strong> better fit after training the network. Thus we can conclude that this loss function is much better for classification type problems.</p>


  </div>

  <footer class="post-footer">
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://forgottenmaster.github.io/">Robin Firth</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
