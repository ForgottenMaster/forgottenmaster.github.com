<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>4.4 - Momentum | Robin Firth</title>
<meta name="keywords" content="" />
<meta name="description" content="So far, when we have updated the weights on our parameters with their gradients, it has been an instantaneous update without any temporal relationship with previous optimisation steps.
This means that between two successive epochs, the velocity change can be instantaneous. One epoch the velocity could be low, and the next it might be super high.
In terms of the updating of the weights with respect to the gradients of parameters, we can imagine this from the graph visualisation as being the object moving up and down the line of the graph, or bouncing around the troughs and valleys created by such a graph (there was a diagram in a previous post showing the arrows bouncing around like that).">
<meta name="author" content="">
<link rel="canonical" href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/momentum/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css" integrity="sha256-b2AFbUTT9&#43;tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js" integrity="sha256-uVus3DnjejMqn4g7Hni&#43;Srwf3KK8HyZB9V4809q9TWE="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://forgottenmaster.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://forgottenmaster.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://forgottenmaster.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://forgottenmaster.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://forgottenmaster.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.110.0">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="4.4 - Momentum" />
<meta property="og:description" content="So far, when we have updated the weights on our parameters with their gradients, it has been an instantaneous update without any temporal relationship with previous optimisation steps.
This means that between two successive epochs, the velocity change can be instantaneous. One epoch the velocity could be low, and the next it might be super high.
In terms of the updating of the weights with respect to the gradients of parameters, we can imagine this from the graph visualisation as being the object moving up and down the line of the graph, or bouncing around the troughs and valleys created by such a graph (there was a diagram in a previous post showing the arrows bouncing around like that)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/momentum/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-05T19:13:16&#43;01:00" />
<meta property="article:modified_time" content="2022-05-05T19:13:16&#43;01:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="4.4 - Momentum"/>
<meta name="twitter:description" content="So far, when we have updated the weights on our parameters with their gradients, it has been an instantaneous update without any temporal relationship with previous optimisation steps.
This means that between two successive epochs, the velocity change can be instantaneous. One epoch the velocity could be low, and the next it might be super high.
In terms of the updating of the weights with respect to the gradients of parameters, we can imagine this from the graph visualisation as being the object moving up and down the line of the graph, or bouncing around the troughs and valleys created by such a graph (there was a diagram in a previous post showing the arrows bouncing around like that)."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://forgottenmaster.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Machine Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Deep Learning From Scratch",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Chapter 4 - Extensions",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/"
    }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "4.4 - Momentum",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/momentum/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "4.4 - Momentum",
  "name": "4.4 - Momentum",
  "description": "So far, when we have updated the weights on our parameters with their gradients, it has been an instantaneous update without any temporal relationship with previous optimisation steps.\nThis means that between two successive epochs, the velocity change can be instantaneous. One epoch the velocity could be low, and the next it might be super high.\nIn terms of the updating of the weights with respect to the gradients of parameters, we can imagine this from the graph visualisation as being the object moving up and down the line of the graph, or bouncing around the troughs and valleys created by such a graph (there was a diagram in a previous post showing the arrows bouncing around like that).",
  "keywords": [
    
  ],
  "articleBody": "So far, when we have updated the weights on our parameters with their gradients, it has been an instantaneous update without any temporal relationship with previous optimisation steps.\nThis means that between two successive epochs, the velocity change can be instantaneous. One epoch the velocity could be low, and the next it might be super high.\nIn terms of the updating of the weights with respect to the gradients of parameters, we can imagine this from the graph visualisation as being the object moving up and down the line of the graph, or bouncing around the troughs and valleys created by such a graph (there was a diagram in a previous post showing the arrows bouncing around like that).\nHowever, in real life, objects don’t suddenly change direction on an instant and instead if the velocity (gradient) stops or becomes 0, the object will slow down over time before coming to a stop.\nWe can simulate this momentum in our neural network optimisation also.\nThe rationale is that when we reach a trough in the graph, and the gradient is 0, the weights will still be updated for a time (although the rate will slow on successive iterations). This preservation of momentum can allow the weights to escape a false, or local minima.\nFormula What this means in practice is that the amount we use to update the weight of each parameter (the velocity) isn’t just a function of the gradient at that point, and the learning rate but instead is a weighted average of the velocities at previous timesteps with the weights decaying exponentially.\nThe formula to calculate this sum is pretty simple and is defined as:\n$$ update = \\nabla_t + (\\mu \\times \\nabla_{t-1}) + (\\mu^2 \\times \\nabla_{t-2}) + … $$\nWhere \\(\\mu\\) is the momentum parameter, or, how much the velocity from the previous timestep is degraded.\nFor example if the momentum parameter is 0.9, then in a given time step, we multiply the velocity from the previous timestep by 0.9, or 90%, the one from two timesteps ago by \\(0.9^2\\) (0.81), the one from three time steps ago by \\(0.9^3\\) (0.729), and so on. We then add them all up to get the total velocity for the current time step.\nHowever in practice, we won’t compute the entire history each update, as we’ll soon see.\nAlgorithm In order to implement the momentum, we can’t be computing the entire historical sum every update, so we should find a better way to calculate what the velocity should be.\nAs it turns out, we can just do the following steps to update the velocity with momentum, and the current gradient:\nMultiply the velocity by the momentum parameter (0.9 in the above example) Add the current gradient in As an example, this is what the stored velocity would represent at each timestep, starting at t=1:\n\\(\\nabla_1\\) \\(\\nabla_2 + (\\mu \\times \\nabla_1)\\) \\(\\nabla_3 + (\\mu \\times \\nabla_2) + (\\mu^2 \\times \\nabla_1)\\) And so on, all with us only needing to store a single quantity and not a history of previous quantities.\nImplementation To implement this, we will be create a new implementor of the Optimiser trait, so far we have only had the basic Stochastic Gradient Descent (SGD) optimiser.\nWe will still be following the same basic algorithm of SGD and updating the weight with a proportion of the gradient, however we will want to incorporate momentum into the calculation of velocity.\nWe call this new optimiser SGDMomentum\nSDGMomentum The structure itself is a bit more than with the regular SGD optimiser. The regular one only had a learning rate, which was the proportion of the gradient to update the weights with.\nThis new optimiser needs two additional pieces of data stored to retain state between update steps:\nvelocities - This will be a Vec of Arrays with the elements in the same order as they appear in the iterator returned by the parameters_and_gradients() function on Network. Thus, the velocity at index i will be the velocity for the parameter/weight at index i. momentum - This is a simple parameter like learning rate which represents the multiplier we will update the current velocity with before updating it with the current gradient. Represents the degredation of velocity when no forces are acting on it (slowing down). One thing to note is that we can only calculate the velocities when there is a valid iterator of parameter weights and gradients from the network, and this is only valid after one forward and backward pass have been performed.\nThis means that when we create the optimiser the vector will be uninitialised and we will need to initialise it the first time we’re optimising. We will therefore use Option to represent the None state initially, and it will be replaced with Some once the velocities are initialised.\nThe whole definition of our structure looks as follows\npub struct SGDMomentum\u003cT\u003e { velocities: Option\u003cVec\u003cArray\u003cT, Ix2\u003e\u003e\u003e, learning_rate: T, momentum: T, } Construction In order to construct a new instance of the Optimiser we require both a learning rate, and a momentum parameter. The velocities as mentioned previously are initialised to None by default until the first step is performed.\nThe “new” function we add to handle the creation of a new instance looks as follows\nimpl\u003cT\u003e SGDMomentum\u003cT\u003e { pub fn new(learning_rate: T, momentum: T) -\u003e Self { Self { velocities: None, learning_rate, momentum, } } } Optimiser Now we just need to implement the Optimiser trait for SGDMomentum, which requires only a single function, step. Step operates on a mutable reference to self, to allow us to retain state between calls (which is ideal because that’s precisely what we’re doing with velocities), and it takes a mutable reference to the Network to optimise.\nThe very first thing we will want to do is to check if the velocities list has been initialised (is some). If it’s still None then this is the first time we’ve had the step function called, and we need to initialise one velocities array for each parameter weight entry, and in the same order.\nEach velocity Array will have the same shape as the associated network parameter weight, since we will be performing an elementwise subtraction when we update the weights. The elements will start at all zeroes initially as the velocity is “at rest”.\nIn order to check if we need to initialise the velocities, we can use the following function provided by Option\nif self.velocities.is_none() { // initialisation code } What we want to do to create the Vec of velocities is take the iterator provided by the network that iterates over all the parameter weights in the network (provided by the parameters_and_gradients function), and for each element of that iterator, we want to produce an Array of the same shape that’s all zeroes.\nThere is a method provided to us by the Iterator trait for this which is the map function. Finally, we can take that new iterator and use the collect function to allocate a Vec and collect the iterator into it.\nThe code for that looks as follows:\nnet.parameters_and_gradients() .map(|(param, _)| Array::zeros(param.raw_dim())) .collect() Then we can do the actual update. For this we will once again want to iterate over the parameters and gradients in the network, but we want to do this in tandem with their current velocities.\nAnother provided method by the Iterator trait is zip which is called on an iterator, takes another iterator and produces a new iterator whose elements are a tuple of the elements from each.\nTherefore each element in the zipped iterator will be a tuple containing the parameter/gradient, and the associated velocity for that parameter.\nFinally we want to run “some code” for each element in the zipped iterator, which we can do with the for_each method.\nThis section of code then looks as follows:\nnet.parameters_and_gradients() .zip(self.velocities.as_mut().unwrap().iter_mut()) .for_each(|((parameter, gradient), velocity)| { // do stuff here }); The code we put inside the callback to for_each will be the code to update one parameter, given its velocity.\nWe can do this elementwise by zipping the weight, gradient, and velocity arrays together which will let us iterate over their elements in an arbitrary order, but in lockstep with one another.\nndarray gives us a macro to perform this iteration, called azip (probably for Array Zip).\nThe way this macro is called is you provide a set of specifiers and arrays from which to draw the values for each specifier, in a syntax like a parameter list. The code to run for each set of elements goes in curly braces. For example, the following snippet:\nazip!((parameter in parameter, gradient in gradient, velocity in velocity) { // do stuff here }); will iterate over the elements of the parameter, gradient, and velocity arrays in lockstep with each other, and for each element will bind the appropriate element of parameter to the binding “parameter”, the appropriate element of gradient to the binding “gradient”, and velocity to “velocity”. This is a feature in Rust called shadowing and is useful because it means that inside the function, we can’t accidentally use them as arrays and are limited to only operating on the individual elements provided for that iteration.\nInside the function body, to update a single element we want to do the following sequence of steps:\nMultiply the current velocity by momentum Add the correct proportion of the current gradient to the velocity (using learning rate) Update the parameter weight by the new velocity We can use standard arithmetic operations for these, however we need to explicitly clone the various parameters since we’re in a generic context and only require Clone, not Copy as a trait bound. The resulting function body is then:\n*velocity = (*velocity).clone() * momentum.clone() + (*gradient).clone() * learning_rate.clone(); *parameter -= (*velocity).clone(); Note the use of the dereference (*) operator, this is because we’re iterating over references to arrays, meaning the elements we’re iterating over are also references. We need to dereference them whenever we want to access the value (like to update the parameter, or velocity).\nIn order to support these operations, the following trait bounds are required on the element type T\nClone - Required because as mentioned before, we need to reuse things like the learning rate and momentum, and also arithmetic operators take operands by value (which we can’t do in generics out of a reference, can’t move from a reference) Mul",
  "wordCount" : "2167",
  "inLanguage": "en",
  "datePublished": "2022-05-05T19:13:16+01:00",
  "dateModified": "2022-05-05T19:13:16+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/momentum/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Robin Firth",
    "logo": {
      "@type": "ImageObject",
      "url": "https://forgottenmaster.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://forgottenmaster.github.io/" accesskey="h" title="Robin Firth (Alt + H)">Robin Firth</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://forgottenmaster.github.io/resume/" title="Résumé">
                    <span>Résumé</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://forgottenmaster.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/">Posts</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/">Machine Learning</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/">Deep Learning From Scratch</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/">Chapter 4 - Extensions</a></div>
    <h1 class="post-title">
      4.4 - Momentum
    </h1>
    <div class="post-meta"><span title='2022-05-05 19:13:16 +0100 BST'>May 5, 2022</span>&nbsp;·&nbsp;11 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#formula" aria-label="Formula">Formula</a></li>
                <li>
                    <a href="#algorithm" aria-label="Algorithm">Algorithm</a></li>
                <li>
                    <a href="#implementation" aria-label="Implementation">Implementation</a><ul>
                        
                <li>
                    <a href="#sdgmomentum" aria-label="SDGMomentum">SDGMomentum</a></li>
                <li>
                    <a href="#construction" aria-label="Construction">Construction</a></li>
                <li>
                    <a href="#optimisert" aria-label="Optimiser">Optimiser<!-- raw HTML omitted --></a></li></ul>
                </li>
                <li>
                    <a href="#testing" aria-label="Testing">Testing</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>So far, when we have updated the weights on our parameters with their gradients, it has been an instantaneous update without any temporal relationship with previous optimisation steps.</p>
<p>This means that between two successive epochs, the velocity change can be instantaneous. One epoch the velocity could be low, and the next it might be super high.</p>
<p>In terms of the updating of the weights with respect to the gradients of parameters, we can imagine this from the graph visualisation as being the object moving up and down the line of the graph, or bouncing around the troughs and valleys created by such a graph (there was a diagram in a previous post showing the arrows bouncing around like that).</p>
<p>However, in real life, objects don&rsquo;t suddenly change direction on an instant and instead if the velocity (gradient) stops or becomes 0, the object will slow down over time before coming to a stop.</p>
<p>We can simulate this <em>momentum</em> in our neural network optimisation also.</p>
<p>The rationale is that when we reach a trough in the graph, and the gradient is 0, the weights will still be updated for a time (although the rate will slow on successive iterations). This preservation of momentum can allow the weights to escape a false, or local minima.</p>
<h3 id="formula">Formula<a hidden class="anchor" aria-hidden="true" href="#formula">#</a></h3>
<p>What this means in practice is that the amount we use to update the weight of each parameter (the velocity) isn&rsquo;t just a function of the gradient <em>at that point</em>, and the learning rate but instead is a weighted average of the velocities at previous timesteps with the weights decaying exponentially.</p>
<p>The formula to calculate this sum is pretty simple and is defined as:</p>
<p>$$ update = \nabla_t + (\mu \times \nabla_{t-1}) + (\mu^2 \times \nabla_{t-2}) + &hellip; $$</p>
<p>Where \(\mu\) is the momentum parameter, or, how much the velocity from the previous timestep is degraded.</p>
<p>For example if the momentum parameter is 0.9, then in a given time step, we multiply the velocity from the previous timestep by 0.9, or 90%, the one from two timesteps ago by \(0.9^2\) (0.81), the one from three time steps ago by \(0.9^3\) (0.729), and so on. We then add them all up to get the total velocity for the current time step.</p>
<p>However in practice, we won&rsquo;t compute the entire history each update, as we&rsquo;ll soon see.</p>
<h3 id="algorithm">Algorithm<a hidden class="anchor" aria-hidden="true" href="#algorithm">#</a></h3>
<p>In order to implement the momentum, we can&rsquo;t be computing the entire historical sum every update, so we should find a better way to calculate what the velocity should be.</p>
<p>As it turns out, we can just do the following steps to update the velocity with momentum, and the current gradient:</p>
<ol>
<li>Multiply the velocity by the momentum parameter (0.9 in the above example)</li>
<li>Add the current gradient in</li>
</ol>
<p>As an example, this is what the stored velocity would represent at each timestep, starting at t=1:</p>
<ol>
<li>\(\nabla_1\)</li>
<li>\(\nabla_2 + (\mu \times \nabla_1)\)</li>
<li>\(\nabla_3 + (\mu \times \nabla_2) + (\mu^2 \times \nabla_1)\)</li>
</ol>
<p>And so on, all with us only needing to store a single quantity and not a history of previous quantities.</p>
<h3 id="implementation">Implementation<a hidden class="anchor" aria-hidden="true" href="#implementation">#</a></h3>
<p>To implement this, we will be create a new implementor of the <strong>Optimiser</strong> trait, so far we have only had the basic Stochastic Gradient Descent (SGD) optimiser.</p>
<p>We will still be following the same basic algorithm of SGD and updating the weight with a proportion of the gradient, however we will want to incorporate momentum into the calculation of velocity.</p>
<p>We call this new optimiser <strong>SGDMomentum</strong></p>
<h4 id="sdgmomentum">SDGMomentum<a hidden class="anchor" aria-hidden="true" href="#sdgmomentum">#</a></h4>
<p>The structure itself is a bit more than with the regular SGD optimiser. The regular one only had a learning rate, which was the proportion of the gradient to update the weights with.</p>
<p>This new optimiser needs two additional pieces of data stored to retain state between update steps:</p>
<ol>
<li><strong>velocities</strong> - This will be a Vec of Arrays with the elements in the <strong>same order</strong> as they appear in the iterator returned by the parameters_and_gradients() function on Network. Thus, the velocity at index i will be the velocity for the parameter/weight at index i.</li>
<li><strong>momentum</strong> - This is a simple parameter like learning rate which represents the multiplier we will update the current velocity with before updating it with the current gradient. Represents the degredation of velocity when no forces are acting on it (slowing down).</li>
</ol>
<p>One thing to note is that we can only calculate the velocities when there is a valid iterator of parameter weights and gradients from the network, and this is only valid after one forward and backward pass have been performed.</p>
<p>This means that when we create the optimiser the vector will be uninitialised and we will need to initialise it the first time we&rsquo;re optimising. We will therefore use Option to represent the None state initially, and it will be replaced with Some once the velocities are initialised.</p>
<p>The whole definition of our structure looks as follows</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">SGDMomentum</span><span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    velocities: Option<span style="color:#f92672">&lt;</span>Vec<span style="color:#f92672">&lt;</span>Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;&gt;&gt;</span>,
</span></span><span style="display:flex;"><span>    learning_rate: <span style="color:#a6e22e">T</span>,
</span></span><span style="display:flex;"><span>    momentum: <span style="color:#a6e22e">T</span>,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h4 id="construction">Construction<a hidden class="anchor" aria-hidden="true" href="#construction">#</a></h4>
<p>In order to construct a new instance of the Optimiser we <strong>require</strong> both a learning rate, and a momentum parameter. The velocities as mentioned previously are initialised to None by default until the first step is performed.</p>
<p>The &ldquo;new&rdquo; function we add to handle the creation of a new instance looks as follows</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> SGDMomentum<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">new</span>(learning_rate: <span style="color:#a6e22e">T</span>, momentum: <span style="color:#a6e22e">T</span>) -&gt; <span style="color:#a6e22e">Self</span> {
</span></span><span style="display:flex;"><span>        Self {
</span></span><span style="display:flex;"><span>            velocities: None,
</span></span><span style="display:flex;"><span>            learning_rate,
</span></span><span style="display:flex;"><span>            momentum,
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h4 id="optimisert">Optimiser<!-- raw HTML omitted --><a hidden class="anchor" aria-hidden="true" href="#optimisert">#</a></h4>
<p>Now we just need to implement the Optimiser<!-- raw HTML omitted --> trait for SGDMomentum<!-- raw HTML omitted -->, which requires only a single function, <strong>step</strong>. Step operates on a mutable reference to self, to allow us to retain state between calls (which is ideal because that&rsquo;s precisely what we&rsquo;re doing with velocities), and it takes a mutable reference to the Network to optimise.</p>
<p>The very first thing we will want to do is to check if the velocities list has been initialised (is some). If it&rsquo;s still None then this is the first time we&rsquo;ve had the step function called, and we need to initialise one velocities array for each parameter weight entry, and in the same order.</p>
<p>Each velocity Array will have the same shape as the associated network parameter weight, since we will be performing an elementwise subtraction when we update the weights. The elements will start at all zeroes initially as the velocity is &ldquo;at rest&rdquo;.</p>
<p>In order to check if we need to initialise the velocities, we can use the following function provided by Option</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">if</span> self.velocities.is_none() {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// initialisation code
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>}</span></span></code></pre></div>
<p>What we want to do to create the Vec of velocities is take the iterator provided by the network that iterates over all the parameter weights in the network (provided by the parameters_and_gradients function), and for each element of that iterator, we want to produce an Array of the same shape that&rsquo;s all zeroes.</p>
<p>There is a method provided to us by the Iterator trait for this which is the map function. Finally, we can take that new iterator and use the <strong>collect</strong> function to allocate a Vec and collect the iterator into it.</p>
<p>The code for that looks as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>net.parameters_and_gradients()
</span></span><span style="display:flex;"><span>    .map(<span style="color:#f92672">|</span>(param, _)<span style="color:#f92672">|</span> Array::zeros(param.raw_dim()))
</span></span><span style="display:flex;"><span>    .collect()</span></span></code></pre></div>
<p>Then we can do the actual update. For this we will once again want to iterate over the parameters and gradients in the network, but we want to do this in tandem with their current velocities.</p>
<p>Another provided method by the Iterator trait is <strong>zip</strong> which is called on an iterator, takes another iterator and produces a new iterator whose elements are a tuple of the elements from each.</p>
<p>Therefore each element in the zipped iterator will be a tuple containing the parameter/gradient, and the associated velocity for that parameter.</p>
<p>Finally we want to run &ldquo;some code&rdquo; for each element in the zipped iterator, which we can do with the for_each method.</p>
<p>This section of code then looks as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>net.parameters_and_gradients()
</span></span><span style="display:flex;"><span>    .zip(self.velocities.as_mut().unwrap().iter_mut())
</span></span><span style="display:flex;"><span>    .for_each(<span style="color:#f92672">|</span>((parameter, gradient), velocity)<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">// do stuff here
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    });</span></span></code></pre></div>
<p>The code we put inside the callback to for_each will be the code to update <strong>one</strong> parameter, given its velocity.</p>
<p>We can do this elementwise by zipping the weight, gradient, and velocity arrays together which will let us iterate over their elements in an arbitrary order, but in lockstep with one another.</p>
<p>ndarray gives us a macro to perform this iteration, called <strong>azip</strong> (probably for Array Zip).</p>
<p>The way this macro is called is you provide a set of specifiers and arrays from which to draw the values for each specifier, in a syntax like a parameter list. The code to run for each set of elements goes in curly braces. For example, the following snippet:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>azip<span style="color:#f92672">!</span>((parameter <span style="color:#66d9ef">in</span> parameter, gradient <span style="color:#66d9ef">in</span> gradient, velocity <span style="color:#66d9ef">in</span> velocity) {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// do stuff here
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>});</span></span></code></pre></div>
<p>will iterate over the elements of the parameter, gradient, and velocity arrays in lockstep with each other, and for each element will bind the appropriate element of parameter to the binding &ldquo;parameter&rdquo;, the appropriate element of gradient to the binding &ldquo;gradient&rdquo;, and velocity to &ldquo;velocity&rdquo;. This is a feature in Rust called <strong>shadowing</strong> and is useful because it means that inside the function, we can&rsquo;t accidentally use them as arrays and are limited to only operating on the individual elements provided for that iteration.</p>
<p>Inside the function body, to update a single element we want to do the following sequence of steps:</p>
<ol>
<li>Multiply the current velocity by momentum</li>
<li>Add the correct proportion of the current gradient to the velocity (using learning rate)</li>
<li>Update the parameter weight by the new velocity</li>
</ol>
<p>We can use standard arithmetic operations for these, however we need to explicitly clone the various parameters since we&rsquo;re in a generic context and only require Clone, not Copy as a trait bound. The resulting function body is then:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#f92672">*</span>velocity <span style="color:#f92672">=</span> (<span style="color:#f92672">*</span>velocity).clone() <span style="color:#f92672">*</span> momentum.clone() <span style="color:#f92672">+</span> (<span style="color:#f92672">*</span>gradient).clone() <span style="color:#f92672">*</span> learning_rate.clone();
</span></span><span style="display:flex;"><span><span style="color:#f92672">*</span>parameter <span style="color:#f92672">-=</span> (<span style="color:#f92672">*</span>velocity).clone();</span></span></code></pre></div>
<p>Note the use of the dereference (*) operator, this is because we&rsquo;re iterating over references to arrays, meaning the elements we&rsquo;re iterating over are also references. We need to dereference them whenever we want to access the value (like to update the parameter, or velocity).</p>
<p>In order to support these operations, the following trait bounds are required on the element type T</p>
<ol>
<li><strong>Clone</strong> - Required because as mentioned before, we need to reuse things like the learning rate and momentum, and also arithmetic operators take operands by value (which we can&rsquo;t do in generics out of a reference, can&rsquo;t move from a reference)</li>
<li><strong>Mul&lt;Output=T&gt;</strong> - This is required because we&rsquo;re multiplying a T by a T to produce a new T when we&rsquo;re updating the velocity</li>
<li><strong>SubAssign</strong> - Used when updating the parameter by velocity, we use the -= operator</li>
<li><strong>Zero</strong> - Required for initialising the velocities on the first update, initialised to all zeroes which requires we know what &ldquo;zero&rdquo; of a &ldquo;T&rdquo; means</li>
</ol>
<p>The code then in its entirety looks as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span>T: Clone <span style="color:#f92672">+</span> Mul<span style="color:#f92672">&lt;</span>Output <span style="color:#f92672">=</span> T<span style="color:#f92672">&gt;</span> <span style="color:#f92672">+</span> SubAssign <span style="color:#f92672">+</span> Zero<span style="color:#f92672">&gt;</span> Optimiser<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">for</span> SGDMomentum<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">step</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, net: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">mut</span> Network<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>) {
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">// if this is the first step/iteration, we should set up the velocities
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        <span style="color:#75715e">// at zeroes, but matching the dimensions of the parameters.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        <span style="color:#66d9ef">if</span> self.velocities.is_none() {
</span></span><span style="display:flex;"><span>            self.velocities <span style="color:#f92672">=</span> Some(
</span></span><span style="display:flex;"><span>                net.parameters_and_gradients()
</span></span><span style="display:flex;"><span>                    .map(<span style="color:#f92672">|</span>(param, _)<span style="color:#f92672">|</span> Array::zeros(param.raw_dim()))
</span></span><span style="display:flex;"><span>                    .collect(),
</span></span><span style="display:flex;"><span>            );
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">// weight updating can be done with the parameters and gradients iterator in lockstep
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        <span style="color:#75715e">// with the velocities.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        <span style="color:#66d9ef">let</span> momentum <span style="color:#f92672">=</span> self.momentum.clone();
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">let</span> learning_rate <span style="color:#f92672">=</span> self.learning_rate.clone();
</span></span><span style="display:flex;"><span>        net.parameters_and_gradients()
</span></span><span style="display:flex;"><span>            .zip(self.velocities.as_mut().unwrap().iter_mut())
</span></span><span style="display:flex;"><span>            .for_each(<span style="color:#f92672">|</span>((parameter, gradient), velocity)<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>                azip<span style="color:#f92672">!</span>((parameter <span style="color:#66d9ef">in</span> parameter, gradient <span style="color:#66d9ef">in</span> gradient, velocity <span style="color:#66d9ef">in</span> velocity) {
</span></span><span style="display:flex;"><span>                    <span style="color:#f92672">*</span>velocity <span style="color:#f92672">=</span> (<span style="color:#f92672">*</span>velocity).clone() <span style="color:#f92672">*</span> momentum.clone() <span style="color:#f92672">+</span> (<span style="color:#f92672">*</span>gradient).clone() <span style="color:#f92672">*</span> learning_rate.clone();
</span></span><span style="display:flex;"><span>                    <span style="color:#f92672">*</span>parameter <span style="color:#f92672">-=</span> (<span style="color:#f92672">*</span>velocity).clone();
</span></span><span style="display:flex;"><span>                });
</span></span><span style="display:flex;"><span>            });
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h3 id="testing">Testing<a hidden class="anchor" aria-hidden="true" href="#testing">#</a></h3>
<p>As you may recall from the previous post, the accuracy for the basic neural network was around <strong>16%</strong> which was the same as a simple linear regression. This seemingly is due to it falling into a local minima that it can&rsquo;t escape.</p>
<p>Momentum allows it to escape a local minima to get a higher accuracy, which is tested by replacing our standard SGD optimiser with the new SGDMomentum optimiser. For this test we will use a learning rate of 0.1, and a momentum of 0.9</p>
<p>After changing the code and running, against the same test as we had before, we get the following output:</p>
<pre tabindex="0"><code>Accuracy (training): 59.955999999999996%
Accuracy (testing): 60.126000000000005%
</code></pre><p>So we see that it&rsquo;s increased to &ldquo;better than average&rdquo;. This is a decent jump from 16%!</p>
<p>For softmax, we were already at a pretty high accuracy, with the standard SGD optimiser scoring about <strong>91%</strong> accuracy.</p>
<p>However, momentum gives us gains even here, and when we run the same problem with the same network, and change to using the momentum based optimiser we get slightly better results again:</p>
<pre tabindex="0"><code>Accuracy (training): 93.47999999999999%
Accuracy (testing): 93.522%
</code></pre><p>So from these we can see that using a momentum-based optimiser is better than a basic SGD optimiser.</p>
<p>As a side note, using momentum doesn&rsquo;t improve linear regression at all. This is likely because linear regression can only simulate a straight line, so the best or most accurate it can get to it was already achieving, and using momentum to find that ideal state doesn&rsquo;t help.</p>


  </div>

  <footer class="post-footer">
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://forgottenmaster.github.io/">Robin Firth</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
