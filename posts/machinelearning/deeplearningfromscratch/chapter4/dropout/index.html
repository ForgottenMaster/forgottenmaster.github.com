<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>4.6 - Dropout | Robin Firth</title>
<meta name="keywords" content="" />
<meta name="description" content="Out of all the modifications for neural networks we&rsquo;ve covered in order to try to get the network to train the problem &ldquo;better&rdquo;, we haven&rsquo;t actually tried what people would think is the most obvious - adding more neurons to the network.
Introduction Adding more neurons to the network can take one of two forms:
 Adding more neurons to a hidden layer in the network (expanding wide) Adding more hidden layers to the network (expanding long)  More neurons means more parameters in the network that can be tweaked - if you recall, each weight is a contribution to the overall prediction and a set of neurons in the network can represent some &ldquo;learned feature&rdquo; that the network deems important.">
<meta name="author" content="">
<link rel="canonical" href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/dropout/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css" integrity="sha256-b2AFbUTT9&#43;tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://forgottenmaster.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://forgottenmaster.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://forgottenmaster.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://forgottenmaster.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://forgottenmaster.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.92.2" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="4.6 - Dropout" />
<meta property="og:description" content="Out of all the modifications for neural networks we&rsquo;ve covered in order to try to get the network to train the problem &ldquo;better&rdquo;, we haven&rsquo;t actually tried what people would think is the most obvious - adding more neurons to the network.
Introduction Adding more neurons to the network can take one of two forms:
 Adding more neurons to a hidden layer in the network (expanding wide) Adding more hidden layers to the network (expanding long)  More neurons means more parameters in the network that can be tweaked - if you recall, each weight is a contribution to the overall prediction and a set of neurons in the network can represent some &ldquo;learned feature&rdquo; that the network deems important." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/dropout/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-07T02:11:16&#43;01:00" />
<meta property="article:modified_time" content="2022-05-07T02:11:16&#43;01:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="4.6 - Dropout"/>
<meta name="twitter:description" content="Out of all the modifications for neural networks we&rsquo;ve covered in order to try to get the network to train the problem &ldquo;better&rdquo;, we haven&rsquo;t actually tried what people would think is the most obvious - adding more neurons to the network.
Introduction Adding more neurons to the network can take one of two forms:
 Adding more neurons to a hidden layer in the network (expanding wide) Adding more hidden layers to the network (expanding long)  More neurons means more parameters in the network that can be tweaked - if you recall, each weight is a contribution to the overall prediction and a set of neurons in the network can represent some &ldquo;learned feature&rdquo; that the network deems important."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://forgottenmaster.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Machine Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Deep Learning From Scratch",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Chapter 4 - Extensions",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/"
    }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "4.6 - Dropout",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/dropout/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "4.6 - Dropout",
  "name": "4.6 - Dropout",
  "description": "Out of all the modifications for neural networks we\u0026rsquo;ve covered in order to try to get the network to train the problem \u0026ldquo;better\u0026rdquo;, we haven\u0026rsquo;t actually tried what people would think is the most obvious - adding more neurons to the network.\nIntroduction Adding more neurons to the network can take one of two forms:\n Adding more neurons to a hidden layer in the network (expanding wide) Adding more hidden layers to the network (expanding long)  More neurons means more parameters in the network that can be tweaked - if you recall, each weight is a contribution to the overall prediction and a set of neurons in the network can represent some \u0026ldquo;learned feature\u0026rdquo; that the network deems important.",
  "keywords": [
    
  ],
  "articleBody": "Out of all the modifications for neural networks we’ve covered in order to try to get the network to train the problem “better”, we haven’t actually tried what people would think is the most obvious - adding more neurons to the network.\nIntroduction Adding more neurons to the network can take one of two forms:\n Adding more neurons to a hidden layer in the network (expanding wide) Adding more hidden layers to the network (expanding long)  More neurons means more parameters in the network that can be tweaked - if you recall, each weight is a contribution to the overall prediction and a set of neurons in the network can represent some “learned feature” that the network deems important.\nWith a linear regression, we only have a single neuron which means only a single parameter we can tweak in order to change the prediction. It’s clear that there are some numbers in a linear regression that just can’t possibly be made because we only have a single multiplier and bias term (y = mx + c).\nSo if fewer data points means that the network is less accurate then surely adding more neurons to the network means it will be more accurate, right?\nWell….yes, and no. It’s true that more neurons means we can model more complex graphs, however it’s also true that the network will overfit to the training data, and not perform very well at all when used on testing data, or “in the wild”.\nDropout is a way to let us add additional capacity (extra neurons) to the network, while also making it less likely that the network overfits the training data.\nWhat is dropout? How do we avoid the network overfitting the training data?. We simply need to randomly set neurons in the network back to 0, or remove their contribution from the predictions in each epoch.\nThe intuition here is that it prevents a neuron’s weight from “locking on” to a particular value and keeping it for the entirety of training. When we set a neuron’s weight back to 0 then it must recalculate it from other neurons in the network in the optimisation step.\nThis continuous set back of random neurons, and their subsequent recalculation from other neurons means that the network ends up not overfitting (or at least less likely). After all, how can you overfit the data if you keep changing?\nOne thing to remember though, is when we’re making predictions for the purposes of testing, or when being used in the wild to predict using a trained network - we do not want to use dropout, since that changes the state of the network that we’ve worked so hard to train!\nHowever, during training, dropout ends up reducing the total magnitude of the values that are output from the layer by an average of (1-p) where p is the probability of dropping the neuron.\nThis makes sense, because when we dropout a neuron and set it to 0, the entire contribution to the weighted sum from that neuron is dropped to 0. The total weighted sum from the layer which was calculated from M neurons is now only calculated from \\(M \\times (1 - p)\\) neurons. When averaging this, it means that if the chance of dropout is 30% for example, it means that 30% of the overall sum passing forward is reduced by 30%.\nTherefore, when we’re making predictions, we want to not drop out the neuron, but we do want to simulate this reduction in magnitude so as to keep the values going forward at the expected range to not give false answers. We’ll do that by allowing the caller to specify they’re using inference mode, and we’ll just simulate the dropout by multiplying the whole output by the keep probability.\nImplementing Dropout In order to implement dropout, we’ll create a BasicOperation that is tacked onto the end of the operation sequence of the layer when we create the layer.\nHowever there is a minor problem, as we’ll soon see that will require a little refactor to our code in order to allow operations to retain state.\nUp until now, our operations were stateless and have just been used as a vtable, however in order for us to apply the same dropout mask as calculated in the forward pass (remember this is generated randomly each forward pass), we’ll need to make operations mutable.\nMaking Operation’s forward and backward passes mutable In order for us to allow implementations of operations to store state in their forward pass for use during their backward pass, we will need to change the trait implementation for our BasicOperation and ParameterOperation traits such that they look as follows:\npub trait BasicOperationT: BasicOperationCloneT { fn calculate_output(\u0026mut self, input: \u0026ArrayT, Ix2) - ArrayT, Ix2; fn calculate_input_gradient( \u0026mut self, output_gradient: \u0026ArrayT, Ix2, input: \u0026ArrayT, Ix2, ) - ArrayT, Ix2; } pub trait ParameterOperationT: ParameterOperationCloneT { fn calculate_output(\u0026mut self, input: \u0026ArrayT, Ix2, parameter: \u0026ArrayT, Ix2) - ArrayT, Ix2; fn calculate_input_gradient( \u0026mut self, output_gradient: \u0026ArrayT, Ix2, input: \u0026ArrayT, Ix2, parameter: \u0026ArrayT, Ix2, ) - ArrayT, Ix2; fn calculate_parameter_gradient( \u0026mut self, output_gradient: \u0026ArrayT, Ix2, input: \u0026ArrayT, Ix2, parameter: \u0026ArrayT, Ix2, ) - ArrayT, Ix2; } That is, we simply need to change \u0026self to \u0026mut self in the calculation functions however this has the knock on effect then of requiring that all implementations are updated to the new signature (changing a trait is a breaking change in Rust).\nI won’t show that here, but again it’s just a case of changing all the \u0026self’s to \u0026mut self.\nThis has the unfortunate knock on effect of requiring that the Operation struct is also updated, this is because it will be calling these methods, which previously were able to be called on an immutable reference. Now, however it will need a mutable object to be able to call them, so we need to update Operation also.\nAgain, this is a simple enough change to not show here.\nIdentifying “inference” mode Up until now, when we go through the forward pass, we have no idea if we’re in training mode or inference mode. However for the dropout operation we will be required to know if we’re in training or inference mode.\nThis is because, as mentioned previously when we’re training the network, we want fully fledged dropout of neurons, however when we’re only in inference mode we don’t actually want to do any changes to the network, but still need to simulate the dropout.\nTo do this, we will firstly create an enum indicating the mode that we’re in. We could use a boolean for this as there’s only two states, however using an enumeration is more typesafe and more readable at the call site (it’s better to see Mode.Inference instead of just “true”).\n#[derive(Clone, Copy, PartialEq)] pub enum Mode { Training, Inference, } We will need to take the correct mode into the Network.forward method, since we only require the forward pass to be aware of what mode we’re operating in. We have three “entry point” functions when interacting with the Network. These are:\n forward - Called on a fully trained network to produce predictions. Should always run in inference mode train - Called when we are training the network (performs a forward and backward pass). Always runs in training mode forward_loss - Called both by training and also by examples on a trained network to see the loss values. Can run in both modes  Therefore we will move the contents into a helper function (inner_forward) which will take a Mode and can be called by both forward_loss and forward, passing the appropriate mode. We change forward to call this with a fixed mode of Inference, and change train to call it with a fixed mode of Training. forward_loss changes to take a Mode and pass it along. These 4 relevant functions now look as follows:\npub fn train(\u0026mut self, batch: ArrayT, Ix2, targets: ArrayT, Ix2) - T { let (_, loss) = self.forward_loss(batch, targets, Mode::Training); self.backward(); loss } pub fn forward(\u0026mut self, batch: ArrayT, Ix2) - ArrayT, Ix2 { self.inner_forward(batch, Mode::Inference) } pub fn forward_loss( \u0026mut self, batch: ArrayT, Ix2, targets: ArrayT, Ix2, mode: Mode, ) - (ArrayT, Ix2, T) { let predictions = self.inner_forward(batch, mode); let loss = self.loss.forward(predictions.clone(), targets); (predictions, loss) } fn inner_forward(\u0026mut self, batch: ArrayT, Ix2, mode: Mode) - ArrayT, Ix2 { self.layers .iter_mut() .fold(batch, |state, layer| layer.forward(\u0026state)) } Next we will need to pass this along to the forward method of the Layer type inside inner_forward. Updating the code here is simple enough, we simply pass the mode into the layer.forward call.\nHowever as you might have guessed, this doesn’t compile!. Layer::forward is not expecting a second parameter, so we will need to make sure we add one:\n// Layer::forward pub fn forward(\u0026mut self, input: \u0026ArrayT, Ix2, mode: Mode) - ArrayT, Ix2 { // rest of the code is the same here } We’re one level deeper now, we need to thread this inference mode down to the next lowest level which is into the individual operations themselves, which we can do inside the Layer::forward method when we’re going over the operations. Adding this gives us a compile error once more, due to Operation::forward not expecting this parameter. So we add it:\n// Operation::forward pub fn forward(\u0026mut self, input: \u0026ArrayT, Ix2, mode: Mode) - ArrayT, Ix2 { // rest of code stays the same } Finally, we need to thread this mode into the calculate_output function on the BasicOperation or ParameterOperation trait object. However, you guessed it!, we need to update those traits to take the mode as a parameter into the calculate_output function:\n// BasicOperation fn calculate_output(\u0026mut self, input: \u0026ArrayT, Ix2, mode: Mode) - ArrayT, Ix2; // ParameterOperation fn calculate_output( \u0026mut self, input: \u0026ArrayT, Ix2, parameter: \u0026ArrayT, Ix2, mode: Mode ) - ArrayT, Ix2; As a result, we need to update all implementors to satisfy the new signature, but I won’t show that here.\nWith that, we’re finally done!, we are now able to specify the operating mode to the Network and it’s correctly passed down all the way to the individual operations. We can finally implement the Dropout operation!\nThe dropout operation We’ll implement the dropout operation as a BasicOperation that we will tack onto the end of the appropriate layers, and with the above changes should be pretty easy to implement. We will need to store some internal state however. Namely, we need to record the generated random mask from the forward pass that we use to drop out some of the neurons so that we can drop the associated gradients on the backward pass (the gradient associated with an input we nullify shouldn’t be passed back in the network).\nWe will also need to specify the proportion of the neurons that are kept which is the multiplier we use in inference mode, but also lets us generate the random mask in training mode.\nThe struct definition itself is pretty simple:\n#[derive(Clone)] pub struct DropoutT { mask: ArrayT, Ix2, keep_probability: T, seed: Optionu64 } Note that we also include the ability to specify a seed for the RNG to generate the mask. This is required for testing purposes as this is the only operation with a random element so we need to make sure we can handle a seed being passed.\nAs with the other operations, we will want to work with boxed versions of this type so as to be able to use it in the list of operations setup by our layer setup. Therefore we add a new_boxed method to produce a new Dropout instance. We will also add a new_boxed_with_seed function that specifies a seed for RNG also.\nimplT: Default DropoutT { pub fn new_boxed(keep_probability: T) - BoxSelf { Box::new(Self { mask: Default::default(), keep_probability, seed: None, }) } pub fn new_boxed_with_seed(keep_probability: T, seed: u64) - BoxSelf { Box::new(Self { mask: Default::default(), keep_probability, seed: Some(seed), }) } } Note that we do require Default on elements of type T in order to use default on Array, even if we’re creating an array of size 0, just because there seems to be no “empty” constructor to construct a new, empty array and Default is only implemented for Array in the case that the element type is also Default.\nNow we just need to implement the BasicOperation trait for our Dropout type.\nAs far as trait bounds go, we will need the signature to look as follows:\nimpl T: 'static + Clone + MulOutput = T + One + PartialOrd + SampleUniform + ScalarOperand + Zero,  BasicOperationT for DropoutT where Standard: DistributionT, { // implementation follows } The explanations for these are below:\n ‘static - This is required because to be able to be clonable by the BasicOperationClone supertrait we need the elements to be of type T. The other operations so far have been empty, so the lifetime of T doesn’t matter as we never store any, and the operation type is ‘static. However for this one we store a mask of T’s and so we need to explicitly say T has to be ‘static for our type overall to be ‘static. Clone - This is required also for the cloning behaviour, we need T to be clonable so we can be clonable. Mul - This is required because we must multiply the elements of the input by our keep probability which is a T, or by our mask which is an Array of T. One - Required because in order to generate the probabilities for dropout, and to generate the mask of zeroes and ones, we will need to know what one of a T is. PartialOrd - Required because we generate the dropout probabilities randomly, and need to determine whether a given value is kept or dropped. We do this by doing a comparison of the random value against our keep_probability. SampleUniform - Required because we’re using a random number generator to generate random samples within the range 0 to 1, and in a generic context, to do this we need to only use types that can be sampled in this way. ScalarOperand - Required because we multiply the input by the keep probability if we’re in inference mode. This allows the operation to be broadcast to the elements of the array correctly. Zero - Required because in order to generate the probabilities for dropout, and to generate the mask of zeroes and ones, we will need to know what zero of a T is. Standard: Distribution - This one is a little “backward” in that we’re stating that the Standard trait from rand supports being a distribution over our type T.  For the calculate_output function, we firstly need to check whether we’re in inference mode or not. If we are then we will simply multiply the elements of the input array by our keep probability to simulate the magnitude dropoff, but without actually dropping any neurons out. We will make a mask of all 1’s just in case the backward pass is called (it shouldn’t be in inference mode). This looks like follows:\nfn calculate_output(\u0026mut self, input: \u0026ArrayT, Ix2, mode: Mode) - ArrayT, Ix2 { if mode == Mode::Inference { self.mask = Array::ones(input.raw_dim()); input * self.keep_probability.clone() } else { // training mode implementation goes here  } } Training mode is a little bit more complex!\nFirstly we need to create an RNG either truly random or with the seed if we have one. We can do this in the same way as we do in the DenseLayerSetup type:\nlet mut random = match self.seed { Some(seed) = StdRng::seed_from_u64(seed), None = StdRng::from_rng(thread_rng()).unwrap(), }; What we then need to do is calculate how many total elements we need to generate. This is easy enough - it’s the number of rows multiplied by the number of columns:\nlet number_required = input.ncols() * input.nrows(); To generate the mask, we know that we will be generating a sequence of T’s as a flat vector, and need to reshape it into a 2-D array of the correct size. This is pretty easy with the Array::from_shape_vec method we’ve been using:\nself.mask = Array::from_shape_vec( (input.nrows(), input.ncols()), ??? ).unwrap(); So we just have to fill in the ???’s which will produce the vector of randomly selected 1’s and 0’s.\nTo do this we can firstly create an iterator over the range (0..number_required) and then map that iterator onto one that produces T’s. The map function we will pass doesn’t actually use the index, so we replace that with an _.\nThe first thing we do is to generate a random T within the range 0 to 1. Since we’re in a generic context, we don’t know what 0 or 1 is in the context of a T, so we need to rely on the fact we’ve constrained T to implement the Zero and One traits so we can ask for the appropriate representations.\nThe actual generation can be done by the gen_range function provided by rand:\nlet elem: T = random.gen_range(T::zero()..=T::one()); Next we check if the generated element is less or equal to our keep probability. If it is, then we return a 1. If however the generated element is above the keep probability then it’s in the dropout range and should be emitted as a 0. This snippet looks as follows:\nif elem  self.keep_probability { T::one() } else { T::zero() } Finally we can take that iterator that’s now emitting random T::zero()’s and T::one()’s, and collect it into a vector for reshaping.\nAfter we’ve got the mask, we just multiply the input by it to get the output:\ninput * self.mask.clone() In its entirety, the calculate_output function looks as follows:\nfn calculate_output(\u0026mut self, input: \u0026ArrayT, Ix2, mode: Mode) - ArrayT, Ix2 { if mode == Mode::Inference { self.mask = Array::ones(input.raw_dim()); input * self.keep_probability.clone() } else { let mut random = match self.seed { Some(seed) = StdRng::seed_from_u64(seed), None = StdRng::from_rng(thread_rng()).unwrap(), }; let number_required = input.ncols() * input.nrows(); self.mask = Array::from_shape_vec( (input.nrows(), input.ncols()), (0..number_required) .map(|_| { let elem: T = random.gen_range(T::zero()..=T::one()); if elem  self.keep_probability { T::one() } else { T::zero() } }) .collect(), ) .unwrap(); input * self.mask.clone() } } By contrast however, the calculate_input_gradient function is incredibly simple!. The only job for that function is to drop gradients associated with the elements that were dropped in the forward pass. We stored the mask that was generated so this is easy, and in fact I’ll paste the code in its entirety here:\nfn calculate_input_gradient( \u0026mut self, output_gradient: \u0026ArrayT, Ix2, _input: \u0026ArrayT, Ix2, ) - ArrayT, Ix2 { output_gradient * self.mask.clone() } And with that, our Dropout operation is done!\nLayerSetupAdapter Now we need to provide a way to ensure that the Dropout operator can be opted into when creating the layers. The best way to do this is to write a wrapper around any implementation of a LayerSetupand ensure that we also implement the LayerSetup.\nWhen layer_setup is called we can call the wrapped one to get the initial list of operations for the layer, and then we can tack a Dropout onto the end. This is called the Composite pattern and is quite pervasive in Rust because we don’t have inheritance, instead the user can choose their functionality and combine it like this.\nThat way we don’t need to duplicate the dropout functionality if a different LayerSetup other than DenseLayerSetup is created later.\nThe struct will be generic over two types:\n T - The type of the underlying elements in the array U - The type of the concrete instance of LayerSetup that we’re actually decorating  It also needs to hold onto the keep_probability (also a T) for later during setup. The definition then looks as follows:\n#[derive(Clone)] pub struct DropoutLayerAdapterT, U { layer_setup: U, keep_probability: T, } We also need a new_boxed constructor function like we have for our other types, but this isn’t that noteworthy so will show it here in it’s entirety:\nimplT, U DropoutLayerAdapterT, U { pub fn new_boxed(layer_setup: U, keep_probability: T) - BoxSelf { Box::new(Self { layer_setup, keep_probability, }) } } Finally, we need to actually implement the LayerSetuptrait for our adapter so that it can stand in for the decorated type in the network. The trait bounds here seem funky:\nimpl T: 'static + Clone + Default + One + PartialOrd + SampleUniform + ScalarOperand + Zero, U: Clone + 'static + LayerSetupT,  LayerSetupT for DropoutLayerAdapterT, U where Standard: DistributionT However, we don’t need any of this functionality to implement the trait, these are simply bounds “inherited” from the various types in use that we’re decorating (specifically LayerSetup, and Dropout).\nThe implementation is simple also, as we just need to:\n Call the wrapped LayerSetup to get a list of Operations Push our Dropout operation onto the end of the list Return the modified list  In code, this equates to:\nlet mut operations = self.layer_setup.layer_setup(num_neurons, input, seed); operations.push(Operation::new_basic(Dropout::new_boxed( self.keep_probability.clone(), ))); operations And that’s it!\nThe caller can now create a DenseLayerSetup (or any LayerSetup) either without dropout as before:\nDenseLayerSetup::new_boxed(Tanh::new_boxed()) Or with dropout, by wrapping it:\nDropoutLayerAdapter::new_boxed(DenseLayerSetup::new(Tanh::new_boxed()), KEEP_PROBABILITY) Testing Finally we can create an example to test this new dropout functionality. We’ll take the latest example which included:\n Softmax cross entropy loss function Momentum Exponential learning rate decay  And we’ll add dropout with a keep probability of 65% to the hidden layer (example is just called “dropout”)….\nAccuracy (training): 91.414%\rAccuracy (testing): 91.522%\rWhat happened!?\nWell, we’re dropping out neurons which will make it less likely to overfit, but will also slow down, or set back the training. Recall that dropout lets us increase the capacity of the network. Let’s go ahead and do that by adding another hidden layer with twice the number of neurons:\nAccuracy (training): 95.372%\rAccuracy (testing): 95.322%\rThis is almost the same, but a little lower than we had previously. However we are able to have multiple layers now to model more complex relationships which dropout will cope with. Tweaking hyperparameters some more will likely result in gains.\nAs a sanity test, let’s remove the dropout with this new layer:\nAccuracy (training): 87.46199999999999%\rAccuracy (testing): 87.434%\rSo we can see that indeed the dropout operator is letting us expand the layers and capacity of the network while preventing degraded accuracy.\n",
  "wordCount" : "3705",
  "inLanguage": "en",
  "datePublished": "2022-05-07T02:11:16+01:00",
  "dateModified": "2022-05-07T02:11:16+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/dropout/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Robin Firth",
    "logo": {
      "@type": "ImageObject",
      "url": "https://forgottenmaster.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://forgottenmaster.github.io/" accesskey="h" title="Robin Firth (Alt + H)">Robin Firth</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://forgottenmaster.github.io/resume/" title="Résumé">
                    <span>Résumé</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://forgottenmaster.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/">Posts</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/">Machine Learning</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/">Deep Learning From Scratch</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter4/">Chapter 4 - Extensions</a></div>
    <h1 class="post-title">
      4.6 - Dropout
    </h1>
    <div class="post-meta"><span title='2022-05-07 02:11:16 +0100 BST'>May 7, 2022</span>&nbsp;·&nbsp;18 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#what-is-dropout" aria-label="What is dropout?">What is dropout?</a></li>
                <li>
                    <a href="#implementing-dropout" aria-label="Implementing Dropout">Implementing Dropout</a><ul>
                        
                <li>
                    <a href="#making-operations-forward-and-backward-passes-mutable" aria-label="Making Operation&amp;rsquo;s forward and backward passes mutable">Making Operation&rsquo;s forward and backward passes mutable</a></li>
                <li>
                    <a href="#identifying-inference-mode" aria-label="Identifying &amp;ldquo;inference&amp;rdquo; mode">Identifying &ldquo;inference&rdquo; mode</a></li>
                <li>
                    <a href="#the-dropout-operation" aria-label="The dropout operation">The dropout operation</a></li></ul>
                </li>
                <li>
                    <a href="#layersetupadapter" aria-label="LayerSetupAdapter">LayerSetupAdapter</a></li>
                <li>
                    <a href="#testing" aria-label="Testing">Testing</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Out of all the modifications for neural networks we&rsquo;ve covered in order to try to get the network to train the problem &ldquo;better&rdquo;, we haven&rsquo;t actually tried what people would think is the most obvious - adding more neurons to the network.</p>
<h3 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h3>
<p>Adding more neurons to the network can take one of two forms:</p>
<ol>
<li>Adding more neurons to a hidden layer in the network (expanding wide)</li>
<li>Adding more hidden layers to the network (expanding long)</li>
</ol>
<p>More neurons means more parameters in the network that can be tweaked - if you recall, each weight is a contribution to the overall prediction and a set of neurons in the network can represent some &ldquo;learned feature&rdquo; that the network deems important.</p>
<p>With a linear regression, we only have a single neuron which means only a single parameter we can tweak in order to change the prediction. It&rsquo;s clear that there are some numbers in a linear regression that just can&rsquo;t possibly be made because we only have a single multiplier and bias term (y = mx + c).</p>
<p>So if fewer data points means that the network is less accurate then surely adding more neurons to the network means it will be <em>more accurate</em>, right?</p>
<p>Well&hellip;.yes, and no. It&rsquo;s true that more neurons means we can model more complex graphs, however it&rsquo;s also true that the network will <strong>overfit</strong> to the training data, and not perform very well at all when used on testing data, or &ldquo;in the wild&rdquo;.</p>
<p>Dropout is a way to let us add additional capacity (extra neurons) to the network, while also making it less likely that the network overfits the training data.</p>
<h3 id="what-is-dropout">What is dropout?<a hidden class="anchor" aria-hidden="true" href="#what-is-dropout">#</a></h3>
<p>How do we avoid the network overfitting the training data?. We simply need to randomly set neurons in the network back to <strong>0</strong>, or remove their contribution from the predictions in each epoch.</p>
<p>The intuition here is that it prevents a neuron&rsquo;s weight from &ldquo;locking on&rdquo; to a particular value and keeping it for the entirety of training. When we set a neuron&rsquo;s weight back to 0 then it must recalculate it from other neurons in the network in the optimisation step.</p>
<p>This continuous set back of random neurons, and their subsequent recalculation from other neurons means that the network ends up not overfitting (or at least less likely). After all, how can you overfit the data if you keep changing?</p>
<p>One thing to remember though, is when we&rsquo;re making predictions for the purposes of testing, or when being used in the wild to predict using a trained network - we do <strong>not</strong> want to use dropout, since that changes the state of the network that we&rsquo;ve worked so hard to train!</p>
<p>However, during training, dropout ends up reducing the total <strong>magnitude</strong> of the values that are output from the layer by an average of (1-p) where p is the probability of dropping the neuron.</p>
<p>This makes sense, because when we dropout a neuron and set it to 0, the entire contribution to the weighted sum from that neuron is dropped to 0. The total weighted sum from the layer which was calculated from M neurons is now only calculated from <strong>\(M \times (1 - p)\)</strong> neurons. When averaging this, it means that if the chance of dropout is 30% for example, it means that 30% of the overall sum passing forward is reduced by 30%.</p>
<p>Therefore, when we&rsquo;re making predictions, we want to not drop out the neuron, but we <strong>do</strong> want to simulate this reduction in magnitude so as to keep the values going forward at the expected range to not give false answers. We&rsquo;ll do that by allowing the caller to specify they&rsquo;re using <strong>inference</strong> mode, and we&rsquo;ll just simulate the dropout by multiplying the whole output by the keep probability.</p>
<h3 id="implementing-dropout">Implementing Dropout<a hidden class="anchor" aria-hidden="true" href="#implementing-dropout">#</a></h3>
<p>In order to implement dropout, we&rsquo;ll create a BasicOperation that is tacked onto the end of the operation sequence of the layer when we create the layer.</p>
<p>However there is a minor problem, as we&rsquo;ll soon see that will require a little refactor to our code in order to allow operations to retain state.</p>
<p>Up until now, our operations were stateless and have just been used as a vtable, however in order for us to apply the <strong>same</strong> dropout mask as calculated in the forward pass (remember this is generated randomly each forward pass), we&rsquo;ll need to make operations mutable.</p>
<h4 id="making-operations-forward-and-backward-passes-mutable">Making Operation&rsquo;s forward and backward passes mutable<a hidden class="anchor" aria-hidden="true" href="#making-operations-forward-and-backward-passes-mutable">#</a></h4>
<p>In order for us to allow implementations of operations to store state in their forward pass for use during their backward pass, we will need to change the trait implementation for our BasicOperation and ParameterOperation traits such that they look as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">trait</span> BasicOperation<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>: <span style="color:#a6e22e">BasicOperationClone</span><span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> {
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_output</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>;
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_input_gradient</span>(
        <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self,
        output_gradient: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
        input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
    ) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>;
}

<span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">trait</span> ParameterOperation<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>: <span style="color:#a6e22e">ParameterOperationClone</span><span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> {
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_output</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, parameter: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>;
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_input_gradient</span>(
        <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self,
        output_gradient: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
        input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
        parameter: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
    ) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>;
    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_parameter_gradient</span>(
        <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self,
        output_gradient: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
        input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
        parameter: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
    ) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>;
}</code></pre></div>
<p>That is, we simply need to change <strong>&amp;self</strong> to <strong>&amp;mut self</strong> in the calculation functions however this has the knock on effect then of requiring that all implementations are updated to the new signature (changing a trait is a breaking change in Rust).</p>
<p>I won&rsquo;t show that here, but again it&rsquo;s just a case of changing all the &amp;self&rsquo;s to &amp;mut self.</p>
<p>This has the unfortunate knock on effect of requiring that the Operation struct is also updated, this is because it will be calling these methods, which previously were able to be called on an immutable reference. Now, however it will need a mutable object to be able to call them, so we need to update Operation also.</p>
<p>Again, this is a simple enough change to not show here.</p>
<h4 id="identifying-inference-mode">Identifying &ldquo;inference&rdquo; mode<a hidden class="anchor" aria-hidden="true" href="#identifying-inference-mode">#</a></h4>
<p>Up until now, when we go through the forward pass, we have no idea if we&rsquo;re in training mode or inference mode. However for the dropout operation we will be required to know if we&rsquo;re in training or inference mode.</p>
<p>This is because, as mentioned previously when we&rsquo;re training the network, we want fully fledged dropout of neurons, however when we&rsquo;re only in inference mode we don&rsquo;t actually want to do any changes to the network, but still need to simulate the dropout.</p>
<p>To do this, we will firstly create an enum indicating the mode that we&rsquo;re in. We could use a boolean for this as there&rsquo;s only two states, however using an enumeration is more typesafe and more readable at the call site (it&rsquo;s better to see Mode.Inference instead of just &ldquo;true&rdquo;).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#75715e">#[derive(Clone, Copy, PartialEq)]</span>
<span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">enum</span> <span style="color:#a6e22e">Mode</span> {
    Training,
    Inference,
}</code></pre></div>
<p>We will need to take the correct mode into the Network.forward method, since we only require the forward pass to be aware of what mode we&rsquo;re operating in. We have three &ldquo;entry point&rdquo; functions when interacting with the Network. These are:</p>
<ol>
<li><strong>forward</strong> - Called on a fully trained network to produce predictions. Should always run in inference mode</li>
<li><strong>train</strong> - Called when we are training the network (performs a forward and backward pass). Always runs in training mode</li>
<li><strong>forward_loss</strong> - Called both by training and also by examples on a trained network to see the loss values. Can run in both modes</li>
</ol>
<p>Therefore we will move the contents into a helper function (inner_forward) which will take a Mode and can be called by both forward_loss and forward, passing the appropriate mode. We change forward to call this with a fixed mode of Inference, and change train to call it with a fixed mode of Training. forward_loss changes to take a Mode and pass it along. These 4 relevant functions now look as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">train</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, batch: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, targets: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">T</span> {
    <span style="color:#66d9ef">let</span> (_, loss) <span style="color:#f92672">=</span> self.forward_loss(batch, targets, Mode::Training);
    self.backward();
    loss
}

<span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">forward</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, batch: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
    self.inner_forward(batch, Mode::Inference)
}

<span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">forward_loss</span>(
    <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self,
    batch: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
    targets: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
    mode: <span style="color:#a6e22e">Mode</span>,
) -&gt; (Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, T) {
    <span style="color:#66d9ef">let</span> predictions <span style="color:#f92672">=</span> self.inner_forward(batch, mode);
    <span style="color:#66d9ef">let</span> loss <span style="color:#f92672">=</span> self.loss.forward(predictions.clone(), targets);
    (predictions, loss)
}

<span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">inner_forward</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, batch: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, mode: <span style="color:#a6e22e">Mode</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
    self.layers
        .iter_mut()
        .fold(batch, <span style="color:#f92672">|</span>state, layer<span style="color:#f92672">|</span> layer.forward(<span style="color:#f92672">&amp;</span>state))
}</code></pre></div>
<p>Next we will need to pass this along to the forward method of the <strong>Layer</strong> type inside inner_forward. Updating the code here is simple enough, we simply pass the mode into the layer.forward call.</p>
<p>However as you might have guessed, this doesn&rsquo;t compile!. Layer::forward is not expecting a second parameter, so we will need to make sure we add one:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#75715e">// Layer::forward
</span><span style="color:#75715e"></span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">forward</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, mode: <span style="color:#a6e22e">Mode</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
    <span style="color:#75715e">// rest of the code is the same here
</span><span style="color:#75715e"></span>}</code></pre></div>
<p>We&rsquo;re one level deeper now, we need to thread this inference mode down to the next lowest level which is into the individual operations themselves, which we can do inside the Layer::forward method when we&rsquo;re going over the operations. Adding this gives us a compile error once more, due to Operation::forward not expecting this parameter. So we add it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#75715e">// Operation::forward
</span><span style="color:#75715e"></span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">forward</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, mode: <span style="color:#a6e22e">Mode</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
    <span style="color:#75715e">// rest of code stays the same
</span><span style="color:#75715e"></span>}</code></pre></div>
<p>Finally, we need to thread this mode into the <strong>calculate_output</strong> function on the BasicOperation or ParameterOperation trait object. However, you guessed it!, we need to update those traits to take the mode as a parameter into the calculate_output function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#75715e">// BasicOperation
</span><span style="color:#75715e"></span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_output</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, mode: <span style="color:#a6e22e">Mode</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>;

<span style="color:#75715e">// ParameterOperation
</span><span style="color:#75715e"></span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_output</span>(
    <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self,
    input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
    parameter: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
    mode: <span style="color:#a6e22e">Mode</span>
) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>;</code></pre></div>
<p>As a result, we need to update all implementors to satisfy the new signature, but I won&rsquo;t show that here.</p>
<p>With that, we&rsquo;re finally done!, we are now able to specify the operating mode to the Network and it&rsquo;s correctly passed down all the way to the individual operations. We can finally implement the Dropout operation!</p>
<h4 id="the-dropout-operation">The dropout operation<a hidden class="anchor" aria-hidden="true" href="#the-dropout-operation">#</a></h4>
<p>We&rsquo;ll implement the dropout operation as a BasicOperation that we will tack onto the end of the appropriate layers, and with the above changes should be pretty easy to implement. We will need to store some internal state however. Namely, we need to record the generated random mask from the forward pass that we use to drop out some of the neurons so that we can drop the associated gradients on the backward pass (the gradient associated with an input we nullify shouldn&rsquo;t be passed back in the network).</p>
<p>We will also need to specify the proportion of the neurons that are <strong>kept</strong> which is the multiplier we use in inference mode, but also lets us generate the random mask in training mode.</p>
<p>The struct definition itself is pretty simple:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#75715e">#[derive(Clone)]</span>
<span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">Dropout</span><span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> {
    mask: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
    keep_probability: <span style="color:#a6e22e">T</span>,
    seed: Option<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">u64</span><span style="color:#f92672">&gt;</span>
}</code></pre></div>
<p>Note that we also include the ability to specify a seed for the RNG to generate the mask. This is required for testing purposes as this is the only operation with a random element so we need to make sure we can handle a seed being passed.</p>
<p>As with the other operations, we will want to work with <strong>boxed</strong> versions of this type so as to be able to use it in the list of operations setup by our layer setup. Therefore we add a new_boxed method to produce a new Dropout instance. We will also add a new_boxed_with_seed function that specifies a seed for RNG also.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span>T: Default<span style="color:#f92672">&gt;</span> Dropout<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> {
    <span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">new_boxed</span>(keep_probability: <span style="color:#a6e22e">T</span>) -&gt; Box<span style="color:#f92672">&lt;</span>Self<span style="color:#f92672">&gt;</span> {
        Box::new(Self {
            mask: Default::default(),
            keep_probability,
            seed: None,
        })
    }

    <span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">new_boxed_with_seed</span>(keep_probability: <span style="color:#a6e22e">T</span>, seed: <span style="color:#66d9ef">u64</span>) -&gt; Box<span style="color:#f92672">&lt;</span>Self<span style="color:#f92672">&gt;</span> {
        Box::new(Self {
            mask: Default::default(),
            keep_probability,
            seed: Some(seed),
        })
    }
}</code></pre></div>
<p>Note that we do require Default on elements of type T in order to use default on Array, even if we&rsquo;re creating an array of size 0, just because there seems to be no &ldquo;empty&rdquo; constructor to construct a new, empty array and Default is only implemented for Array in the case that the element type is also Default.</p>
<p>Now we just need to implement the BasicOperation trait for our Dropout type.</p>
<p>As far as trait bounds go, we will need the signature to look as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span>
        T: <span style="color:#f92672">&#39;</span>static <span style="color:#f92672">+</span> Clone <span style="color:#f92672">+</span> Mul<span style="color:#f92672">&lt;</span>Output <span style="color:#f92672">=</span> T<span style="color:#f92672">&gt;</span> <span style="color:#f92672">+</span> One <span style="color:#f92672">+</span> PartialOrd <span style="color:#f92672">+</span> SampleUniform <span style="color:#f92672">+</span> ScalarOperand <span style="color:#f92672">+</span> Zero,
    <span style="color:#f92672">&gt;</span> BasicOperation<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">for</span> Dropout<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>
<span style="color:#66d9ef">where</span>
    Standard: <span style="color:#a6e22e">Distribution</span><span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>,
{
    <span style="color:#75715e">// implementation follows
</span><span style="color:#75715e"></span>}</code></pre></div>
<p>The explanations for these are below:</p>
<ol>
<li><strong>&lsquo;static</strong> - This is required because to be able to be clonable by the BasicOperationClone supertrait we need the elements to be of type T. The other operations so far have been empty, so the lifetime of T doesn&rsquo;t matter as we never store any, and the operation type is &lsquo;static. However for this one we store a mask of T&rsquo;s and so we need to explicitly say T has to be &lsquo;static for our type overall to be &lsquo;static.</li>
<li><strong>Clone</strong> - This is required also for the cloning behaviour, we need T to be clonable so we can be clonable.</li>
<li><strong>Mul&lt;Output=T&gt;</strong> - This is required because we must multiply the elements of the input by our keep probability which is a T, or by our mask which is an Array of T.</li>
<li><strong>One</strong> - Required because in order to generate the probabilities for dropout, and to generate the mask of zeroes and ones, we will need to know what one of a T is.</li>
<li><strong>PartialOrd</strong> - Required because we generate the dropout probabilities randomly, and need to determine whether a given value is kept or dropped. We do this by doing a comparison of the random value against our keep_probability.</li>
<li><strong>SampleUniform</strong> - Required because we&rsquo;re using a random number generator to generate random samples within the range 0 to 1, and in a generic context, to do this we need to only use types that can be sampled in this way.</li>
<li><strong>ScalarOperand</strong> - Required because we multiply the input by the keep probability if we&rsquo;re in inference mode. This allows the operation to be broadcast to the elements of the array correctly.</li>
<li><strong>Zero</strong> - Required because in order to generate the probabilities for dropout, and to generate the mask of zeroes and ones, we will need to know what zero of a T is.</li>
<li><strong>Standard: Distribution<!-- raw HTML omitted --></strong> - This one is a little &ldquo;backward&rdquo; in that we&rsquo;re stating that the Standard trait from rand supports being a distribution over our type T.</li>
</ol>
<p>For the calculate_output function, we firstly need to check whether we&rsquo;re in inference mode or not. If we are then we will simply multiply the elements of the input array by our keep probability to simulate the magnitude dropoff, but without actually dropping any neurons out. We will make a mask of all 1&rsquo;s just in case the backward pass is called (it shouldn&rsquo;t be in inference mode). This looks like follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_output</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, mode: <span style="color:#a6e22e">Mode</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
    <span style="color:#66d9ef">if</span> mode <span style="color:#f92672">==</span> Mode::Inference {
        self.mask <span style="color:#f92672">=</span> Array::ones(input.raw_dim());
        input <span style="color:#f92672">*</span> self.keep_probability.clone()
    } <span style="color:#66d9ef">else</span> {
        <span style="color:#75715e">// training mode implementation goes here
</span><span style="color:#75715e"></span>    }
}</code></pre></div>
<p>Training mode is a little bit more complex!</p>
<p>Firstly we need to create an RNG either truly random or with the seed if we have one. We can do this in the same way as we do in the DenseLayerSetup type:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> random <span style="color:#f92672">=</span> <span style="color:#66d9ef">match</span> self.seed {
    Some(seed) <span style="color:#f92672">=&gt;</span> StdRng::seed_from_u64(seed),
    None <span style="color:#f92672">=&gt;</span> StdRng::from_rng(thread_rng()).unwrap(),
};</code></pre></div>
<p>What we then need to do is calculate how many <strong>total</strong> elements we need to generate. This is easy enough - it&rsquo;s the number of rows multiplied by the number of columns:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">let</span> number_required <span style="color:#f92672">=</span> input.ncols() <span style="color:#f92672">*</span> input.nrows();</code></pre></div>
<p>To generate the mask, we know that we will be generating a sequence of T&rsquo;s as a flat vector, and need to reshape it into a 2-D array of the correct size. This is pretty easy with the Array::from_shape_vec method we&rsquo;ve been using:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust">self.mask <span style="color:#f92672">=</span> Array::from_shape_vec(
    (input.nrows(), input.ncols()),
    <span style="color:#f92672">???</span>
).unwrap();</code></pre></div>
<p>So we just have to fill in the ???&rsquo;s which will produce the vector of randomly selected 1&rsquo;s and 0&rsquo;s.</p>
<p>To do this we can firstly create an iterator over the range (0..number_required) and then map that iterator onto one that produces T&rsquo;s. The map function we will pass doesn&rsquo;t actually use the index, so we replace that with an _.</p>
<p>The first thing we do is to generate a random T within the range 0 to 1. Since we&rsquo;re in a generic context, we don&rsquo;t know what 0 or 1 is in the context of a T, so we need to rely on the fact we&rsquo;ve constrained T to implement the Zero and One traits so we can ask for the appropriate representations.</p>
<p>The actual generation can be done by the gen_range function provided by rand:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">let</span> elem: <span style="color:#a6e22e">T</span> <span style="color:#f92672">=</span> random.gen_range(T::zero()<span style="color:#f92672">..=</span>T::one());</code></pre></div>
<p>Next we check if the generated element is less or equal to our keep probability. If it is, then we return a <strong>1</strong>. If however the generated element is above the keep probability then it&rsquo;s in the dropout range and should be emitted as a <strong>0</strong>. This snippet looks as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust">    <span style="color:#66d9ef">if</span> elem <span style="color:#f92672">&lt;=</span> self.keep_probability {
        T::one()
    } <span style="color:#66d9ef">else</span> {
        T::zero()
    }</code></pre></div>
<p>Finally we can take that iterator that&rsquo;s now emitting random T::zero()&rsquo;s and T::one()&rsquo;s, and collect it into a vector for reshaping.</p>
<p>After we&rsquo;ve got the mask, we just multiply the input by it to get the output:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust">input <span style="color:#f92672">*</span> self.mask.clone()</code></pre></div>
<p>In its entirety, the calculate_output function looks as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_output</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, mode: <span style="color:#a6e22e">Mode</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
    <span style="color:#66d9ef">if</span> mode <span style="color:#f92672">==</span> Mode::Inference {
        self.mask <span style="color:#f92672">=</span> Array::ones(input.raw_dim());
        input <span style="color:#f92672">*</span> self.keep_probability.clone()
    } <span style="color:#66d9ef">else</span> {
        <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> random <span style="color:#f92672">=</span> <span style="color:#66d9ef">match</span> self.seed {
            Some(seed) <span style="color:#f92672">=&gt;</span> StdRng::seed_from_u64(seed),
            None <span style="color:#f92672">=&gt;</span> StdRng::from_rng(thread_rng()).unwrap(),
        };
        <span style="color:#66d9ef">let</span> number_required <span style="color:#f92672">=</span> input.ncols() <span style="color:#f92672">*</span> input.nrows();
        self.mask <span style="color:#f92672">=</span> Array::from_shape_vec(
            (input.nrows(), input.ncols()),
            (<span style="color:#ae81ff">0</span><span style="color:#f92672">..</span>number_required)
                .map(<span style="color:#f92672">|</span>_<span style="color:#f92672">|</span> {
                    <span style="color:#66d9ef">let</span> elem: <span style="color:#a6e22e">T</span> <span style="color:#f92672">=</span> random.gen_range(T::zero()<span style="color:#f92672">..=</span>T::one());
                    <span style="color:#66d9ef">if</span> elem <span style="color:#f92672">&lt;=</span> self.keep_probability {
                        T::one()
                    } <span style="color:#66d9ef">else</span> {
                        T::zero()
                    }
                })
                .collect(),
        )
        .unwrap();
        input <span style="color:#f92672">*</span> self.mask.clone()
    }
}</code></pre></div>
<p>By contrast however, the calculate_input_gradient function is incredibly simple!. The only job for that function is to drop gradients associated with the elements that were dropped in the forward pass. We stored the mask that was generated so this is easy, and in fact I&rsquo;ll paste the code in its entirety here:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_input_gradient</span>(
    <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self,
    output_gradient: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
    _input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
    output_gradient <span style="color:#f92672">*</span> self.mask.clone()
}</code></pre></div>
<p>And with that, our Dropout operation is done!</p>
<h3 id="layersetupadapter">LayerSetupAdapter<a hidden class="anchor" aria-hidden="true" href="#layersetupadapter">#</a></h3>
<p>Now we need to provide a way to ensure that the Dropout operator can be opted into when creating the layers. The best way to do this is to write a wrapper around <strong>any</strong> implementation of a LayerSetup<!-- raw HTML omitted --> and ensure that we also implement the LayerSetup<!-- raw HTML omitted -->.</p>
<p>When layer_setup is called we can call the wrapped one to get the initial list of operations for the layer, and then we can tack a Dropout onto the end. This is called the Composite pattern and is quite pervasive in Rust because we don&rsquo;t have inheritance, instead the user can choose their functionality and combine it like this.</p>
<p>That way we don&rsquo;t need to duplicate the dropout functionality if a different LayerSetup other than DenseLayerSetup is created later.</p>
<p>The struct will be generic over two types:</p>
<ol>
<li><strong>T</strong> - The type of the underlying elements in the array</li>
<li><strong>U</strong> - The type of the concrete instance of LayerSetup that we&rsquo;re actually decorating</li>
</ol>
<p>It also needs to hold onto the keep_probability (also a T) for later during setup. The definition then looks as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#75715e">#[derive(Clone)]</span>
<span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">DropoutLayerAdapter</span><span style="color:#f92672">&lt;</span>T, U<span style="color:#f92672">&gt;</span> {
    layer_setup: <span style="color:#a6e22e">U</span>,
    keep_probability: <span style="color:#a6e22e">T</span>,
}</code></pre></div>
<p>We also need a new_boxed constructor function like we have for our other types, but this isn&rsquo;t that noteworthy so will show it here in it&rsquo;s entirety:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span>T, U<span style="color:#f92672">&gt;</span> DropoutLayerAdapter<span style="color:#f92672">&lt;</span>T, U<span style="color:#f92672">&gt;</span> {
    <span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">new_boxed</span>(layer_setup: <span style="color:#a6e22e">U</span>, keep_probability: <span style="color:#a6e22e">T</span>) -&gt; Box<span style="color:#f92672">&lt;</span>Self<span style="color:#f92672">&gt;</span> {
        Box::new(Self {
            layer_setup,
            keep_probability,
        })
    }
}</code></pre></div>
<p>Finally, we need to actually implement the LayerSetup<!-- raw HTML omitted --> trait for our adapter so that it can stand in for the decorated type in the network. The trait bounds here seem funky:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span>
        T: <span style="color:#f92672">&#39;</span>static <span style="color:#f92672">+</span> Clone <span style="color:#f92672">+</span> Default <span style="color:#f92672">+</span> One <span style="color:#f92672">+</span> PartialOrd <span style="color:#f92672">+</span> SampleUniform <span style="color:#f92672">+</span> ScalarOperand <span style="color:#f92672">+</span> Zero,
        U: Clone <span style="color:#f92672">+</span> <span style="color:#f92672">&#39;</span>static <span style="color:#f92672">+</span> LayerSetup<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>,
    <span style="color:#f92672">&gt;</span> LayerSetup<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">for</span> DropoutLayerAdapter<span style="color:#f92672">&lt;</span>T, U<span style="color:#f92672">&gt;</span>
<span style="color:#66d9ef">where</span>
    Standard: <span style="color:#a6e22e">Distribution</span><span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span></code></pre></div>
<p>However, we don&rsquo;t need any of this functionality to implement the trait, these are simply bounds &ldquo;inherited&rdquo; from the various types in use that we&rsquo;re decorating (specifically LayerSetup, and Dropout).</p>
<p>The implementation is simple also, as we just need to:</p>
<ol>
<li>Call the wrapped LayerSetup to get a list of Operations</li>
<li>Push our Dropout operation onto the end of the list</li>
<li>Return the modified list</li>
</ol>
<p>In code, this equates to:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> operations <span style="color:#f92672">=</span> self.layer_setup.layer_setup(num_neurons, input, seed);
operations.push(Operation::new_basic(Dropout::new_boxed(
    self.keep_probability.clone(),
)));
operations</code></pre></div>
<p>And that&rsquo;s it!</p>
<p>The caller can now create a DenseLayerSetup (or any LayerSetup) either without dropout as before:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust">DenseLayerSetup::new_boxed(Tanh::new_boxed())</code></pre></div>
<p>Or with dropout, by wrapping it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust">DropoutLayerAdapter::new_boxed(DenseLayerSetup::new(Tanh::new_boxed()), KEEP_PROBABILITY)</code></pre></div>
<h3 id="testing">Testing<a hidden class="anchor" aria-hidden="true" href="#testing">#</a></h3>
<p>Finally we can create an example to test this new dropout functionality. We&rsquo;ll take the latest example which included:</p>
<ol>
<li>Softmax cross entropy loss function</li>
<li>Momentum</li>
<li>Exponential learning rate decay</li>
</ol>
<p>And we&rsquo;ll add dropout with a keep probability of 65% to the hidden layer (example is just called &ldquo;dropout&rdquo;)&hellip;.</p>
<pre tabindex="0"><code>Accuracy (training): 91.414%
Accuracy (testing): 91.522%
</code></pre><p>What happened!?</p>
<p>Well, we&rsquo;re dropping out neurons which will make it less likely to overfit, but will also slow down, or set back the training. Recall that dropout lets us increase the <strong>capacity</strong> of the network. Let&rsquo;s go ahead and do that by adding another hidden layer with twice the number of neurons:</p>
<pre tabindex="0"><code>Accuracy (training): 95.372%
Accuracy (testing): 95.322%
</code></pre><p>This is almost the same, but a little lower than we had previously. However we are able to have multiple layers now to model more complex relationships which dropout will cope with. Tweaking hyperparameters some more will likely result in gains.</p>
<p>As a sanity test, let&rsquo;s remove the dropout with this new layer:</p>
<pre tabindex="0"><code>Accuracy (training): 87.46199999999999%
Accuracy (testing): 87.434%
</code></pre><p>So we can see that indeed the dropout operator is letting us expand the layers and capacity of the network while preventing degraded accuracy.</p>


  </div>

  <footer class="post-footer">
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://forgottenmaster.github.io/">Robin Firth</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
