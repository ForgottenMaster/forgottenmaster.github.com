<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>2.3 - Neural Networks | Robin Smith</title>
<meta name="keywords" content="">
<meta name="description" content="The previous post showed the most basic type of machine learning and how to train it, but the concepts learnt there (forward output calculations, backward gradient calculations) will apply equally here too.
What was the problem with linear regression? The main problem with linear regression that we have is that we are training to find a set of weights, which are essentially the coefficients of an N-dimensional line. Therefore we&rsquo;re trying to fit a straight line to a set of points.">
<meta name="author" content="">
<link rel="canonical" href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/neuralnetworks/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.4599eadb9eb2ad3d0a8d6827b41a8fda8f2f4af226b63466c09c5fddbc8706b7.css" integrity="sha256-RZnq256yrT0KjWgntBqP2o8vSvImtjRmwJxf3byHBrc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://forgottenmaster.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://forgottenmaster.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://forgottenmaster.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://forgottenmaster.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://forgottenmaster.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/neuralnetworks/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="2.3 - Neural Networks" />
<meta property="og:description" content="The previous post showed the most basic type of machine learning and how to train it, but the concepts learnt there (forward output calculations, backward gradient calculations) will apply equally here too.
What was the problem with linear regression? The main problem with linear regression that we have is that we are training to find a set of weights, which are essentially the coefficients of an N-dimensional line. Therefore we&rsquo;re trying to fit a straight line to a set of points." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/neuralnetworks/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-02-19T09:43:54+00:00" />
<meta property="article:modified_time" content="2022-02-19T09:43:54+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="2.3 - Neural Networks"/>
<meta name="twitter:description" content="The previous post showed the most basic type of machine learning and how to train it, but the concepts learnt there (forward output calculations, backward gradient calculations) will apply equally here too.
What was the problem with linear regression? The main problem with linear regression that we have is that we are training to find a set of weights, which are essentially the coefficients of an N-dimensional line. Therefore we&rsquo;re trying to fit a straight line to a set of points."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://forgottenmaster.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Machine Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Deep Learning From Scratch",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Chapter 2 - Fundamentals",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/"
    }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "2.3 - Neural Networks",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/neuralnetworks/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "2.3 - Neural Networks",
  "name": "2.3 - Neural Networks",
  "description": "The previous post showed the most basic type of machine learning and how to train it, but the concepts learnt there (forward output calculations, backward gradient calculations) will apply equally here too.\nWhat was the problem with linear regression? The main problem with linear regression that we have is that we are training to find a set of weights, which are essentially the coefficients of an N-dimensional line. Therefore we\u0026rsquo;re trying to fit a straight line to a set of points.",
  "keywords": [
    
  ],
  "articleBody": "The previous post showed the most basic type of machine learning and how to train it, but the concepts learnt there (forward output calculations, backward gradient calculations) will apply equally here too.\nWhat was the problem with linear regression? The main problem with linear regression that we have is that we are training to find a set of weights, which are essentially the coefficients of an N-dimensional line. Therefore we’re trying to fit a straight line to a set of points.\nThe problem here is then that there is only so much we can do to fit a straight line to a set of points, and the error shows this. Linear regression works perfectly if the data set points are located already along some straight line, but in reality they rarely are.\nHow can we fix this? First of all, we need to introduce more tweakable values for the training algorithm to operate on. The more tweakable values that the algorithm has access to, then the more it is able to warp the resulting “line”.\nIn a neural network, what we end up finding and fitting to the data is not an N-dimensional line but an N-dimensional curve. More tweakables means more warp points.\nHowever, no matter how many sets of weights we end up using for training, the functions we use in linear regression will always produce a straight line. So additionally, as part of the modification, we will need to introduce a nonlinear function - that is, one which doesn’t map input values to output values along a straight line but along somme curve instead.\nStep 1: More weights The first part of this is to introduce the ability to have more weights that can be tweaked, and should be fairly straightforward to achieve.\nIn a standard linear regression, the formula we had for the initial “apply weights to observations” step was:\n$$ M = X \\cdot W $$\nThis formula remains the same, however if we look at the dimensionality of X, W, and M in the original linear regression we can see that we have:\n$$ [num\\_observations, num\\_features] \\cdot [num\\_features, 1] = [num\\_observations, 1] $$\nThe idea is that we change the weights from a column vector (matrix with column count of 1) to a matrix with N columns. N here is an arbitrary number of “learned features”, which can be considered nodes in the hidden layer of a neural network.\nThese “learned features” are an abstract notion, they don’t directly map to the actual features of the data set, but rather are features of the patterns in the data.\nIn short these learned features can represent whatever the network requires it to, we just won’t really know what they mean.\nLooking at the dimensionality of the new calculation we see:\n$$ [num\\_observations, num\\_features] \\cdot [num\\_features, N] = [num\\_observations, N] $$\nStep 2: A nonlinear function Once we have the weighted sum matrix, we will go ahead and feed it through a nonlinear function, which will ensure first nonlinearity in the output data, but also ensures that the output data is in the range 0 to 1.\nFor such a function, we will use the sigmoid function which has a graph that looks like this:\nThe sigmoid function is a good nonlinear function to choose for a few reasons:\nThis function preserves information about the input (specifically the sign of it). If we were to use a nonlinear function such as squaring the input, then both -3 and 3 for example would give the same answer. With the sigmoid function -3 and 3 will give different outputs. It is nonlinear. This seems like an obvious point, but it is really important that we choose a nonlinear function for this step. Outputs are always in the range of 0 to 1 which is a nice property to have to keep numbers small. The most important property is that it has a derivative which is calculated easily in terms of itself. This derivative is shown below $$ \\frac {\\partial \\sigma} {\\partial x}(x) = \\sigma(x) \\times (1 - \\sigma(x)) $$\nStep 3: Final linear regression The final step to get the actual answer is to perform one final linear regression on the outputs of the sigmoid function.\nA diagram showing the entire process can be seen below:\nConceptual diagram With the above function graph, we are able to see the actual operations that occur, but doesn’t conceptually show what a neural network looks like. From the implementation point of view the learned features are just columns in the initial matrix multiplication.\nA more common diagram showing what a neural network looks like conceptually can be seen below:\nThis shows the matrix multiplication (the lines between combinations of the input features - the initial nodes), and the output of the matrix multiplication (the middle layer of nodes which represent the learned features). Finally it shows the combination of the intermediate learned features to give a single output loss value as before.\nLoss gradients The overall loss gradient is calculated the same as before, as the mean squared sum of error between the predicted output and the target output for each observation.\nThe difference now is that we have two sets of weights and biases. One of these is for the initial matrix multiplication with the input data set and has N columns for the N learned features.\nThe second set of weights is just a single column vector with the same length as N, that gets multiplied with the learned features to give the final output.\nThe main difference between calculating the gradients in a linear regression and in this neural network is that we must calculate the partial derivatives for both sets of weights and biases.\nAs before, the caveat with calculating the gradient of the biases requires us to sum along the row axis to represent the same bias number being added to each observation.\nThe diagram below shows the overall gradient calculation process - as before we calculate the partial derivatives of each function at the outputs, and using the chain rule, multiply them together to get the final gradient.\nThe following table shows what each partial derivative is. The logic here is already covered previously in chapter 1, with the sigmoid function being explained earlier in this post.\nIn the following table, we use the function ones(X) which is terminology from the Python API and simply takes a matrix X and produces a matrix where each element has the value 1, with the same shape as X (same number of rows and columns).\nNote that in the table the function ^ is actually written as A because I couldn’t manage to get the carat symbol to escape properly in LaTeX.\nDerivative Calculation $$\\frac {\\partial A} {\\partial P}(P, y)$$ $$-(y - P)$$ $$\\frac {\\partial \\alpha} {\\partial M_2}(M_2, B_2)$$ $$ones(M_2)$$ $$\\frac {\\partial \\alpha} {\\partial B_2}(M_2, B_2)$$ $$ones(B_2)$$ $$\\frac {\\partial v} {\\partial W_2}(O_1, W_2)$$ $${O_1}^T$$ $$\\frac {\\partial v} {\\partial O_1}(O_1, W_2)$$ $${W_2}^T$$ $$\\frac {\\partial \\sigma} {\\partial N_1}(N_1)$$ $$\\sigma(N_1) \\times (1 - \\sigma(N_1))$$ $$\\frac {\\partial \\alpha} {\\partial M_1}(M_1, B_1)$$ $$ones(M_1)$$ $$\\frac {\\partial \\alpha} {\\partial B_1}(M_1, B_1)$$ $$ones(B_1)$$ $$\\frac {\\partial v} {\\partial W_1}(X, W_1)$$ $$X^T$$ Code The code is located HERE and is a modification of the previous post’s code to extend in the ways described here.\nThe data set is the same so the errors that we get after training can be compared to the errors we got with a simple linear regression.\nIf the neural network is able to fit the data better then the final total error should be lower.\nConclusion Surprisingly after implementation, it looks like for this data set the neural network is actually worse. As I get the following output:\nFinal error (training): 0.20948163078668652\rFinal error (testing): 0.2119911310559618 This shows that in fact the training error turns out worse than the linear regression method (though very slightly).\nWhen tested on the testing data, it looks like there’s a noticable gap in the error between training and testing.\nI’m not sure what’s causing it however I can speculate on two things:\nWe get around 0.2 error in all cases both for the neural network and the linear regression methods. The neural network has possibly overfit the training data and so doesn’t perform so well on the testing data. This kind of variance is to be expected on certain data sets and neural networks will really shine with more data.\nI have no reason to believe the implementation is incorrect at this point so won’t dwell on this too much and instead will continue on with the abstractions portion of the book.\n",
  "wordCount" : "1435",
  "inLanguage": "en",
  "datePublished": "2022-02-19T09:43:54Z",
  "dateModified": "2022-02-19T09:43:54Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/neuralnetworks/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Robin Smith",
    "logo": {
      "@type": "ImageObject",
      "url": "https://forgottenmaster.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://forgottenmaster.github.io/" accesskey="h" title="Robin Smith (Alt + H)">Robin Smith</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://forgottenmaster.github.io/resume/" title="Résumé">
                    <span>Résumé</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://forgottenmaster.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/">Posts</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/">Machine Learning</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/">Deep Learning From Scratch</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter2/">Chapter 2 - Fundamentals</a></div>
    <h1 class="post-title entry-hint-parent">
      2.3 - Neural Networks
    </h1>
    <div class="post-meta"><span title='2022-02-19 09:43:54 +0000 UTC'>February 19, 2022</span>&nbsp;·&nbsp;7 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#what-was-the-problem-with-linear-regression" aria-label="What was the problem with linear regression?">What was the problem with linear regression?</a></li>
                <li>
                    <a href="#how-can-we-fix-this" aria-label="How can we fix this?">How can we fix this?</a><ul>
                        
                <li>
                    <a href="#step-1-more-weights" aria-label="Step 1: More weights">Step 1: More weights</a></li>
                <li>
                    <a href="#step-2-a-nonlinear-function" aria-label="Step 2: A nonlinear function">Step 2: A nonlinear function</a></li>
                <li>
                    <a href="#step-3-final-linear-regression" aria-label="Step 3: Final linear regression">Step 3: Final linear regression</a></li></ul>
                </li>
                <li>
                    <a href="#conceptual-diagram" aria-label="Conceptual diagram">Conceptual diagram</a></li>
                <li>
                    <a href="#loss-gradients" aria-label="Loss gradients">Loss gradients</a></li>
                <li>
                    <a href="#code" aria-label="Code">Code</a></li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>The <a href="../linearregression">previous post</a> showed the most basic type of machine learning and how to train it, but the concepts learnt there (forward output calculations, backward gradient calculations) will apply equally here too.</p>
<h3 id="what-was-the-problem-with-linear-regression">What was the problem with linear regression?<a hidden class="anchor" aria-hidden="true" href="#what-was-the-problem-with-linear-regression">#</a></h3>
<p>The main problem with linear regression that we have is that we are training to find a set of weights, which are essentially the coefficients of an N-dimensional line. Therefore we&rsquo;re trying to fit a straight line to a set of points.</p>
<p>The problem here is then that there is only so much we can do to fit a straight line to a set of points, and the error shows this. Linear regression works perfectly if the data set points are located already along some straight line, but in reality they rarely are.</p>
<h3 id="how-can-we-fix-this">How can we fix this?<a hidden class="anchor" aria-hidden="true" href="#how-can-we-fix-this">#</a></h3>
<p>First of all, we need to introduce more tweakable values for the training algorithm to operate on. The more tweakable values that the algorithm has access to, then the more it is able to warp the resulting &ldquo;line&rdquo;.</p>
<p>In a neural network, what we end up finding and fitting to the data is not an N-dimensional line but an N-dimensional curve. More tweakables means more warp points.</p>
<p>However, no matter how many sets of weights we end up using for training, the functions we use in linear regression will always <strong>produce</strong> a straight line. So additionally, as part of the modification, we will need to introduce a <em>nonlinear</em> function - that is, one which doesn&rsquo;t map input values to output values along a straight line but along somme curve instead.</p>
<h4 id="step-1-more-weights">Step 1: More weights<a hidden class="anchor" aria-hidden="true" href="#step-1-more-weights">#</a></h4>
<p>The first part of this is to introduce the ability to have more weights that can be tweaked, and should be fairly straightforward to achieve.</p>
<p>In a standard linear regression, the formula we had for the initial &ldquo;apply weights to observations&rdquo; step was:</p>
<p>$$ M = X \cdot W $$</p>
<p>This formula remains the same, however if we look at the dimensionality of X, W, and M in the original linear regression we can see that we have:</p>
<p>$$ [num\_observations, num\_features] \cdot [num\_features, 1] = [num\_observations, 1] $$</p>
<p>The idea is that we change the weights from a column vector (matrix with column count of 1) to a matrix with <em>N</em> columns. N here is an arbitrary number of &ldquo;learned features&rdquo;, which can be considered nodes in the hidden layer of a neural network.</p>
<p>These &ldquo;learned features&rdquo; are an abstract notion, they don&rsquo;t directly map to the actual features of the data set, but rather are features of the <em>patterns</em> in the data.</p>
<p>In short these learned features can represent whatever the network requires it to, we just won&rsquo;t really know what they <em>mean</em>.</p>
<p>Looking at the dimensionality of the new calculation we see:</p>
<p>$$ [num\_observations, num\_features] \cdot [num\_features, N] = [num\_observations, N] $$</p>
<h4 id="step-2-a-nonlinear-function">Step 2: A nonlinear function<a hidden class="anchor" aria-hidden="true" href="#step-2-a-nonlinear-function">#</a></h4>
<p>Once we have the weighted sum matrix, we will go ahead and feed it through a nonlinear function, which will ensure first nonlinearity in the output data, but also ensures that the output data is in the range 0 to 1.</p>
<p>For such a function, we will use the <strong>sigmoid</strong> function which has a graph that looks like this:</p>
<p><img loading="lazy" src="sigmoid_graph.png" alt="Sigmoid Graph"  />
</p>
<p>The sigmoid function is a good nonlinear function to choose for a few reasons:</p>
<ol>
<li>This function preserves information about the input (specifically the sign of it). If we were to use a nonlinear function such as squaring the input, then both -3 and 3 for example would give the same answer. With the sigmoid function -3 and 3 will give different outputs.</li>
<li>It is nonlinear. This seems like an obvious point, but it is really important that we choose a nonlinear function for this step.</li>
<li>Outputs are always in the range of 0 to 1 which is a nice property to have to keep numbers small.</li>
<li>The most important property is that it has a derivative which is calculated easily in terms of itself. This derivative is shown below</li>
</ol>
<p>$$ \frac {\partial \sigma} {\partial x}(x) = \sigma(x) \times (1 - \sigma(x)) $$</p>
<h4 id="step-3-final-linear-regression">Step 3: Final linear regression<a hidden class="anchor" aria-hidden="true" href="#step-3-final-linear-regression">#</a></h4>
<p>The final step to get the actual answer is to perform one final linear regression on the outputs of the sigmoid function.</p>
<p>A diagram showing the entire process can be seen below:</p>
<p><img loading="lazy" src="neural_network_function_graph.png" alt="Neural Network Graph"  />
</p>
<h3 id="conceptual-diagram">Conceptual diagram<a hidden class="anchor" aria-hidden="true" href="#conceptual-diagram">#</a></h3>
<p>With the above function graph, we are able to see the actual operations that occur, but doesn&rsquo;t conceptually show what a neural network looks like. From the implementation point of view the learned features are just columns in the initial matrix multiplication.</p>
<p>A more common diagram showing what a neural network looks like conceptually can be seen below:</p>
<p><img loading="lazy" src="neural_network_diagram.png" alt="Neural Network Diagram"  />
</p>
<p>This shows the matrix multiplication (the lines between combinations of the input features - the initial nodes), and the output of the matrix multiplication (the middle layer of nodes which represent the learned features). Finally it shows the combination of the intermediate learned features to give a single output loss value as before.</p>
<h3 id="loss-gradients">Loss gradients<a hidden class="anchor" aria-hidden="true" href="#loss-gradients">#</a></h3>
<p>The overall loss gradient is calculated the same as before, as the mean squared sum of error between the predicted output and the target output for each observation.</p>
<p>The difference now is that we have <em>two</em> sets of weights and biases. One of these is for the initial matrix multiplication with the input data set and has N columns for the N learned features.</p>
<p>The second set of weights is just a single column vector with the same length as N, that gets multiplied with the learned features to give the final output.</p>
<p>The main difference between calculating the gradients in a linear regression and in this neural network is that we must calculate the partial derivatives for both sets of weights and biases.</p>
<p>As before, the caveat with calculating the gradient of the biases requires us to sum along the row axis to represent the same bias number being added to each observation.</p>
<p>The diagram below shows the overall gradient calculation process - as before we calculate the partial derivatives of each function at the outputs, and using the chain rule, multiply them together to get the final gradient.</p>
<p>The following table shows what each partial derivative is. The logic here is already covered previously in chapter 1, with the sigmoid function being explained earlier in this post.</p>
<p>In the following table, we use the function <strong>ones(X)</strong> which is terminology from the Python API and simply takes a matrix X and produces a matrix where each element has the value 1, with the same <em>shape</em> as X (same number of rows and columns).</p>
<p>Note that in the table the function <strong>^</strong> is actually written as <strong>A</strong> because I couldn&rsquo;t manage to get the carat symbol to escape properly in LaTeX.</p>
<table>
<thead>
<tr>
<th>Derivative</th>
<th>Calculation</th>
</tr>
</thead>
<tbody>
<tr>
<td>$$\frac {\partial A} {\partial P}(P, y)$$</td>
<td>$$-(y - P)$$</td>
</tr>
<tr>
<td>$$\frac {\partial \alpha} {\partial M_2}(M_2, B_2)$$</td>
<td>$$ones(M_2)$$</td>
</tr>
<tr>
<td>$$\frac {\partial \alpha} {\partial B_2}(M_2, B_2)$$</td>
<td>$$ones(B_2)$$</td>
</tr>
<tr>
<td>$$\frac {\partial v} {\partial W_2}(O_1, W_2)$$</td>
<td>$${O_1}^T$$</td>
</tr>
<tr>
<td>$$\frac {\partial v} {\partial O_1}(O_1, W_2)$$</td>
<td>$${W_2}^T$$</td>
</tr>
<tr>
<td>$$\frac {\partial \sigma} {\partial N_1}(N_1)$$</td>
<td>$$\sigma(N_1) \times (1 - \sigma(N_1))$$</td>
</tr>
<tr>
<td>$$\frac {\partial \alpha} {\partial M_1}(M_1, B_1)$$</td>
<td>$$ones(M_1)$$</td>
</tr>
<tr>
<td>$$\frac {\partial \alpha} {\partial B_1}(M_1, B_1)$$</td>
<td>$$ones(B_1)$$</td>
</tr>
<tr>
<td>$$\frac {\partial v} {\partial W_1}(X, W_1)$$</td>
<td>$$X^T$$</td>
</tr>
</tbody>
</table>
<h3 id="code">Code<a hidden class="anchor" aria-hidden="true" href="#code">#</a></h3>
<p>The code is located <a href="/posts/machinelearning/deeplearningfromscratch/chapter2/neuralnetworks/neural_network.zip">HERE</a> and is a modification of the previous post&rsquo;s code to extend in the ways described here.</p>
<p>The data set is the same so the errors that we get after training can be compared to the errors we got with a simple linear regression.</p>
<p>If the neural network is able to fit the data better then the final total error should be lower.</p>
<h3 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h3>
<p>Surprisingly after implementation, it looks like for this data set the neural network is actually worse. As I get the following output:</p>
<pre tabindex="0"><code>Final error (training): 0.20948163078668652
Final error (testing): 0.2119911310559618
</code></pre><p>This shows that in fact the training error turns out worse than the linear regression method (though <em>very</em> slightly).</p>
<p>When tested on the testing data, it looks like there&rsquo;s a noticable gap in the error between training and testing.</p>
<p>I&rsquo;m not sure what&rsquo;s causing it however I can speculate on two things:</p>
<ol>
<li>We get around 0.2 error in all cases both for the neural network and the linear regression methods.</li>
<li>The neural network has possibly <em>overfit</em> the training data and so doesn&rsquo;t perform so well on the testing data.</li>
</ol>
<p>This kind of variance is to be expected on certain data sets and neural networks will really shine with more data.</p>
<p>I have no reason to believe the implementation is incorrect at this point so won&rsquo;t dwell on this too much and instead will continue on with the abstractions portion of the book.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://forgottenmaster.github.io/">Robin Smith</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
