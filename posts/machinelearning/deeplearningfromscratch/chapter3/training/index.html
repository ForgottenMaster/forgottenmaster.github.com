<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>3.8 - Training | Robin Firth</title>
<meta name="keywords" content="" />
<meta name="description" content="The last part to the puzzle, the last thing we need to do is to be able to encapsulate the algorithm for training a given network. In the book, this is represented as the &ldquo;Trainer&rdquo; type which binds together a network with an Optimiser instance. However there is only one implementation for this training process, and customisation is instead via the various trait objects in the Network setup, along with the selected Optimiser.">
<meta name="author" content="">
<link rel="canonical" href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/training/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css" integrity="sha256-b2AFbUTT9&#43;tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://forgottenmaster.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://forgottenmaster.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://forgottenmaster.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://forgottenmaster.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://forgottenmaster.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.96.0" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="3.8 - Training" />
<meta property="og:description" content="The last part to the puzzle, the last thing we need to do is to be able to encapsulate the algorithm for training a given network. In the book, this is represented as the &ldquo;Trainer&rdquo; type which binds together a network with an Optimiser instance. However there is only one implementation for this training process, and customisation is instead via the various trait objects in the Network setup, along with the selected Optimiser." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/training/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-09T16:46:42&#43;01:00" />
<meta property="article:modified_time" content="2022-04-09T16:46:42&#43;01:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="3.8 - Training"/>
<meta name="twitter:description" content="The last part to the puzzle, the last thing we need to do is to be able to encapsulate the algorithm for training a given network. In the book, this is represented as the &ldquo;Trainer&rdquo; type which binds together a network with an Optimiser instance. However there is only one implementation for this training process, and customisation is instead via the various trait objects in the Network setup, along with the selected Optimiser."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://forgottenmaster.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Machine Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Deep Learning From Scratch",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Chapter 3 - Deep Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/"
    }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "3.8 - Training",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/training/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "3.8 - Training",
  "name": "3.8 - Training",
  "description": "The last part to the puzzle, the last thing we need to do is to be able to encapsulate the algorithm for training a given network. In the book, this is represented as the \u0026ldquo;Trainer\u0026rdquo; type which binds together a network with an Optimiser instance. However there is only one implementation for this training process, and customisation is instead via the various trait objects in the Network setup, along with the selected Optimiser.",
  "keywords": [
    
  ],
  "articleBody": "The last part to the puzzle, the last thing we need to do is to be able to encapsulate the algorithm for training a given network. In the book, this is represented as the “Trainer” type which binds together a network with an Optimiser instance. However there is only one implementation for this training process, and customisation is instead via the various trait objects in the Network setup, along with the selected Optimiser.\nTherefore making it a type for no reason (a good reason would be to use it as a trait object for example) just results in more code. For now, training is just done via a single function which will take the Network and Optimiser to use.\nAlgorithm As a reminder of the process that we will go through when we train, here is an overview of the sequence of steps that we will run each epoch\n Clone the network if needed. We provide a tweakable value for the frequency to evaluate the network at (i.e. every X epochs). In the case that after evaluation the network is performing worse than before, then we terminate the training early and return to the previous network state. To restore the network state, we need to clone the network if it’s one of these evaluation epochs. Permute the data. When we feed the training data into the network, we feed it in a different order each epoch. Feeding data in the same batches each time can result in overfitting. permuting the samples each time helps the network to generalise. Generate batches. We don’t necessarily feed all the data into the network at once (we might do though, or might feed them one by one, or somewhere in between). We provide a parameter to the function to determine how large each batch of data is when we feed it through the network. For each batch…  Tell the network to perform a forward, and then a backward pass to calculate/update the parameter gradients. Tell the optmiser to apply its optimisation algorithm to update the parameters in the network for next epoch   If this is an evaluation epoch, check the loss value of the network after training…  If it’s worse then abort training and return the previous snapshot of the network If it’s better, continue training and update the best loss value encountered for next evaluation    In order to achieve these we will need the following three functions, which we will cover in the following sections:\n A function to permute the rows of the training data A function to batch the rows of the training data up A function to orchestrate the training algorithm  Permuting the data As mentioned in the overview, to avoid overfitting the network to a specific setup or order of data, we want to shuffle the rows of the training data set around. Note that we want to always keep each row with its corresponding entry in the training target array.\nThis means that as we’re shuffling, we need to apply the same ordering to both the training input data, and the training targets arrays.\nIn order to permute the data, we will essentially be shuffling the rows of the data in tandem with their target counterparts in the targets array. This means we must zip each row of the input data array with row in the targets array, to get a single list of tuples that we can then shuffle.\nAfter shuffling, we need to re-split the data again into arrays to pass back.\nNOTE: There’s definitely a better way of building the output/permuted arrays than what I’m doing here, and there’ll probably be an optimisation stage coming up before moving to the next chapter of the book which may also include refactoring the API to remove unnecessary traits, etc.\nSignature The signature of the permute_data method looks as follows:\nfn permute_dataT: Clone + Default(  batch: \u0026ArrayT, Ix2,  targets: \u0026ArrayT, Ix2,  seed: Optionu64, ) - (ArrayT, Ix2, ArrayT, Ix2) We require Cloning on the elements of the array, due to us using the append method to add rows to the arrays we’re building up, this requires the elements to be cloned into the storage location. We require Default due to us wanting to initialise the output arrays with a set number of columns. Despite initialising with 0 rows, the Array::default method requires the type T to be default constructible.\nFor the other parameters, we take the batch of data to permute, and the targets array. We also take an optional random seed which is used to determine the random permutation order for the rows. We need to be able to specify this seed for unit testing, but in practice this will be given as None for “true” randomness.\nThe return values from this function are of course the permuted batch and targets arrays. These are returned by value as they are brand new arrays produced using the inputs as a basis.\nShape checking The first step in the permutation is to cache the number of rows and columns in the batch and targets arrays (we expect the targets to only be a single column but it’s indeed possible to produce more). We do require that the number of rows in the batch and targets arrays is the same though:\nlet (batch_row_count, batch_col_count) = (batch.nrows(), batch.ncols()); let (targets_row_count, targets_col_count) = (targets.nrows(), targets.ncols()); assert_eq!(batch_row_count, targets_row_count); Zipping the arrays In order to shuffle the batches and targets together we will need to zip them into a single vector. First step is to get an iterator that produces one row at a time from the Array. We can then zip those iterators together to get an iterator that produces a tuple with each step. The tuple has a row from each of the two arrays in it.\nIn order to produce an iterator over the rows of the array we can firstly get a flat slice of the data in the batch. This slice will have a row every NUM_COLS elements. We can use the chunks method on slices to get an iterator over chunks of size NUM_COLS which gives us the rows of the array.\nThis all looks as follows once coded up:\nlet batch_rows = batch.as_slice().unwrap().chunks(batch_col_count); let targets_rows = targets.as_slice().unwrap().chunks(targets_col_count); let mut zipped_rows = batch_rows.zip(targets_rows).collect::Vec_(); Creating the RNG Next we need the random number generator. This construction is basically the same as we have for initialising the weights in the network and constructs either from the given seed if there is one, or constructs a random generator:\nlet mut random_generator = match seed {  Some(seed) = StdRng::seed_from_u64(seed),  None = StdRng::from_rng(thread_rng()).unwrap(), }; Shuffling the data Shuffling a vector with a random generator is an extension method provided to us by the rand crate. We can simply call it and pass a mutable reference to the generator. The reference needs to be mutable because the generator internally contains state it has to update upon generating a random number, in order to affect the next one produced.\nThis is a single line:\nzipped_rows.shuffle(\u0026mut random_generator); Building the permuted arrays In order to build the permuted arrays for returning, we will start with empty arrays, and then upon traversing the zipped vector, will add each row to the end of the appropriate array. This is analagous to building a Vecby iterating and pushing elements on one at a time. This is likely one of the things that isn’t very efficient, but as a starting point it works.\nThere are a few ways that we can construct the initial Array instances, however we want to make sure that we are constructing two dimensional arrays, and that the number of columns we’re specifying is correct. The choice I picked for initialising the arrays is using the default constructor. This takes a shape description. Initially we’ll fix the number of columns, but will have 0 rows that will be extended as we append.\nThis default code looks as follows:\nlet mut batch = Array::default((0, batch_col_count)); let mut targets = Array::default((0, targets_col_count)); For the appending of the rows, we iterate over the entries in the zipped array. Remember that each entry in the zipped array is a tuple, where the first element is a row from the batch array, and the second is the corresponding row from the targets array.\nThe structure of the iteration then is as follows:\nzipped_rows  .into_iter()  .for_each(|(batch_row, targets_row)| {  // append code follows  }); Only one thing remains, which is to actually append batch_row to the batch array we’re building, and targets_row to the targets array we’re building. As mentioned before, we use the append method on the array. This method takes two parameters:\n An axis along which we’re appending the new data - in our case this will be the 0th axis (rows) as we’re adding each row as a row in the Array. An array or array view to append - in this case, we just shape the row data into an ArrayView with 1 row, and the requisite number of columns.  The code for this when updating the batch looks like follows:\nbatch  .append(  Axis(0),  ArrayView::from_shape((1, batch_col_count), batch_row).unwrap(),  )  .unwrap(); And the targets follows the same format:\ntargets  .append(  Axis(0),  ArrayView::from_shape((1, targets_col_count), targets_row).unwrap(),  )  .unwrap(); Generating the batches In comparison to the function to permute the arrays, generating the batches to feed into the network for training is shorter. Essentially what we would like is to get an iterator over two arrays (the batch and targets arrays).\nEach element in this iterator will be a pair of new arrays. One array will be a batch of rows from the “batches” input array, and the next will be a batch of rows from the “targets” input array.\nSince we are yielding new arrays from the iterator, we need elements to be cloneable, which is why we require the element type in the arrays to be cloneable. Since we don’t require ownership of the input arrays and are simply producing new arrays from them, we take references to them. However, since we’re yielding an iterator over those arrays, we must make sure the lifetime of the produced iterator is tied to them, so we can’t drop them before being finished with the iterator.\nThis results in the following signature:\nfn generate_batchesa, T: Clone(  batch: \u0026'a ArrayT, Ix2,  targets: \u0026'a ArrayT, Ix2,  size: usize, ) - impl IteratorItem = (ArrayT, Ix2, ArrayT, Ix2) + 'a Firstly, we need to make sure that both input arrays have the same number of rows. Each yielded tuple is a row from each, so they must match:\nassert_eq!(batch.nrows(), targets.nrows()); In order to produce the desired iterator we need to do the following:\n Get an iterator over chunks of size rows from the batch array Get an iterator over chunks of size rows from the targets array Zip the two together to get an iterator over chunks from both arrays Map this tuple of array views into a tuple of owned arrays  For steps 1 and 2 we can use the built in method on arrays axis_chunks_iter which takes an axis along which to iterate (again we will iterate over the 0th axis - the rows), and the size we would like each cunk to be.\nFor step 3 we use zip to zip the iterators together.\nFor step 4 we can use the map adapter, and use the to_owned method to turn the ArrayView chunks into owned Array instances.\nThe full code for this will look as follows:\nbatch  .axis_chunks_iter(Axis(0), size)  .zip(targets.axis_chunks_iter(Axis(0), size))  .map(|(view1, view2)| (view1.to_owned(), view2.to_owned())) The training function Finally we can implement the main attraction, the function which will take the network to train, an optimisation strategy, and the training+testing data, along with other parameters and actually run the training process on the network.\nThe signature for it isn’t exactly pretty:\npub fn trainT: Clone + Default + PartialOrd(  network: \u0026mut NetworkT,  optimiser: \u0026mut impl OptimiserT,  batch_train: ArrayT, Ix2,  targets_train: ArrayT, Ix2,  batch_test: ArrayT, Ix2,  targets_test: ArrayT, Ix2,  epochs: u32,  eval_every: u32,  batch_size: usize,  seed: Optionu64, ) Firstly an explanation of the traits in use:\n Clone: We need to clone the Networkin order to create a snapshot/undo point for early termination. We can only do this if T itself is Clone Default: This is required because as explained before, permute_data requires T to be Default PartialOrd: We need to evaluate the calculated loss from the network against the previous best loss for early termination. In order to do this, we need the loss value (of type T) to support comparison  Next, we’ll explain the parameters of the train function (although they’ll be fairly self explanatory):\n network: The neural network to train, taken my mutable reference as we need to update the parameters within optimiser: The optimisation strategy to use, also taken by mutable reference, in case the optimiser stores state between epochs batch_train: The input data from the training batch targets_train: The output data/targets for the training batch batch_test: The input data from the testing batch targets_test: The output data/targets for the testing batch epochs: The maximum number of iterations of training that we will go through eval_every: The frequency at which we evaluate the loss, and revert to the previous state/abort if it regresses batch_size: The size of each batch we feed through the network during a given epoch seed: An optional seed for the RNG when training. During testing this will be provided, during normal use it should be None  Note that epochs is the maximum number of epochs we go through, since we’re implementing early stopping if the loss regresses and starts to diverge from the desired loss.\nWe make a variable to track the “best” loss value during evaluation, which needs to be mutable as we upate it as we train. The actual iteration over the epochs is as expected:\nlet mut best_loss = None; for e in 0..epochs {  // training code here } Note that we do need to use a traditional for loop, rather than a for_each call on the iterator because we can’t early abort the for_each style where we can break out of a traditional loop.\nThe first step in the epoch is to store off a snapshot of the Network if necessary. We don’t need to clone the network every epoch, just at the frequency we are evaluating. We therefore use an Option to store the snapshot:\nlet last_model = if (e + 1) % eval_every == 0 {  Some((*network).clone()) } else {  None }; Secondly comes the actual training for the epoch. The sequence of steps for this will be:\n Permute the training data Generate batches from the training data For each batch, train the network, and step the optimiser  In code this looks like the following:\nlet (batch_train, targets_train) = permute_data(\u0026batch_train, \u0026targets_train, seed.clone()); generate_batches(\u0026batch_train, \u0026targets_train, batch_size).for_each(|(batch, targets)| {  network.train(batch, targets);  optimiser.step(network); }); Finally, after training the network for one epoch, if we need to evaluate the loss, we do that. We can use the forward_loss method on the testing data in order to get the predictions (which we won’t use), and the loss value (which we will). If the best loss so far is None, or if this one is better, then we update the best loss value and continue. In the case that the loss is now worse than we had, we revert to the snapshot and break the training process.\nCode for all of this is shown below:\nif let Some(last_model) = last_model {  let (_, test_loss) = network.forward_loss(batch_test.clone(), targets_test.clone());  if best_loss.is_none() || test_loss  *best_loss.as_ref().unwrap() {  best_loss = Some(test_loss);  } else {  *network = last_model;  break;  } } ",
  "wordCount" : "2596",
  "inLanguage": "en",
  "datePublished": "2022-04-09T16:46:42+01:00",
  "dateModified": "2022-04-09T16:46:42+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/training/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Robin Firth",
    "logo": {
      "@type": "ImageObject",
      "url": "https://forgottenmaster.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://forgottenmaster.github.io/" accesskey="h" title="Robin Firth (Alt + H)">Robin Firth</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://forgottenmaster.github.io/resume/" title="Résumé">
                    <span>Résumé</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://forgottenmaster.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/">Posts</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/">Machine Learning</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/">Deep Learning From Scratch</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/">Chapter 3 - Deep Learning</a></div>
    <h1 class="post-title">
      3.8 - Training
    </h1>
    <div class="post-meta"><span title='2022-04-09 16:46:42 +0100 BST'>April 9, 2022</span>&nbsp;·&nbsp;13 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#algorithm" aria-label="Algorithm">Algorithm</a></li>
                <li>
                    <a href="#permuting-the-data" aria-label="Permuting the data">Permuting the data</a><ul>
                        
                <li>
                    <a href="#signature" aria-label="Signature">Signature</a></li>
                <li>
                    <a href="#shape-checking" aria-label="Shape checking">Shape checking</a></li>
                <li>
                    <a href="#zipping-the-arrays" aria-label="Zipping the arrays">Zipping the arrays</a></li>
                <li>
                    <a href="#creating-the-rng" aria-label="Creating the RNG">Creating the RNG</a></li>
                <li>
                    <a href="#shuffling-the-data" aria-label="Shuffling the data">Shuffling the data</a></li>
                <li>
                    <a href="#building-the-permuted-arrays" aria-label="Building the permuted arrays">Building the permuted arrays</a></li></ul>
                </li>
                <li>
                    <a href="#generating-the-batches" aria-label="Generating the batches">Generating the batches</a></li>
                <li>
                    <a href="#the-training-function" aria-label="The training function">The training function</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>The last part to the puzzle, the last thing we need to do is to be able to encapsulate the algorithm for training a given network. In the book, this is represented as the &ldquo;Trainer&rdquo; type which binds together a network with an Optimiser instance. However there is only one implementation for this training process, and customisation is instead via the various trait objects in the Network setup, along with the selected Optimiser.</p>
<p>Therefore making it a type for no reason (a good reason would be to use it as a trait object for example) just results in more code. For now, training is just done via a single function which will take the Network and Optimiser to use.</p>
<h3 id="algorithm">Algorithm<a hidden class="anchor" aria-hidden="true" href="#algorithm">#</a></h3>
<p>As a reminder of the process that we will go through when we train, here is an overview of the sequence of steps that we will run each <strong>epoch</strong></p>
<ol>
<li>Clone the network if needed. We provide a tweakable value for the frequency to evaluate the network at (i.e. every X epochs). In the case that after evaluation the network is performing <strong>worse</strong> than before, then we terminate the training early and return to the previous network state. To restore the network state, we need to clone the network if it&rsquo;s one of these evaluation epochs.</li>
<li>Permute the data. When we feed the training data into the network, we feed it in a different order each epoch. Feeding data in the same batches each time can result in overfitting. permuting the samples each time helps the network to generalise.</li>
<li>Generate batches. We don&rsquo;t necessarily feed all the data into the network at once (we might do though, or might feed them one by one, or somewhere in between). We provide a parameter to the function to determine how large each batch of data is when we feed it through the network.</li>
<li>For each batch&hellip;
<ol>
<li>Tell the network to perform a forward, and then a backward pass to calculate/update the parameter gradients.</li>
<li>Tell the optmiser to apply its optimisation algorithm to update the parameters in the network for next epoch</li>
</ol>
</li>
<li>If this is an evaluation epoch, check the loss value of the network after training&hellip;
<ol>
<li>If it&rsquo;s worse then abort training and return the previous snapshot of the network</li>
<li>If it&rsquo;s better, continue training and update the best loss value encountered for next evaluation</li>
</ol>
</li>
</ol>
<p>In order to achieve these we will need the following three functions, which we will cover in the following sections:</p>
<ol>
<li>A function to permute the rows of the training data</li>
<li>A function to batch the rows of the training data up</li>
<li>A function to orchestrate the training algorithm</li>
</ol>
<h3 id="permuting-the-data">Permuting the data<a hidden class="anchor" aria-hidden="true" href="#permuting-the-data">#</a></h3>
<p>As mentioned in the overview, to avoid overfitting the network to a specific setup or order of data, we want to shuffle the <strong>rows</strong> of the training data set around. Note that we want to always keep each row with its corresponding entry in the training target array.</p>
<p>This means that as we&rsquo;re shuffling, we need to apply the same ordering to both the training input data, and the training targets arrays.</p>
<p>In order to permute the data, we will essentially be shuffling the rows of the data in tandem with their target counterparts in the targets array. This means we must zip each row of the input data array with row in the targets array, to get a <em>single</em> list of <strong>tuples</strong> that we can then shuffle.</p>
<p>After shuffling, we need to re-split the data again into arrays to pass back.</p>
<p>NOTE: There&rsquo;s <strong>definitely</strong> a better way of building the output/permuted arrays than what I&rsquo;m doing here, and there&rsquo;ll probably be an optimisation stage coming up before moving to the next chapter of the book which may also include refactoring the API to remove unnecessary traits, etc.</p>
<h4 id="signature">Signature<a hidden class="anchor" aria-hidden="true" href="#signature">#</a></h4>
<p>The signature of the permute_data method looks as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">permute_data</span><span style="color:#f92672">&lt;</span>T: Clone <span style="color:#f92672">+</span> Default<span style="color:#f92672">&gt;</span>(
</span></span><span style="display:flex;"><span>    batch: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    targets: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    seed: Option<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">u64</span><span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>) -&gt; (Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>)</span></span></code></pre></div>
<p>We require Cloning on the elements of the array, due to us using the <strong>append</strong> method to add rows to the arrays we&rsquo;re building up, this requires the elements to be cloned into the storage location. We require Default due to us wanting to initialise the output arrays with a set number of columns. Despite initialising with 0 rows, the Array::default method requires the type T to be default constructible.</p>
<p>For the other parameters, we take the batch of data to permute, and the targets array. We also take an optional random seed which is used to determine the random permutation order for the rows. We need to be able to specify this seed for unit testing, but in practice this will be given as None for &ldquo;true&rdquo; randomness.</p>
<p>The return values from this function are of course the permuted batch and targets arrays. These are returned by value as they are brand new arrays produced using the inputs as a basis.</p>
<h4 id="shape-checking">Shape checking<a hidden class="anchor" aria-hidden="true" href="#shape-checking">#</a></h4>
<p>The first step in the permutation is to cache the number of rows and columns in the batch and targets arrays (we expect the targets to only be a single column but it&rsquo;s indeed possible to produce more). We do require that the number of <em>rows</em> in the batch and targets arrays is the same though:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> (batch_row_count, batch_col_count) <span style="color:#f92672">=</span> (batch.nrows(), batch.ncols());
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> (targets_row_count, targets_col_count) <span style="color:#f92672">=</span> (targets.nrows(), targets.ncols());
</span></span><span style="display:flex;"><span>assert_eq!(batch_row_count, targets_row_count);</span></span></code></pre></div>
<h4 id="zipping-the-arrays">Zipping the arrays<a hidden class="anchor" aria-hidden="true" href="#zipping-the-arrays">#</a></h4>
<p>In order to shuffle the batches and targets together we will need to zip them into a single vector. First step is to get an iterator that produces one row at a time from the Array. We can then zip those iterators together to get an iterator that produces a tuple with each step. The tuple has a row from each of the two arrays in it.</p>
<p>In order to produce an iterator over the rows of the array we can firstly get a flat slice of the data in the batch. This slice will have a row every NUM_COLS elements. We can use the <strong>chunks</strong> method on slices to get an iterator over chunks of size NUM_COLS which gives us the rows of the array.</p>
<p>This all looks as follows once coded up:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> batch_rows <span style="color:#f92672">=</span> batch.as_slice().unwrap().chunks(batch_col_count);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> targets_rows <span style="color:#f92672">=</span> targets.as_slice().unwrap().chunks(targets_col_count);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> zipped_rows <span style="color:#f92672">=</span> batch_rows.zip(targets_rows).collect::<span style="color:#f92672">&lt;</span>Vec<span style="color:#f92672">&lt;</span>_<span style="color:#f92672">&gt;&gt;</span>();</span></span></code></pre></div>
<h4 id="creating-the-rng">Creating the RNG<a hidden class="anchor" aria-hidden="true" href="#creating-the-rng">#</a></h4>
<p>Next we need the random number generator. This construction is basically the same as we have for initialising the weights in the network and constructs either from the given seed if there is one, or constructs a random generator:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> random_generator <span style="color:#f92672">=</span> <span style="color:#66d9ef">match</span> seed {
</span></span><span style="display:flex;"><span>    Some(seed) <span style="color:#f92672">=&gt;</span> StdRng::seed_from_u64(seed),
</span></span><span style="display:flex;"><span>    None <span style="color:#f92672">=&gt;</span> StdRng::from_rng(thread_rng()).unwrap(),
</span></span><span style="display:flex;"><span>};</span></span></code></pre></div>
<h4 id="shuffling-the-data">Shuffling the data<a hidden class="anchor" aria-hidden="true" href="#shuffling-the-data">#</a></h4>
<p>Shuffling a vector with a random generator is an extension method provided to us by the rand crate. We can simply call it and pass a mutable reference to the generator. The reference needs to be mutable because the generator internally contains state it has to update upon generating a random number, in order to affect the next one produced.</p>
<p>This is a single line:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>zipped_rows.shuffle(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> random_generator);</span></span></code></pre></div>
<h4 id="building-the-permuted-arrays">Building the permuted arrays<a hidden class="anchor" aria-hidden="true" href="#building-the-permuted-arrays">#</a></h4>
<p>In order to build the permuted arrays for returning, we will start with empty arrays, and then upon traversing the zipped vector, will add each row to the end of the appropriate array. This is analagous to building a Vec<!-- raw HTML omitted --> by iterating and pushing elements on one at a time. This is likely one of the things that isn&rsquo;t very efficient, but as a starting point it works.</p>
<p>There are a few ways that we can construct the initial Array instances, however we want to make sure that we are constructing <strong>two dimensional</strong> arrays, and that the number of columns we&rsquo;re specifying is correct. The choice I picked for initialising the arrays is using the default constructor. This takes a shape description. Initially we&rsquo;ll fix the number of columns, but will have 0 rows that will be extended as we append.</p>
<p>This default code looks as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> batch <span style="color:#f92672">=</span> Array::default((<span style="color:#ae81ff">0</span>, batch_col_count));
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> targets <span style="color:#f92672">=</span> Array::default((<span style="color:#ae81ff">0</span>, targets_col_count));</span></span></code></pre></div>
<p>For the appending of the rows, we iterate over the entries in the zipped array. Remember that each entry in the zipped array is a tuple, where the first element is a row from the batch array, and the second is the corresponding row from the targets array.</p>
<p>The structure of the iteration then is as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>zipped_rows
</span></span><span style="display:flex;"><span>    .into_iter()
</span></span><span style="display:flex;"><span>    .for_each(<span style="color:#f92672">|</span>(batch_row, targets_row)<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">// append code follows
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    });</span></span></code></pre></div>
<p>Only one thing remains, which is to actually append batch_row to the batch array we&rsquo;re building, and targets_row to the targets array we&rsquo;re building. As mentioned before, we use the <strong>append</strong> method on the array. This method takes two parameters:</p>
<ol>
<li>An axis along which we&rsquo;re appending the new data - in our case this will be the 0th axis (rows) as we&rsquo;re adding each row as a row in the Array.</li>
<li>An array or array view to append - in this case, we just shape the row data into an ArrayView with 1 row, and the requisite number of columns.</li>
</ol>
<p>The code for this when updating the batch looks like follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>batch
</span></span><span style="display:flex;"><span>    .append(
</span></span><span style="display:flex;"><span>        Axis(<span style="color:#ae81ff">0</span>),
</span></span><span style="display:flex;"><span>        ArrayView::from_shape((<span style="color:#ae81ff">1</span>, batch_col_count), batch_row).unwrap(),
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    .unwrap();</span></span></code></pre></div>
<p>And the targets follows the same format:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>targets
</span></span><span style="display:flex;"><span>    .append(
</span></span><span style="display:flex;"><span>        Axis(<span style="color:#ae81ff">0</span>),
</span></span><span style="display:flex;"><span>        ArrayView::from_shape((<span style="color:#ae81ff">1</span>, targets_col_count), targets_row).unwrap(),
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    .unwrap();</span></span></code></pre></div>
<h3 id="generating-the-batches">Generating the batches<a hidden class="anchor" aria-hidden="true" href="#generating-the-batches">#</a></h3>
<p>In comparison to the function to permute the arrays, generating the batches to feed into the network for training is shorter. Essentially what we would like is to get an iterator over two arrays (the batch and targets arrays).</p>
<p>Each element in this iterator will be a pair of new arrays. One array will be a batch of rows from the &ldquo;batches&rdquo; input array, and the next will be a batch of rows from the &ldquo;targets&rdquo; input array.</p>
<p>Since we are yielding new arrays from the iterator, we need elements to be cloneable, which is why we require the element type in the arrays to be cloneable. Since we don&rsquo;t require ownership of the input arrays and are simply producing new arrays from them, we take references to them. However, since we&rsquo;re yielding an iterator over those arrays, we must make sure the lifetime of the produced iterator is tied to them, so we can&rsquo;t drop them before being finished with the iterator.</p>
<p>This results in the following signature:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">generate_batches</span><span style="color:#f92672">&lt;&#39;</span><span style="color:#a6e22e">a</span>, T: Clone<span style="color:#f92672">&gt;</span>(
</span></span><span style="display:flex;"><span>    batch: <span style="color:#66d9ef">&amp;</span><span style="color:#f92672">&#39;</span><span style="color:#a6e22e">a</span> <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    targets: <span style="color:#66d9ef">&amp;</span><span style="color:#f92672">&#39;</span><span style="color:#a6e22e">a</span> <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    size: <span style="color:#66d9ef">usize</span>,
</span></span><span style="display:flex;"><span>) -&gt; <span style="color:#a6e22e">impl</span> Iterator<span style="color:#f92672">&lt;</span>Item <span style="color:#f92672">=</span> (Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>)<span style="color:#f92672">&gt;</span> <span style="color:#f92672">+</span> <span style="color:#f92672">&#39;</span><span style="color:#a6e22e">a</span></span></span></code></pre></div>
<p>Firstly, we need to make sure that both input arrays have the same number of rows. Each yielded tuple is a row from each, so they must match:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>assert_eq!(batch.nrows(), targets.nrows());</span></span></code></pre></div>
<p>In order to produce the desired iterator we need to do the following:</p>
<ol>
<li>Get an iterator over chunks of <strong>size</strong> rows from the batch array</li>
<li>Get an iterator over chunks of <strong>size</strong> rows from the targets array</li>
<li>Zip the two together to get an iterator over chunks from both arrays</li>
<li>Map this tuple of array views into a tuple of owned arrays</li>
</ol>
<p>For steps 1 and 2 we can use the built in method on arrays <strong>axis_chunks_iter</strong> which takes an axis along which to iterate (again we will iterate over the 0th axis - the rows), and the size we would like each cunk to be.</p>
<p>For step 3 we use zip to zip the iterators together.</p>
<p>For step 4 we can use the map adapter, and use the <strong>to_owned</strong> method to turn the ArrayView chunks into owned Array instances.</p>
<p>The full code for this will look as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>batch
</span></span><span style="display:flex;"><span>    .axis_chunks_iter(Axis(<span style="color:#ae81ff">0</span>), size)
</span></span><span style="display:flex;"><span>    .zip(targets.axis_chunks_iter(Axis(<span style="color:#ae81ff">0</span>), size))
</span></span><span style="display:flex;"><span>    .map(<span style="color:#f92672">|</span>(view1, view2)<span style="color:#f92672">|</span> (view1.to_owned(), view2.to_owned()))</span></span></code></pre></div>
<h3 id="the-training-function">The training function<a hidden class="anchor" aria-hidden="true" href="#the-training-function">#</a></h3>
<p>Finally we can implement the main attraction, the function which will take the network to train, an optimisation strategy, and the training+testing data, along with other parameters and actually run the training process on the network.</p>
<p>The signature for it isn&rsquo;t exactly pretty:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">train</span><span style="color:#f92672">&lt;</span>T: Clone <span style="color:#f92672">+</span> Default <span style="color:#f92672">+</span> PartialOrd<span style="color:#f92672">&gt;</span>(
</span></span><span style="display:flex;"><span>    network: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">mut</span> Network<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    optimiser: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">mut</span> <span style="color:#66d9ef">impl</span> Optimiser<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    batch_train: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    targets_train: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    batch_test: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    targets_test: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    epochs: <span style="color:#66d9ef">u32</span>,
</span></span><span style="display:flex;"><span>    eval_every: <span style="color:#66d9ef">u32</span>,
</span></span><span style="display:flex;"><span>    batch_size: <span style="color:#66d9ef">usize</span>,
</span></span><span style="display:flex;"><span>    seed: Option<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">u64</span><span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>)</span></span></code></pre></div>
<p>Firstly an explanation of the traits in use:</p>
<ol>
<li>Clone: We need to clone the Network<!-- raw HTML omitted --> in order to create a snapshot/undo point for early termination. We can only do this if T itself is Clone</li>
<li>Default: This is required because as explained before, permute_data requires T to be Default</li>
<li>PartialOrd: We need to evaluate the calculated loss from the network against the previous best loss for early termination. In order to do this, we need the loss value (of type T) to support comparison</li>
</ol>
<p>Next, we&rsquo;ll explain the parameters of the train function (although they&rsquo;ll be fairly self explanatory):</p>
<ol>
<li>network: The neural network to train, taken my <strong>mutable</strong> reference as we need to update the parameters within</li>
<li>optimiser: The optimisation strategy to use, also taken by mutable reference, in case the optimiser stores state between epochs</li>
<li>batch_train: The input data from the training batch</li>
<li>targets_train: The output data/targets for the training batch</li>
<li>batch_test: The input data from the <em>testing</em> batch</li>
<li>targets_test: The output data/targets for the <em>testing</em> batch</li>
<li>epochs: The <strong>maximum</strong> number of iterations of training that we will go through</li>
<li>eval_every: The frequency at which we evaluate the loss, and revert to the previous state/abort if it regresses</li>
<li>batch_size: The size of each batch we feed through the network during a given epoch</li>
<li>seed: An <em>optional</em> seed for the RNG when training. During testing this will be provided, during normal use it should be None</li>
</ol>
<p>Note that epochs is the maximum number of epochs we go through, since we&rsquo;re implementing early stopping if the loss regresses and starts to diverge from the desired loss.</p>
<p>We make a variable to track the &ldquo;best&rdquo; loss value during evaluation, which needs to be mutable as we upate it as we train. The actual iteration over the epochs is as expected:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> best_loss <span style="color:#f92672">=</span> None;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> e <span style="color:#66d9ef">in</span> <span style="color:#ae81ff">0</span><span style="color:#f92672">..</span>epochs {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// training code here
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>}</span></span></code></pre></div>
<p>Note that we do need to use a traditional for loop, rather than a for_each call on the iterator because we can&rsquo;t early abort the for_each style where we can break out of a traditional loop.</p>
<p>The first step in the epoch is to store off a snapshot of the Network <strong>if necessary</strong>. We don&rsquo;t need to clone the network every epoch, just at the frequency we are evaluating. We therefore use an Option&lt;Network<!-- raw HTML omitted -->&gt; to store the snapshot:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> last_model <span style="color:#f92672">=</span> <span style="color:#66d9ef">if</span> (e <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">%</span> eval_every <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> {
</span></span><span style="display:flex;"><span>    Some((<span style="color:#f92672">*</span>network).clone())
</span></span><span style="display:flex;"><span>} <span style="color:#66d9ef">else</span> {
</span></span><span style="display:flex;"><span>    None
</span></span><span style="display:flex;"><span>};</span></span></code></pre></div>
<p>Secondly comes the actual training for the epoch. The sequence of steps for this will be:</p>
<ol>
<li>Permute the training data</li>
<li>Generate batches from the training data</li>
<li>For each batch, train the network, and step the optimiser</li>
</ol>
<p>In code this looks like the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> (batch_train, targets_train) <span style="color:#f92672">=</span> permute_data(<span style="color:#f92672">&amp;</span>batch_train, <span style="color:#f92672">&amp;</span>targets_train, seed.clone());
</span></span><span style="display:flex;"><span>generate_batches(<span style="color:#f92672">&amp;</span>batch_train, <span style="color:#f92672">&amp;</span>targets_train, batch_size).for_each(<span style="color:#f92672">|</span>(batch, targets)<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>    network.train(batch, targets);
</span></span><span style="display:flex;"><span>    optimiser.step(network);
</span></span><span style="display:flex;"><span>});</span></span></code></pre></div>
<p>Finally, after training the network for one epoch, if we need to evaluate the loss, we do that. We can use the <strong>forward_loss</strong> method on the testing data in order to get the predictions (which we won&rsquo;t use), and the loss value (which we will). If the best loss so far is None, or if this one is better, then we update the best loss value and continue. In the case that the loss is now <em>worse</em> than we had, we revert to the snapshot and break the training process.</p>
<p>Code for all of this is shown below:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">if</span> <span style="color:#66d9ef">let</span> Some(last_model) <span style="color:#f92672">=</span> last_model {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">let</span> (_, test_loss) <span style="color:#f92672">=</span> network.forward_loss(batch_test.clone(), targets_test.clone());
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> best_loss.is_none() <span style="color:#f92672">||</span> test_loss <span style="color:#f92672">&lt;</span> <span style="color:#f92672">*</span>best_loss.as_ref().unwrap() {
</span></span><span style="display:flex;"><span>        best_loss <span style="color:#f92672">=</span> Some(test_loss);
</span></span><span style="display:flex;"><span>    } <span style="color:#66d9ef">else</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">*</span>network <span style="color:#f92672">=</span> last_model;
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">break</span>;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>


  </div>

  <footer class="post-footer">
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://forgottenmaster.github.io/">Robin Firth</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
