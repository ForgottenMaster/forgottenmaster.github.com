<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>3.1 - Operations | Robin Firth</title>
<meta name="keywords" content="" />
<meta name="description" content="This chapter is about creating the abstractions in code that enable us to think of neural networks in a modular fashion, and build up complex deep networks with simple structures.
I will be implementing the abstractions in an accompanying library called Eidetic which is free for use and distribution, since I&rsquo;m only implementing this as a learning opportunity.
Overview The Operation construct will represent and encapsulate a single operation in a neural network, such as the sigmoid function, matrix multiplication, etc.">
<meta name="author" content="">
<link rel="canonical" href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/operations/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css" integrity="sha256-b2AFbUTT9&#43;tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js" integrity="sha256-uVus3DnjejMqn4g7Hni&#43;Srwf3KK8HyZB9V4809q9TWE="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://forgottenmaster.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://forgottenmaster.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://forgottenmaster.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://forgottenmaster.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://forgottenmaster.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.110.0">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="3.1 - Operations" />
<meta property="og:description" content="This chapter is about creating the abstractions in code that enable us to think of neural networks in a modular fashion, and build up complex deep networks with simple structures.
I will be implementing the abstractions in an accompanying library called Eidetic which is free for use and distribution, since I&rsquo;m only implementing this as a learning opportunity.
Overview The Operation construct will represent and encapsulate a single operation in a neural network, such as the sigmoid function, matrix multiplication, etc." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/operations/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-02-20T17:56:32&#43;00:00" />
<meta property="article:modified_time" content="2022-02-20T17:56:32&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="3.1 - Operations"/>
<meta name="twitter:description" content="This chapter is about creating the abstractions in code that enable us to think of neural networks in a modular fashion, and build up complex deep networks with simple structures.
I will be implementing the abstractions in an accompanying library called Eidetic which is free for use and distribution, since I&rsquo;m only implementing this as a learning opportunity.
Overview The Operation construct will represent and encapsulate a single operation in a neural network, such as the sigmoid function, matrix multiplication, etc."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://forgottenmaster.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Machine Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Deep Learning From Scratch",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Chapter 3 - Deep Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/"
    }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "3.1 - Operations",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/operations/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "3.1 - Operations",
  "name": "3.1 - Operations",
  "description": "This chapter is about creating the abstractions in code that enable us to think of neural networks in a modular fashion, and build up complex deep networks with simple structures.\nI will be implementing the abstractions in an accompanying library called Eidetic which is free for use and distribution, since I\u0026rsquo;m only implementing this as a learning opportunity.\nOverview The Operation construct will represent and encapsulate a single operation in a neural network, such as the sigmoid function, matrix multiplication, etc.",
  "keywords": [
    
  ],
  "articleBody": "This chapter is about creating the abstractions in code that enable us to think of neural networks in a modular fashion, and build up complex deep networks with simple structures.\nI will be implementing the abstractions in an accompanying library called Eidetic which is free for use and distribution, since I’m only implementing this as a learning opportunity.\nOverview The Operation construct will represent and encapsulate a single operation in a neural network, such as the sigmoid function, matrix multiplication, etc.\nAn operation should be able to take inputs in and produce an output, as well as take a computed partial derivative from the output side and add its own derivative into the mix (i.e. the chain rule).\nAn Operation structure will have forward and backward methods which achieve this, and we will deal exclusively with 2-dimensional ndarray::Array instances as this is the library of choice to do the lower level matrix representation and operations we’ve picked to use in Rust.\nThere are two types of Operations - ones which take only a single array and produces an output array, and ones that take two arrays.\nAn example of the arity-1 operations would be the sigmoid function, and an example of the arity-2 operations would be the matrix multiplication, or bias addition functions.\nAdditionally there are some operations which will change the shape of the input array as output, and some that will retain the shape.\nAn example of shape preserving operation is the sigmoid function. And an example of a shape changing operation is matrix multiplication.\nFor computing the gradients, an operation will have an output gradient that represents the total gradient calculated by the network to the right/output side of an operation, and will have an input gradient which is what is propagated to the left to other operations on the input side.\nThe general rule is an Operation must make sure that the shape of the input gradient matches the input, and shape of the output gradient matches the output to allow us to correctly reason about these derivatives.\nDiagrams An example of an Operation which takes only one input and produces an output is shown in the diagram below:\nAnd an example of an Operation which takes two inputs and produces an output is shown in the following diagram:\nImplementation Approach In order to ensure that the correct checks are done during use, with respect to the shapes of the input/input gradient, and output/output gradient arrays (additionally parameter/parameter gradient arrays if this is an operation that takes a secondary parameter), we will make the Operation type a concrete type, letting us fix forward/backward functions and perform the appropriate invariant checks.\nSince there are two types of operation, and one taking a secondary parameter needs to do additional work/checks on the parameter and parameter gradient, we will make this type an enumeration type.\nWithin these two broad categories of Operation, we can implement anything that’s able to take an input and produce an output, as well as take an output gradient and produce a gradient of its input.\nIn order to achieve this in Rust, we can use a trait object that we pass into the Operation. This will act as a sort of vtable.\nThis approach lets us hard-code the parts that are important, while allowing customisation on the parts that should change. The following sections will detail each type and explain why the code is that way.\nBasicOperation This is a trait for the “vtable” I mentioned above. It represents the inner calculation for an operation that does not take a secondary parameter.\nThe signature is as follows and is generic over some type T\npub trait BasicOperation\u003cT\u003e The generic parameter here as we’ll see shortly is the underlying data type that’s stored inside of the ndarray::Array instances that we’ll be using.\nThe first function defined in the trait is the one that will be used to calculate an output array from a given input array on the forward pass through this operation.\nfn calculate_output(\u0026self, input: \u0026Array\u003cT, Ix2\u003e) -\u003e Array\u003cT, Ix2\u003e; A few things to note are:\nWe don’t need mutable access to self. It’s intended that these vtables for the inner calculations of operations are stateless, thus it should be sufficient to use the provided input parameter to calculate an output. We don’t put a bound on the type T at this point as it’s good practice in Rust to only put bounds when absolutely necessary so as to not cause bound propagation whenever the type is used. We know that we’re working with 2-dimensional data (the rows representing the observations in the data, and the columns representing the features), so we hardcode the dimensionality of these arrays to ndarray::Ix2. We don’t need to have ownership of the input so to avoid unnecessary cloning, we take by reference. This means that the previous Operation can stash the input, and provide a reference to it, rather than needing to clone and have it discarded anyway. The second function that is defined by this trait is the function that will be called on the backward pass to calculate the gradient of its inputs given the output gradient coming in from the rest of the network.\nfn calculate_input_gradient( \u0026self, output_gradient: \u0026Array\u003cT, Ix2\u003e, input: \u0026Array\u003cT, Ix2\u003e, ) -\u003e Array\u003cT, Ix2\u003e; The points above are valid here too, with a couple of additional things to note:\nThe implementor MUST combine their partial derivative with the output gradient to produce the final gradient. We can’t do this externally in the Operation type where we do shape validation, because the way in which they’re combined varies based on the operation (could be elementwise multiplication or matrix multiplication, etc.). We take the input again here, because calculating the input gradient necessarily needs to know what the input was. Since these implementations are stateless (so they can be reused for multiple operations if needed), we get given the previous input. ParameterOperation The second broad category of operation are those that take a secondary parameter array as well as the input.\nAs with the basic operation we need the ability to calculate the output on the forward pass and the input gradient on the backward pass. However, unlike the BasicOperation, we need to also be able to calculate the gradient of the parameter, as well as route the parameter through the other calculation functions.\nThe signature looks the same as BasicOperation, again here we’re taking a T to define the underlying data type of the elements in the ndarray::Array instances:\npub trait ParameterOperation\u003cT\u003e The next element required by implementations is again, a calculate_output function. This signature is almost identical to that defined by BasicOperation, however since we know there’s a secondary parameter, this signature takes that into account:\nfn calculate_output(\u0026self, input: \u0026Array\u003cT, Ix2\u003e, parameter: \u0026Array\u003cT, Ix2\u003e) -\u003e Array\u003cT, Ix2\u003e; Likewise, the calculate_input_gradient is almost identical to the BasicOperation version, except this too takes an additional parameter (since the gradient of the input can be defined in terms of the secondary parameter):\nfn calculate_input_gradient( \u0026self, output_gradient: \u0026Array\u003cT, Ix2\u003e, input: \u0026Array\u003cT, Ix2\u003e, parameter: \u0026Array\u003cT, Ix2\u003e, ) -\u003e Array\u003cT, Ix2\u003e; This trait however defines a third item. Since this is an operation that takes a secondary parameter, we need to be able to calculate the gradient for it to be able to update (for example the weights). This third item is identical to calculate_input_gradient except that it instead calculates the gradient of the parameter:\nfn calculate_parameter_gradient( \u0026self, output_gradient: \u0026Array\u003cT, Ix2\u003e, input: \u0026Array\u003cT, Ix2\u003e, parameter: \u0026Array\u003cT, Ix2\u003e, ) -\u003e Array\u003cT, Ix2\u003e; WeightMultiply Now that we’ve defined the traits for the two broad categories of operation, we can define the concrete implementations of these operations.\nThe first up is the WeightMultiply operation which we use to multiply our matrix of observations together with the current vector of weights (not including the bias term).\nAs mentioned before, these “vtables” will be stateless, and as such the definition of the WeightMultiply structure is suitably underwhelming:\npub struct WeightMultiply; That is just a unit struct that doesn’t have any fields, but allows us to implement the calculations on it.\nThe definition of the implementation block for WeightMultiply is as expected, just implementing ParameterOperation on WeightMultiply. We need to implement ParameterOperation rather than BasicOperation because the weights are taken in as the secondary parameter. The impl block then is:\nimpl\u003cT: LinalgScalar\u003e ParameterOperation\u003cT\u003e for WeightMultiply { ... } The only thing to note is that we are doing a blanket implementation for all types T, however, this would not allow us to perform any arithmetic operations on the arrays with this element type. We require the ability to calculate the dot product and for the dot product to be available on ndarray::Array then we need to place a bound on the generic type T such that we only accept T’s which conform to the LinalgScalar trait.\nLooking at the definition for this trait we can see that it’s just requiring a bunch of mathematical traits so we can multiply and add values (required for dot product):\npub trait LinalgScalar: 'static + Copy + Zero + One + Add\u003cOutput = Self\u003e + Sub\u003cOutput = Self\u003e + Mul\u003cOutput = Self\u003e + Div\u003cOutput = Self\u003e { } Getting into the actual implementations of the ParameterOperation trait now, and starting with the easiest one….calculating the output on the forward pass. This is just a matrix multiplication between the input matrix of data, and the secondary parameter, our weights:\nfn calculate_output(\u0026self, input: \u0026Array\u003cT, Ix2\u003e, parameter: \u0026Array\u003cT, Ix2\u003e) -\u003e Array\u003cT, Ix2\u003e { input.dot(parameter) } Now we can calculate the gradients on the backward pass. Thinking back to the previous posts, we know that when a dot product is involved, the partial derivative is just the transpose of the other array to the one we care about. We can see this in the definition of the calculate_input_gradient function:\nfn calculate_input_gradient( \u0026self, output_gradient: \u0026Array\u003cT, Ix2\u003e, _input: \u0026Array\u003cT, Ix2\u003e, parameter: \u0026Array\u003cT, Ix2\u003e, ) -\u003e Array\u003cT, Ix2\u003e { output_gradient.dot(\u0026(*parameter).clone().reversed_axes()) } Here we are doing the following sequence of events when calculating the input gradient:\nWe dereference the input parameter array reference to get a value (we can’t move out of this value though as we got it from dereferencing a borrow that we don’t own) We clone the parameter matrix, which gives us a brand new copy that we do own and have permission to consume We call reversed_axes to get the transpose matrix. This function requires self by value and consumes it, re-using the memory. This is why we had to clone the parameter array, because we can’t consume out of a borrow We then take a borrow to the newly transposed matrix. This is required because we’re calling the dot product function on a reference to an array, and so requires the right hand side operand to also be a reference The partial gradient, and contribution from this operation is the transposed parameter matrix. However the dot product is required since we are returning the total gradient thus far, incorporating our partial gradient into what we were told from the rest of the network.\nThe calculation of the parameter gradient is similar, however notice that we have the output_gradient on the right hand side this time, not the left:\nfn calculate_parameter_gradient( \u0026self, output_gradient: \u0026Array\u003cT, Ix2\u003e, input: \u0026Array\u003cT, Ix2\u003e, _parameter: \u0026Array\u003cT, Ix2\u003e, ) -\u003e Array\u003cT, Ix2\u003e { (*input).clone().reversed_axes().dot(output_gradient) } BiasAdd The second operation we need to implement is the ability to add a bias to the newly minted weighted product from the previous operation.\nThis too, is a ParameterOperation as we need to set the bias term as the additional parameter.\nThe struct definition is a unit struct just like WeightMultiply as we just use it as a trait object to get the behaviour from:\npub struct BiasAdd; As before too, for implementation we can only implement the trait for elements that conform to the LinalgScalar trait. In this case, it’s because we need the elementwise multiplication, and addition:\nimpl\u003cT: LinalgScalar\u003e ParameterOperation\u003cT\u003e for BiasAdd { ... } The calculate_output method implementation here is very simple, it’s just adding the parameter to the input. However, we must check that the bias term consists of only 1 row (with n elements, where n is the number of features).\nThe bias term is smaller in size than the input, but ndarray Arrays will broadcast the addition across the whole matrix.\nfn calculate_output(\u0026self, input: \u0026Array\u003cT, Ix2\u003e, parameter: \u0026Array\u003cT, Ix2\u003e) -\u003e Array\u003cT, Ix2\u003e { assert_eq!(parameter.nrows(), 1); input + parameter } For calculating the input gradient, we know from the reasoning in chapters 1 and 2 that the partial derivative for addition will just be a matrix with the same shape as the input but full of all 1’s. As before we need to multiply this by the output gradient to take into account the chain rule.\nIn ndarray, we can get an array of 1’s of a certain shape by using the associated function “ones”. This will take a tuple defining the shape we want. We can use the method “raw_dim” to get the shape of another array (in this case our input matrix):\nfn calculate_input_gradient( \u0026self, output_gradient: \u0026Array\u003cT, Ix2\u003e, input: \u0026Array\u003cT, Ix2\u003e, _parameter: \u0026Array\u003cT, Ix2\u003e, ) -\u003e Array\u003cT, Ix2\u003e { Array::ones(input.raw_dim()) * output_gradient } Finally, the calculation of the parameter gradient. This will start off being calculated the same way as we calculated the input gradient above, however we need to sum the resulting matrix along the axis of the rows.\nThis, as mentioned a few times previously, is to account for the bias term being added multiple times to the input data.\nWe will get a column vector from this, but to conform to the shape expected from the input gradient we convert to a row vector.\nIn order to get a view along a specific axis of an array, we can use the map_axis method which takes the axis we’re traversing along, and a callback function which is called for each subview. In our case, for each subview, we want to get the sum.\nThe method into_shape lets us reshape the same data, in our case we ensure we are producing a row vector. This can fail if the provided shape isn’t compatible with the provided data, but we just unwrap it here as I don’t care too much about error handling at the moment.\nThe full code for this method then is:\nfn calculate_parameter_gradient( \u0026self, output_gradient: \u0026Array\u003cT, Ix2\u003e, _input: \u0026Array\u003cT, Ix2\u003e, parameter: \u0026Array\u003cT, Ix2\u003e, ) -\u003e Array\u003cT, Ix2\u003e { let parameter_gradient = Array::ones(parameter.raw_dim()) * output_gradient; parameter_gradient .map_axis(Axis(0), |view| view.sum()) .into_shape((1, parameter_gradient.ncols())) .unwrap() } Sigmoid The sigmoid activation function is our only BasicOperation so far. It doesn’t require a secondary parameter as it just modifies each element of the array by applying the sigmoid function.\nThis function as explained previously is defined as:\n$$ f(x) = \\frac 1 {1 + e^{-x}} $$\nAnd the derivative is defined in terms of itself as:\n$$ \\frac {\\partial f} {\\partial x}(x) = f(x) \\times (1 - f(x)) $$\nFirst we have the struct definition which, you guessed it!, is a unit struct as we don’t have a need for state:\npub struct Sigmoid; The trait bounds for Sigmoid here are a little different however than previously. The previous trait bound of LinalgScalar doesn’t work for us here as that only provides basic arithmetic operations. However what we need is:\nThe ability to calculate the exponential function with the type T The ability to produce a quantity of “one T” The basic mathematical operators of +, -, *, / The ability to use unary negation Looking in the num_traits crate we can see that two traits provide the “exp” method. These are Real and Float….however Real is just a subtrait of Float with additional stuff. Since we’re trying to be least restrictive we pick Float.\nThis trait requires a bunch of others that ultimately end up satisfying all the criteria we want.\nimpl\u003cT: Float\u003e BasicOperation\u003cT\u003e for Sigmoid { ... } To calculate the input, we simply apply the sigmoid function to each element in the input array to produce the output array. We can use mapv to call a function on each element of an Array to produce a new array of the same shape:\nfn calculate_output(\u0026self, input: \u0026Array\u003cT, Ix2\u003e) -\u003e Array\u003cT, Ix2\u003e { input.mapv(|elem| T::one() / (T::one() + (-elem).exp())) } The only other function we need to implement (with this being a BasicOperation) is the calculate_input_gradient function.\nSince the partial derivative is defined in terms of itself, we first recalculate the output from the input. We can then use mapv again on this to apply the partial derivative formula elementwise before finally multiplying it together with the current output gradient as per the chain rule:\nfn calculate_input_gradient( \u0026self, output_gradient: \u0026Array\u003cT, Ix2\u003e, input: \u0026Array\u003cT, Ix2\u003e, ) -\u003e Array\u003cT, Ix2\u003e { let output = self.calculate_output(input); // recalculate the output at the input. let partial_derivative = output.mapv(|elem| elem * (T::one() - elem)); partial_derivative * output_gradient } Linear We implement a passthrough operation for those situations where we need to provide an operation but don’t actually require it to do anything. Since the code is so simple, and the functions have been defined above, I’ll just list the code in its entirety here:\npub struct Linear; impl\u003cT: Clone\u003e BasicOperation\u003cT\u003e for Linear { fn calculate_output(\u0026self, input: \u0026Array\u003cT, Ix2\u003e) -\u003e Array\u003cT, Ix2\u003e { (*input).clone() } fn calculate_input_gradient( \u0026self, output_gradient: \u0026Array\u003cT, Ix2\u003e, _input: \u0026Array\u003cT, Ix2\u003e, ) -\u003e Array\u003cT, Ix2\u003e { (*output_gradient).clone() } } Operation Finally the big bad Operation struct. This is the type that wraps around these vtables we’ve been making and provides the invariant/shape checking functionality, as well as providing a single type to let us store different operations together.\nNote that we could have made Operation a trait, and stored a list of trait objects. However doing this would allow a user to bypass the array shape checks, and caching, etc. which we definitely do not want.\nThe way we have it set up now is that the user only is able to customise the calculations but not able to opt-out of checks, and the general flow.\nSince this is the largest type, I’ll split this section up into a sub section per item.\nOperationData This is the smallest unit so we’ll start here. This wraps over either a BasicOperation or a ParameterOperation and provides a homogenous type to allow us to store in the Operation wrapper.\nThis is defined as an enumeration because we need to store different data depending on whether we’re wrapping a BasicOperation or ParameterOperation.\nIn the case of the basic operation, we need a BasicOperation trait object only to provide the calculations.\nIn the case of parameter operations, we need to additionally provide storage for the parameter array and the parameter gradient.\nThe definition of this enum is as follows:\nenum OperationData\u003c'a, T\u003e { Basic(\u0026'a dyn BasicOperation\u003cT\u003e), Parameter { operation: \u0026'a dyn ParameterOperation\u003cT\u003e, parameter: Array\u003cT, Ix2\u003e, parameter_gradient: Option\u003cArray\u003cT, Ix2\u003e\u003e, }, } Notes on this are:\nWe are storing a reference to the BasicOperation/ParameterOperation implementation. We could use a Box but this would involve putting it on the heap. As we are going to be constructing the network up front, there’s no need for it to live on the heap so we store by reference instead. The downside here of course, is that we now need to add a lifetime annotation for the borrow. The parameter field is an Array because as we’ll see in a minute, the “constructor” takes a parameter and so we’re guaranteed to have it from construction time. However the parameter gradient is only guaranteed to be calculated after the first backward pass. We store this as an Option so that we get a panic if we try to unwrap before a backward pass is performed. Operation This is the actual type that users will be dealing with and store in a list of operations within the layer later on.\nIt’s a simple struct that stores the input and input gradient (both Option as they’re invalid until the first forward/backward pass is performed), and the calculated output also invalid by default.\nThe last thing it stores is the OperationData so it can access the vtable (and parameter, parameter_gradient fields if appropriate).\nAgain, we need to plumb the lifetime annotation through. This time though, we’re not storing a reference directly in Operation, but we do need to plumb it through to OperationData so it knows how long the reference it has lives for.\nFinished definition of the Operation struct is therefore:\npub struct Operation\u003c'a, T\u003e { input: Option\u003cArray\u003cT, Ix2\u003e\u003e, // last provided input on a forward pass input_gradient: Option\u003cArray\u003cT, Ix2\u003e\u003e, // last calculated input_gradient on the backward pass output: Option\u003cArray\u003cT, Ix2\u003e\u003e, // last calculated output on the forward pass data: OperationData\u003c'a, T\u003e, // stores additional data that isn't common between the two kinds of operation } Constructors We have two “constructors” for the two categories of operation (basic and parameterised). These are super simple and as expected just create the instance.\nIn the case of a basic operation, the only thing that’s required is a BasicOperation “vtable” trait object that we can use for calculations. Again we need to ensure that the lifetimes match up correctly (lifetime ‘a is defined in the impl block):\npub fn new_basic(operation: \u0026'a dyn BasicOperation\u003cT\u003e) -\u003e Self { Self { input: None, input_gradient: None, output: None, data: OperationData::Basic(operation), } } The second is for parameterised operations. This takes a ParameterOperation trait object along with an initial value for the parameter:\npub fn new_parameter( operation: \u0026'a dyn ParameterOperation\u003cT\u003e, parameter: Array\u003cT, Ix2\u003e, ) -\u003e Self { Self { input: None, input_gradient: None, output: None, data: OperationData::Parameter { operation, parameter, parameter_gradient: None, }, } } Forward Pass Now we can implement the final two functions. These two (forward and backward) are the meat of the neural network and performs the necessary validation during the backward pass, as well as ensuring data is cached on the forward pass for use in gradient calculation.\nThe forward function will cache the input array. However, because we’re taking the input array by reference will cache a clone of it.\nThe signature and this input stashing line are:\npub fn forward(\u0026mut self, input: \u0026Array\u003cT, Ix2\u003e) -\u003e Array\u003cT, Ix2\u003e { self.input = Some((*input).clone()); The next section of code is to calculate the output array, which we also stash. We do this by delegating to the OperationData trait object we’re wrapping over.\nDue to this being an enumeration, we need to do a match on the data and in the case of a parameter operation, remember to pass along the parameter with the input:\nself.output = Some(match \u0026self.data { OperationData::Basic(operation) =\u003e operation.calculate_output(input), OperationData::Parameter { operation, parameter, .. } =\u003e operation.calculate_output(input, \u0026parameter), }); Finally we need to return the output. But we can’t just return the stashed array as we need that for shape checking on the backward pass. We therefore need to clone this array to get a new one to return. Due to us holding the array in an Option, we need to do this with the as_ref method:\n(*self.output.as_ref().unwrap()).clone() Backward Pass The backward pass looks more complex than forward because of the asserts checking the shapes of the arrays that we have. However at its core, we’re still delegating the actual calculation to the internal OperationData.\nWe begin by checking that the shape of the output gradient we got passed in matches the shape of the last processed output that was generated on the forward pass:\nassert_eq!( self.output.as_ref().unwrap().raw_dim(), output_gradient.raw_dim() ); Again, because we’re storing the output in an Option, we need to use the as_ref().unwrap() sequence.\nThe next step is to get the input gradient. As with the output gradient we can just do a match on our vtable type and ask it to calculate for us. However the match arm for parameterised operations is larger because we also need to calculate the parameter gradient (and verify its shape) in the process of calculating the input gradient:\nself.input_gradient = Some(match \u0026mut self.data { OperationData::Basic(operation) =\u003e { operation.calculate_input_gradient(output_gradient, self.input.as_ref().unwrap()) } OperationData::Parameter { operation, parameter, parameter_gradient, } =\u003e { *parameter_gradient = Some(operation.calculate_parameter_gradient( output_gradient, self.input.as_ref().unwrap(), \u0026parameter, )); assert_eq!( parameter_gradient.as_ref().unwrap().raw_dim(), parameter.raw_dim() ); operation.calculate_input_gradient( output_gradient, self.input.as_ref().unwrap(), \u0026parameter, ) } }); We then need to assert the shape of the input gradient we just calculated matches the shape of the input we processed in the forward pass:\nassert_eq!( self.input.as_ref().unwrap().raw_dim(), self.input_gradient.as_ref().unwrap().raw_dim() ); Finally we need to return the calculated input gradient, but since we’re keeping it for later querying by the optimizer, we need to return a clone:\n(*self.input_gradient.as_ref().unwrap()).clone() Usage The next step in defining our abstractions will be to bundle multiple operations into a single layer, but already see an example of the usage of these Operation instances we have.\nFor example, below is a chain of a multiply, bias add, and sigmoid. We first build the network as a vector of Operations and running through the network is just a case of running through each of the operations.\nWe can create the neural network in a separate function to which we pass the initial parameters. In this code we take the weights and bias terms and set up a very simple sequence of operations:\nfn create_network\u003c'a\u003e(weights: Array\u003cf64, Ix2\u003e, bias: Array\u003cf64, Ix2\u003e) -\u003e Vec\u003cOperation\u003c'a, f64\u003e\u003e { vec![ Operation::new_parameter(\u0026WeightMultiply, weights), Operation::new_parameter(\u0026BiasAdd, bias), Operation::new_basic(\u0026Sigmoid), ] } Note that the compilers lifetime inference rules is enough to find a value for ‘a that satisfies the lifetime we’re using them for and is the smallest such lifetime that covers all cases.\nRemember that a reference having the lifetime ‘a does not mean exactly that lifetime, but rather, that lifetime or longer.\nTherefore it’s rare to require multiple lifetimes and is only required when we need to track them separately through the borrow checker.\nIn our case, all of these operations will always be kept together in the neural network, so the compiler finding a single lifetime (the shortest one) for all the operations here is enough.\nWe can then initiate our weights, bias, and input:\nlet input = Array::from_shape_vec((2, 3), vec![0.5, 0.0, 3.0, 0.0, 0.25, 0.0]).unwrap(); let weights = Array::from_shape_vec((3, 1), vec![1.0, 1.0, 1.0]).unwrap(); let bias = Array::from_shape_vec((1, 1), vec![1.0]).unwrap(); Then pass the weights and bias to the neural network creation function:\nlet mut net = create_network(weights, bias); This needs to be mutable due to us wanting to call the forward and backward functions on the operations in this Vec.\nIn order to run through the chain of functions we can use a fold. The initial state of the fold is just the input array. The fold operation will return the final result which is the output of the forward pass:\nlet output = net.iter_mut().fold(input, |state, op| op.forward(\u0026state)); For the backward pass we need an initial array for the output gradient. In this example we’ll just use an array of 1’s. We also know that the shape of the output gradient and output must match, so we can just make an output gradient of the same shape:\nlet output_gradient = Array::ones(output.raw_dim()); The backward pass is the same, we can use a fold again. This time starting with the output_gradient and running backwards through the chain (hence the .rev() call):\nlet input_gradient = net .iter_mut() .rev() .fold(output_gradient, |state, op| op.backward(\u0026state)); ",
  "wordCount" : "4521",
  "inLanguage": "en",
  "datePublished": "2022-02-20T17:56:32Z",
  "dateModified": "2022-02-20T17:56:32Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/operations/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Robin Firth",
    "logo": {
      "@type": "ImageObject",
      "url": "https://forgottenmaster.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://forgottenmaster.github.io/" accesskey="h" title="Robin Firth (Alt + H)">Robin Firth</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://forgottenmaster.github.io/resume/" title="Résumé">
                    <span>Résumé</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://forgottenmaster.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/">Posts</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/">Machine Learning</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/">Deep Learning From Scratch</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/">Chapter 3 - Deep Learning</a></div>
    <h1 class="post-title">
      3.1 - Operations
    </h1>
    <div class="post-meta"><span title='2022-02-20 17:56:32 +0000 UTC'>February 20, 2022</span>&nbsp;·&nbsp;22 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#overview" aria-label="Overview">Overview</a></li>
                <li>
                    <a href="#diagrams" aria-label="Diagrams">Diagrams</a></li>
                <li>
                    <a href="#implementation-approach" aria-label="Implementation Approach">Implementation Approach</a></li>
                <li>
                    <a href="#basicoperation" aria-label="BasicOperation">BasicOperation</a></li>
                <li>
                    <a href="#parameteroperation" aria-label="ParameterOperation">ParameterOperation</a></li>
                <li>
                    <a href="#weightmultiply" aria-label="WeightMultiply">WeightMultiply</a></li>
                <li>
                    <a href="#biasadd" aria-label="BiasAdd">BiasAdd</a></li>
                <li>
                    <a href="#sigmoid" aria-label="Sigmoid">Sigmoid</a></li>
                <li>
                    <a href="#linear" aria-label="Linear">Linear</a></li>
                <li>
                    <a href="#operation" aria-label="Operation">Operation</a><ul>
                        
                <li>
                    <a href="#operationdata" aria-label="OperationData">OperationData</a></li>
                <li>
                    <a href="#operation-1" aria-label="Operation">Operation</a></li>
                <li>
                    <a href="#constructors" aria-label="Constructors">Constructors</a></li>
                <li>
                    <a href="#forward-pass" aria-label="Forward Pass">Forward Pass</a></li>
                <li>
                    <a href="#backward-pass" aria-label="Backward Pass">Backward Pass</a></li></ul>
                </li>
                <li>
                    <a href="#usage" aria-label="Usage">Usage</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>This chapter is about creating the abstractions in code that enable us to think of neural networks in a modular fashion, and build up complex deep networks with simple structures.</p>
<p>I will be implementing the abstractions in an accompanying library called <a href="https://github.com/ForgottenMaster/eidetic">Eidetic</a> which is free for use and distribution, since I&rsquo;m only implementing this as a learning opportunity.</p>
<h3 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h3>
<p>The Operation construct will represent and encapsulate a single operation in a neural network, such as the sigmoid function, matrix multiplication, etc.</p>
<p>An operation should be able to take inputs in and produce an output, as well as take a computed partial derivative from the output side and add its own derivative into the mix (i.e. the chain rule).</p>
<p>An Operation structure will have forward and backward methods which achieve this, and we will deal exclusively with 2-dimensional ndarray::Array instances as this is the library of choice to do the lower level matrix representation and operations we&rsquo;ve picked to use in Rust.</p>
<p>There are two types of Operations - ones which take only a single array and produces an output array, and ones that take two arrays.</p>
<p>An example of the arity-1 operations would be the sigmoid function, and an example of the arity-2 operations would be the matrix multiplication, or bias addition functions.</p>
<p>Additionally there are some operations which will change the shape of the input array as output, and some that will retain the shape.</p>
<p>An example of shape preserving operation is the sigmoid function. And an example of a shape changing operation is matrix multiplication.</p>
<p>For computing the gradients, an operation will have an <strong>output gradient</strong> that represents the total gradient calculated by the network to the right/output side of an operation, and will have an <strong>input gradient</strong> which is what is propagated to the left to other operations on the input side.</p>
<p>The general rule is an Operation must make sure that the shape of the input gradient matches the input, and shape of the output gradient matches the output to allow us to correctly reason about these derivatives.</p>
<h3 id="diagrams">Diagrams<a hidden class="anchor" aria-hidden="true" href="#diagrams">#</a></h3>
<p>An example of an Operation which takes only one input and produces an output is shown in the diagram below:</p>
<p><img loading="lazy" src="single_input.png" alt="Single Input Diagram"  />
</p>
<p>And an example of an Operation which takes two inputs and produces an output is shown in the following diagram:</p>
<p><img loading="lazy" src="double_input.png" alt="Double Input Diagram"  />
</p>
<h3 id="implementation-approach">Implementation Approach<a hidden class="anchor" aria-hidden="true" href="#implementation-approach">#</a></h3>
<p>In order to ensure that the correct checks are done during use, with respect to the shapes of the input/input gradient, and output/output gradient arrays (additionally parameter/parameter gradient arrays if this is an operation that takes a secondary parameter), we will make the Operation type a concrete type, letting us fix forward/backward functions and perform the appropriate invariant checks.</p>
<p>Since there are two types of operation, and one taking a secondary parameter needs to do additional work/checks on the parameter and parameter gradient, we will make this type an enumeration type.</p>
<p>Within these two broad categories of Operation, we can implement anything that&rsquo;s able to take an input and produce an output, as well as take an output gradient and produce a gradient of its input.</p>
<p>In order to achieve this in Rust, we can use a trait object that we pass into the Operation. This will act as a sort of vtable.</p>
<p>This approach lets us hard-code the parts that are important, while allowing customisation on the parts that should change. The following sections will detail each type and explain why the code is that way.</p>
<h3 id="basicoperation">BasicOperation<a hidden class="anchor" aria-hidden="true" href="#basicoperation">#</a></h3>
<p>This is a trait for the &ldquo;vtable&rdquo; I mentioned above. It represents the inner calculation for an operation that does <strong>not</strong> take a secondary parameter.</p>
<p>The signature is as follows and is generic over some type <strong>T</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">trait</span> BasicOperation<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span></span></span></code></pre></div>
<p>The generic parameter here as we&rsquo;ll see shortly is the underlying data type that&rsquo;s stored inside of the ndarray::Array instances that we&rsquo;ll be using.</p>
<p>The first function defined in the trait is the one that will be used to calculate an output array from a given input array on the forward pass through this operation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_output</span>(<span style="color:#f92672">&amp;</span>self, input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>;</span></span></code></pre></div>
<p>A few things to note are:</p>
<ol>
<li>We don&rsquo;t need mutable access to self. It&rsquo;s intended that these vtables for the inner calculations of operations are stateless, thus it should be sufficient to use the provided input parameter to calculate an output.</li>
<li>We don&rsquo;t put a bound on the type T at this point as it&rsquo;s good practice in Rust to only put bounds when absolutely necessary so as to not cause bound propagation whenever the type is used.</li>
<li>We know that we&rsquo;re working with 2-dimensional data (the rows representing the observations in the data, and the columns representing the features), so we hardcode the dimensionality of these arrays to ndarray::Ix2.</li>
<li>We don&rsquo;t need to have ownership of the input so to avoid unnecessary cloning, we take by reference. This means that the previous Operation can stash the input, and provide a reference to it, rather than needing to clone and have it discarded anyway.</li>
</ol>
<p>The second function that is defined by this trait is the function that will be called on the backward pass to calculate the gradient of its inputs given the output gradient coming in from the rest of the network.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_input_gradient</span>(
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&amp;</span>self,
</span></span><span style="display:flex;"><span>    output_gradient: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>;</span></span></code></pre></div>
<p>The points above are valid here too, with a couple of additional things to note:</p>
<ol>
<li>The implementor <strong>MUST</strong> combine their partial derivative with the output gradient to produce the final gradient. We can&rsquo;t do this externally in the Operation type where we do shape validation, because the way in which they&rsquo;re combined varies based on the operation (could be elementwise multiplication or matrix multiplication, etc.).</li>
<li>We take the input again here, because calculating the input gradient necessarily needs to know what the input was. Since these implementations are stateless (so they can be reused for multiple operations if needed), we get given the previous input.</li>
</ol>
<h3 id="parameteroperation">ParameterOperation<a hidden class="anchor" aria-hidden="true" href="#parameteroperation">#</a></h3>
<p>The second broad category of operation are those that take a secondary parameter array as well as the input.</p>
<p>As with the basic operation we need the ability to calculate the output on the forward pass and the input gradient on the backward pass. However, <em>unlike</em> the BasicOperation, we need to also be able to calculate the gradient of the parameter, as well as route the parameter through the other calculation functions.</p>
<p>The signature looks the same as BasicOperation, again here we&rsquo;re taking a T to define the underlying data type of the elements in the ndarray::Array instances:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">trait</span> ParameterOperation<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span></span></span></code></pre></div>
<p>The next element required by implementations is again, a calculate_output function. This signature is <em>almost</em> identical to that defined by BasicOperation, however since we know there&rsquo;s a secondary parameter, this signature takes that into account:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_output</span>(<span style="color:#f92672">&amp;</span>self, input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, parameter: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>;</span></span></code></pre></div>
<p>Likewise, the calculate_input_gradient is almost identical to the BasicOperation version, except this too takes an additional parameter (since the gradient of the input can be defined in terms of the secondary parameter):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_input_gradient</span>(
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&amp;</span>self,
</span></span><span style="display:flex;"><span>    output_gradient: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    parameter: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>;</span></span></code></pre></div>
<p>This trait however defines a third item. Since this is an operation that takes a secondary parameter, we need to be able to calculate the gradient for it to be able to update (for example the weights). This third item is identical to calculate_input_gradient except that it instead calculates the gradient of the parameter:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_parameter_gradient</span>(
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&amp;</span>self,
</span></span><span style="display:flex;"><span>    output_gradient: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    parameter: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>;</span></span></code></pre></div>
<h3 id="weightmultiply">WeightMultiply<a hidden class="anchor" aria-hidden="true" href="#weightmultiply">#</a></h3>
<p>Now that we&rsquo;ve defined the traits for the two broad categories of operation, we can define the concrete implementations of these operations.</p>
<p>The first up is the WeightMultiply operation which we use to multiply our matrix of observations together with the current vector of weights (not including the bias term).</p>
<p>As mentioned before, these &ldquo;vtables&rdquo; will be stateless, and as such the definition of the WeightMultiply structure is suitably underwhelming:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">WeightMultiply</span>;</span></span></code></pre></div>
<p>That is just a <strong>unit struct</strong> that doesn&rsquo;t have any fields, but allows us to implement the calculations on it.</p>
<p>The definition of the implementation block for WeightMultiply is as expected, just implementing <strong>ParameterOperation<!-- raw HTML omitted --></strong> on WeightMultiply. We need to implement ParameterOperation rather than BasicOperation because the weights are taken in as the secondary parameter. The impl block then is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span>T: <span style="color:#a6e22e">LinalgScalar</span><span style="color:#f92672">&gt;</span> ParameterOperation<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">for</span> WeightMultiply {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">..</span>.
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>The only thing to note is that we are doing a <strong>blanket implementation</strong> for all types T, however, this would not allow us to perform any arithmetic operations on the arrays with this element type. We require the ability to calculate the dot product and for the dot product to be available on ndarray::Array then we need to place a bound on the generic type T such that we only accept T&rsquo;s which conform to the <strong>LinalgScalar</strong> trait.</p>
<p>Looking at the definition for this trait we can see that it&rsquo;s just requiring a bunch of mathematical traits so we can multiply and add values (required for dot product):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">trait</span> LinalgScalar:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#39;</span>static
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">+</span> Copy
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">+</span> Zero
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">+</span> One
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">+</span> Add<span style="color:#f92672">&lt;</span>Output <span style="color:#f92672">=</span> Self<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">+</span> Sub<span style="color:#f92672">&lt;</span>Output <span style="color:#f92672">=</span> Self<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">+</span> Mul<span style="color:#f92672">&lt;</span>Output <span style="color:#f92672">=</span> Self<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">+</span> Div<span style="color:#f92672">&lt;</span>Output <span style="color:#f92672">=</span> Self<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>Getting into the actual implementations of the ParameterOperation trait now, and starting with the easiest one&hellip;.calculating the output on the forward pass. This is just a matrix multiplication between the input matrix of data, and the secondary parameter, our weights:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_output</span>(<span style="color:#f92672">&amp;</span>self, input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, parameter: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    input.dot(parameter)
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>Now we can calculate the gradients on the backward pass. Thinking back to the previous posts, we know that when a dot product is involved, the partial derivative is just the transpose of the <em>other</em> array to the one we care about. We can see this in the definition of the calculate_input_gradient function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_input_gradient</span>(
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&amp;</span>self,
</span></span><span style="display:flex;"><span>    output_gradient: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    _input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    parameter: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    output_gradient.dot(<span style="color:#f92672">&amp;</span>(<span style="color:#f92672">*</span>parameter).clone().reversed_axes())
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>Here we are doing the following sequence of events when calculating the <em>input</em> gradient:</p>
<ol>
<li>We dereference the input parameter array reference to get a value (we can&rsquo;t move out of this value though as we got it from dereferencing a borrow that we don&rsquo;t own)</li>
<li>We <em>clone</em> the parameter matrix, which gives us a brand new copy that we <strong>do</strong> own and have permission to consume</li>
<li>We call reversed_axes to get the transpose matrix. This function requires self by value and consumes it, re-using the memory. This is why we had to clone the parameter array, because we can&rsquo;t consume out of a borrow</li>
<li>We then take a borrow to the newly transposed matrix. This is required because we&rsquo;re calling the dot product function on a reference to an array, and so requires the right hand side operand to also be a reference</li>
</ol>
<p>The partial gradient, and contribution from this operation is the transposed parameter matrix. However the dot product is required since we are returning the total gradient thus far, incorporating our partial gradient into what we were told from the rest of the network.</p>
<p>The calculation of the parameter gradient is similar, however notice that we have the output_gradient on the right hand side this time, not the left:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_parameter_gradient</span>(
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&amp;</span>self,
</span></span><span style="display:flex;"><span>    output_gradient: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    _parameter: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    (<span style="color:#f92672">*</span>input).clone().reversed_axes().dot(output_gradient)
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h3 id="biasadd">BiasAdd<a hidden class="anchor" aria-hidden="true" href="#biasadd">#</a></h3>
<p>The second operation we need to implement is the ability to add a bias to the newly minted weighted product from the previous operation.</p>
<p>This too, is a ParameterOperation as we need to set the bias term as the additional parameter.</p>
<p>The struct definition is a unit struct just like WeightMultiply as we just use it as a trait object to get the behaviour from:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">BiasAdd</span>;</span></span></code></pre></div>
<p>As before too, for implementation we can only implement the trait for elements that conform to the LinalgScalar trait. In this case, it&rsquo;s because we need the elementwise multiplication, and addition:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span>T: <span style="color:#a6e22e">LinalgScalar</span><span style="color:#f92672">&gt;</span> ParameterOperation<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">for</span> BiasAdd {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">..</span>.
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>The calculate_output method implementation here is very simple, it&rsquo;s just adding the parameter to the input. However, we must check that the bias term consists of only 1 row (with n elements, where n is the number of features).</p>
<p>The bias term is smaller in size than the input, but ndarray Arrays will <em>broadcast</em> the addition across the whole matrix.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_output</span>(<span style="color:#f92672">&amp;</span>self, input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, parameter: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    assert_eq!(parameter.nrows(), <span style="color:#ae81ff">1</span>);
</span></span><span style="display:flex;"><span>    input <span style="color:#f92672">+</span> parameter
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>For calculating the input gradient, we know from the reasoning in chapters 1 and 2 that the partial derivative for addition will just be a matrix with the same shape as the input but full of all 1&rsquo;s. As before we need to multiply this by the output gradient to take into account the chain rule.</p>
<p>In ndarray, we can get an array of 1&rsquo;s of a certain shape by using the associated function &ldquo;ones&rdquo;. This will take a tuple defining the shape we want. We can use the method &ldquo;raw_dim&rdquo; to get the shape of another array (in this case our input matrix):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_input_gradient</span>(
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&amp;</span>self,
</span></span><span style="display:flex;"><span>    output_gradient: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    _parameter: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    Array::ones(input.raw_dim()) <span style="color:#f92672">*</span> output_gradient
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>Finally, the calculation of the parameter gradient. This will start off being calculated the same way as we calculated the input gradient above, however we need to <strong>sum</strong> the resulting matrix <em>along</em> the axis of the rows.</p>
<p>This, as mentioned a few times previously, is to account for the bias term being added multiple times to the input data.</p>
<p>We will get a column vector from this, but to conform to the shape expected from the input gradient we convert to a row vector.</p>
<p>In order to get a view along a specific axis of an array, we can use the <strong>map_axis</strong> method which takes the axis we&rsquo;re traversing along, and a callback function which is called for each subview. In our case, for each subview, we want to get the sum.</p>
<p>The method <strong>into_shape</strong> lets us reshape the same data, in our case we ensure we are producing a row vector. This can fail if the provided shape isn&rsquo;t compatible with the provided data, but we just unwrap it here as I don&rsquo;t care too much about error handling at the moment.</p>
<p>The full code for this method then is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_parameter_gradient</span>(
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&amp;</span>self,
</span></span><span style="display:flex;"><span>    output_gradient: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    _input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    parameter: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">let</span> parameter_gradient <span style="color:#f92672">=</span> Array::ones(parameter.raw_dim()) <span style="color:#f92672">*</span> output_gradient;
</span></span><span style="display:flex;"><span>    parameter_gradient
</span></span><span style="display:flex;"><span>        .map_axis(Axis(<span style="color:#ae81ff">0</span>), <span style="color:#f92672">|</span>view<span style="color:#f92672">|</span> view.sum())
</span></span><span style="display:flex;"><span>        .into_shape((<span style="color:#ae81ff">1</span>, parameter_gradient.ncols()))
</span></span><span style="display:flex;"><span>        .unwrap()
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h3 id="sigmoid">Sigmoid<a hidden class="anchor" aria-hidden="true" href="#sigmoid">#</a></h3>
<p>The sigmoid activation function is our only BasicOperation so far. It doesn&rsquo;t require a secondary parameter as it just modifies each element of the array by applying the sigmoid function.</p>
<p>This function as explained previously is defined as:</p>
<p>$$ f(x) = \frac 1 {1 + e^{-x}} $$</p>
<p>And the derivative is defined in terms of itself as:</p>
<p>$$ \frac {\partial f} {\partial x}(x) = f(x) \times (1 - f(x)) $$</p>
<p>First we have the struct definition which, you guessed it!, is a unit struct as we don&rsquo;t have a need for state:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">Sigmoid</span>;</span></span></code></pre></div>
<p>The trait bounds for Sigmoid here are a little different however than previously. The previous trait bound of LinalgScalar doesn&rsquo;t work for us here as that only provides basic arithmetic operations. However what we need is:</p>
<ol>
<li>The ability to calculate the exponential function with the type T</li>
<li>The ability to produce a quantity of &ldquo;one T&rdquo;</li>
<li>The basic mathematical operators of +, -, *, /</li>
<li>The ability to use unary negation</li>
</ol>
<p>Looking in the <strong>num_traits</strong> crate we can see that two traits provide the &ldquo;exp&rdquo; method. These are <em>Real</em> and <em>Float</em>&hellip;.however Real is just a subtrait of Float with additional stuff. Since we&rsquo;re trying to be least restrictive we pick Float.</p>
<p>This trait requires a bunch of others that ultimately end up satisfying all the criteria we want.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span>T: <span style="color:#a6e22e">Float</span><span style="color:#f92672">&gt;</span> BasicOperation<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">for</span> Sigmoid {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">..</span>.
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>To calculate the input, we simply apply the sigmoid function to each element in the input array to produce the output array. We can use <strong>mapv</strong> to call a function on each element of an Array to produce a new array of the same shape:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_output</span>(<span style="color:#f92672">&amp;</span>self, input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    input.mapv(<span style="color:#f92672">|</span>elem<span style="color:#f92672">|</span> T::one() <span style="color:#f92672">/</span> (T::one() <span style="color:#f92672">+</span> (<span style="color:#f92672">-</span>elem).exp()))
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>The only other function we need to implement (with this being a BasicOperation) is the calculate_input_gradient function.</p>
<p>Since the partial derivative is defined in terms of itself, we first recalculate the output from the input. We can then use mapv again on this to apply the partial derivative formula elementwise before finally multiplying it together with the current output gradient as per the chain rule:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_input_gradient</span>(
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&amp;</span>self,
</span></span><span style="display:flex;"><span>    output_gradient: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">let</span> output <span style="color:#f92672">=</span> self.calculate_output(input); <span style="color:#75715e">// recalculate the output at the input.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">let</span> partial_derivative <span style="color:#f92672">=</span> output.mapv(<span style="color:#f92672">|</span>elem<span style="color:#f92672">|</span> elem <span style="color:#f92672">*</span> (T::one() <span style="color:#f92672">-</span> elem));
</span></span><span style="display:flex;"><span>    partial_derivative <span style="color:#f92672">*</span> output_gradient
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h3 id="linear">Linear<a hidden class="anchor" aria-hidden="true" href="#linear">#</a></h3>
<p>We implement a passthrough operation for those situations where we need to provide an operation but don&rsquo;t actually require it to do anything. Since the code is so simple, and the functions have been defined above, I&rsquo;ll just list the code in its entirety here:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">Linear</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span>T: Clone<span style="color:#f92672">&gt;</span> BasicOperation<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">for</span> Linear {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_output</span>(<span style="color:#f92672">&amp;</span>self, input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>        (<span style="color:#f92672">*</span>input).clone()
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">calculate_input_gradient</span>(
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">&amp;</span>self,
</span></span><span style="display:flex;"><span>        output_gradient: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>        _input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    ) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>        (<span style="color:#f92672">*</span>output_gradient).clone()
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h3 id="operation">Operation<a hidden class="anchor" aria-hidden="true" href="#operation">#</a></h3>
<p>Finally the big bad Operation struct. This is the type that wraps around these vtables we&rsquo;ve been making and provides the invariant/shape checking functionality, as well as providing a single type to let us store different operations together.</p>
<p>Note that we could have made Operation a trait, and stored a list of trait objects. However doing this would allow a user to bypass the array shape checks, and caching, etc. which we definitely do not want.</p>
<p>The way we have it set up now is that the user only is able to customise the calculations but not able to opt-out of checks, and the general flow.</p>
<p>Since this is the largest type, I&rsquo;ll split this section up into a sub section per item.</p>
<h4 id="operationdata">OperationData<a hidden class="anchor" aria-hidden="true" href="#operationdata">#</a></h4>
<p>This is the smallest unit so we&rsquo;ll start here. This wraps over either a BasicOperation or a ParameterOperation and provides a homogenous type to allow us to store in the Operation wrapper.</p>
<p>This is defined as an enumeration because we need to store different data depending on whether we&rsquo;re wrapping a BasicOperation or ParameterOperation.</p>
<p>In the case of the basic operation, we need a BasicOperation trait object only to provide the calculations.</p>
<p>In the case of parameter operations, we need to additionally provide storage for the parameter array and the parameter gradient.</p>
<p>The definition of this enum is as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">enum</span> <span style="color:#a6e22e">OperationData</span><span style="color:#f92672">&lt;&#39;</span><span style="color:#a6e22e">a</span>, T<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    Basic(<span style="color:#f92672">&amp;&#39;</span><span style="color:#a6e22e">a</span> <span style="color:#66d9ef">dyn</span> BasicOperation<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>),
</span></span><span style="display:flex;"><span>    Parameter {
</span></span><span style="display:flex;"><span>        operation: <span style="color:#66d9ef">&amp;</span><span style="color:#f92672">&#39;</span><span style="color:#a6e22e">a</span> <span style="color:#a6e22e">dyn</span> ParameterOperation<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>        parameter: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>        parameter_gradient: Option<span style="color:#f92672">&lt;</span>Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;&gt;</span>,
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>Notes on this are:</p>
<ol>
<li>We are storing a reference to the BasicOperation/ParameterOperation implementation. We could use a Box but this would involve putting it on the heap. As we are going to be constructing the network up front, there&rsquo;s no need for it to live on the heap so we store by reference instead. The downside here of course, is that we now need to add a lifetime annotation for the borrow.</li>
<li>The parameter field is an Array because as we&rsquo;ll see in a minute, the &ldquo;constructor&rdquo; takes a parameter and so we&rsquo;re guaranteed to have it from construction time.</li>
<li>However the parameter gradient is only guaranteed to be calculated after the first backward pass. We store this as an Option so that we get a panic if we try to unwrap before a backward pass is performed.</li>
</ol>
<h4 id="operation-1">Operation<a hidden class="anchor" aria-hidden="true" href="#operation-1">#</a></h4>
<p>This is the actual type that users will be dealing with and store in a list of operations within the layer later on.</p>
<p>It&rsquo;s a simple struct that stores the input and input gradient (both Option as they&rsquo;re invalid until the first forward/backward pass is performed), and the calculated output also invalid by default.</p>
<p>The last thing it stores is the OperationData so it can access the vtable (and parameter, parameter_gradient fields if appropriate).</p>
<p>Again, we need to plumb the lifetime annotation through. This time though, we&rsquo;re not storing a reference directly in Operation, but we do need to plumb it through to OperationData so <em>it</em> knows how long the reference it has lives for.</p>
<p>Finished definition of the Operation struct is therefore:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">Operation</span><span style="color:#f92672">&lt;&#39;</span><span style="color:#a6e22e">a</span>, T<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    input: Option<span style="color:#f92672">&lt;</span>Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;&gt;</span>, <span style="color:#75715e">// last provided input on a forward pass
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    input_gradient: Option<span style="color:#f92672">&lt;</span>Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;&gt;</span>, <span style="color:#75715e">// last calculated input_gradient on the backward pass
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    output: Option<span style="color:#f92672">&lt;</span>Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;&gt;</span>, <span style="color:#75715e">// last calculated output on the forward pass
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    data: <span style="color:#a6e22e">OperationData</span><span style="color:#f92672">&lt;&#39;</span><span style="color:#a6e22e">a</span>, T<span style="color:#f92672">&gt;</span>, <span style="color:#75715e">// stores additional data that isn&#39;t common between the two kinds of operation
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>}</span></span></code></pre></div>
<h4 id="constructors">Constructors<a hidden class="anchor" aria-hidden="true" href="#constructors">#</a></h4>
<p>We have two &ldquo;constructors&rdquo; for the two categories of operation (basic and parameterised). These are super simple and as expected just create the instance.</p>
<p>In the case of a basic operation, the only thing that&rsquo;s required is a BasicOperation &ldquo;vtable&rdquo; trait object that we can use for calculations. Again we need to ensure that the lifetimes match up correctly (lifetime &lsquo;a is defined in the impl block):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">new_basic</span>(operation: <span style="color:#66d9ef">&amp;</span><span style="color:#f92672">&#39;</span><span style="color:#a6e22e">a</span> <span style="color:#a6e22e">dyn</span> BasicOperation<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Self</span> {
</span></span><span style="display:flex;"><span>    Self {
</span></span><span style="display:flex;"><span>        input: None,
</span></span><span style="display:flex;"><span>        input_gradient: None,
</span></span><span style="display:flex;"><span>        output: None,
</span></span><span style="display:flex;"><span>        data: <span style="color:#a6e22e">OperationData</span>::Basic(operation),
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>The second is for parameterised operations. This takes a ParameterOperation trait object along with an initial value for the parameter:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">new_parameter</span>(
</span></span><span style="display:flex;"><span>    operation: <span style="color:#66d9ef">&amp;</span><span style="color:#f92672">&#39;</span><span style="color:#a6e22e">a</span> <span style="color:#a6e22e">dyn</span> ParameterOperation<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    parameter: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>) -&gt; <span style="color:#a6e22e">Self</span> {
</span></span><span style="display:flex;"><span>    Self {
</span></span><span style="display:flex;"><span>        input: None,
</span></span><span style="display:flex;"><span>        input_gradient: None,
</span></span><span style="display:flex;"><span>        output: None,
</span></span><span style="display:flex;"><span>        data: <span style="color:#a6e22e">OperationData</span>::Parameter {
</span></span><span style="display:flex;"><span>            operation,
</span></span><span style="display:flex;"><span>            parameter,
</span></span><span style="display:flex;"><span>            parameter_gradient: None,
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h4 id="forward-pass">Forward Pass<a hidden class="anchor" aria-hidden="true" href="#forward-pass">#</a></h4>
<p>Now we can implement the final two functions. These two (forward and backward) are the meat of the neural network and performs the necessary validation during the backward pass, as well as ensuring data is cached on the forward pass for use in gradient calculation.</p>
<p>The forward function will cache the input array. However, because we&rsquo;re taking the input array by reference will cache a clone of it.</p>
<p>The signature and this input stashing line are:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">forward</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    self.input <span style="color:#f92672">=</span> Some((<span style="color:#f92672">*</span>input).clone());</span></span></code></pre></div>
<p>The next section of code is to calculate the output array, which we also stash. We do this by delegating to the OperationData trait object we&rsquo;re wrapping over.</p>
<p>Due to this being an enumeration, we need to do a match on the data and in the case of a parameter operation, remember to pass along the parameter with the input:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>self.output <span style="color:#f92672">=</span> Some(<span style="color:#66d9ef">match</span> <span style="color:#f92672">&amp;</span>self.data {
</span></span><span style="display:flex;"><span>    OperationData::Basic(operation) <span style="color:#f92672">=&gt;</span> operation.calculate_output(input),
</span></span><span style="display:flex;"><span>    OperationData::Parameter {
</span></span><span style="display:flex;"><span>        operation,
</span></span><span style="display:flex;"><span>        parameter,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">..</span>
</span></span><span style="display:flex;"><span>    } <span style="color:#f92672">=&gt;</span> operation.calculate_output(input, <span style="color:#f92672">&amp;</span>parameter),
</span></span><span style="display:flex;"><span>});</span></span></code></pre></div>
<p>Finally we need to return the output. But we can&rsquo;t just return the stashed array as we need that for shape checking on the backward pass. We therefore need to clone this array to get a new one to return. Due to us holding the array in an Option, we need to do this with the as_ref method:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>(<span style="color:#f92672">*</span>self.output.as_ref().unwrap()).clone()</span></span></code></pre></div>
<h4 id="backward-pass">Backward Pass<a hidden class="anchor" aria-hidden="true" href="#backward-pass">#</a></h4>
<p>The backward pass <em>looks</em> more complex than forward because of the asserts checking the shapes of the arrays that we have. However at its core, we&rsquo;re still delegating the actual calculation to the internal OperationData.</p>
<p>We begin by checking that the shape of the output gradient we got passed in matches the shape of the last processed output that was generated on the forward pass:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>assert_eq!(
</span></span><span style="display:flex;"><span>    self.output.as_ref().unwrap().raw_dim(),
</span></span><span style="display:flex;"><span>    output_gradient.raw_dim()
</span></span><span style="display:flex;"><span>);</span></span></code></pre></div>
<p>Again, because we&rsquo;re storing the output in an Option, we need to use the as_ref().unwrap() sequence.</p>
<p>The next step is to get the input gradient. As with the output gradient we can just do a match on our vtable type and ask it to calculate for us. However the match arm for parameterised operations is larger because we also need to calculate the parameter gradient (and verify its shape) in the process of calculating the input gradient:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>self.input_gradient <span style="color:#f92672">=</span> Some(<span style="color:#66d9ef">match</span> <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self.data {
</span></span><span style="display:flex;"><span>    OperationData::Basic(operation) <span style="color:#f92672">=&gt;</span> {
</span></span><span style="display:flex;"><span>        operation.calculate_input_gradient(output_gradient, self.input.as_ref().unwrap())
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    OperationData::Parameter {
</span></span><span style="display:flex;"><span>        operation,
</span></span><span style="display:flex;"><span>        parameter,
</span></span><span style="display:flex;"><span>        parameter_gradient,
</span></span><span style="display:flex;"><span>    } <span style="color:#f92672">=&gt;</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">*</span>parameter_gradient <span style="color:#f92672">=</span> Some(operation.calculate_parameter_gradient(
</span></span><span style="display:flex;"><span>            output_gradient,
</span></span><span style="display:flex;"><span>            self.input.as_ref().unwrap(),
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&amp;</span>parameter,
</span></span><span style="display:flex;"><span>        ));
</span></span><span style="display:flex;"><span>        assert_eq!(
</span></span><span style="display:flex;"><span>            parameter_gradient.as_ref().unwrap().raw_dim(),
</span></span><span style="display:flex;"><span>            parameter.raw_dim()
</span></span><span style="display:flex;"><span>        );
</span></span><span style="display:flex;"><span>        operation.calculate_input_gradient(
</span></span><span style="display:flex;"><span>            output_gradient,
</span></span><span style="display:flex;"><span>            self.input.as_ref().unwrap(),
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&amp;</span>parameter,
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>});</span></span></code></pre></div>
<p>We then need to assert the shape of the input gradient we just calculated matches the shape of the input we processed in the forward pass:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>assert_eq!(
</span></span><span style="display:flex;"><span>    self.input.as_ref().unwrap().raw_dim(),
</span></span><span style="display:flex;"><span>    self.input_gradient.as_ref().unwrap().raw_dim()
</span></span><span style="display:flex;"><span>);</span></span></code></pre></div>
<p>Finally we need to return the calculated input gradient, but since we&rsquo;re keeping it for later querying by the optimizer, we need to return a clone:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>(<span style="color:#f92672">*</span>self.input_gradient.as_ref().unwrap()).clone()</span></span></code></pre></div>
<h3 id="usage">Usage<a hidden class="anchor" aria-hidden="true" href="#usage">#</a></h3>
<p>The next step in defining our abstractions will be to bundle multiple operations into a single layer, but already see an example of the usage of these Operation instances we have.</p>
<p>For example, below is a chain of a multiply, bias add, and sigmoid. We first build the network as a vector of Operations and running through the network is just a case of running through each of the operations.</p>
<p>We can create the neural network in a separate function to which we pass the initial parameters. In this code we take the weights and bias terms and set up a very simple sequence of operations:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">create_network</span><span style="color:#f92672">&lt;&#39;</span><span style="color:#a6e22e">a</span><span style="color:#f92672">&gt;</span>(weights: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span><span style="color:#66d9ef">f64</span>, Ix2<span style="color:#f92672">&gt;</span>, bias: <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span><span style="color:#66d9ef">f64</span>, Ix2<span style="color:#f92672">&gt;</span>) -&gt; Vec<span style="color:#f92672">&lt;</span>Operation<span style="color:#f92672">&lt;&#39;</span><span style="color:#a6e22e">a</span>, <span style="color:#66d9ef">f64</span><span style="color:#f92672">&gt;&gt;</span> {
</span></span><span style="display:flex;"><span>    vec![
</span></span><span style="display:flex;"><span>        Operation::new_parameter(<span style="color:#f92672">&amp;</span>WeightMultiply, weights),
</span></span><span style="display:flex;"><span>        Operation::new_parameter(<span style="color:#f92672">&amp;</span>BiasAdd, bias),
</span></span><span style="display:flex;"><span>        Operation::new_basic(<span style="color:#f92672">&amp;</span>Sigmoid),
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>Note that the compilers lifetime inference rules is enough to find a value for &lsquo;a that satisfies the lifetime we&rsquo;re using them for and is the smallest such lifetime that covers all cases.</p>
<p>Remember that a reference having the lifetime &lsquo;a does <strong>not</strong> mean <em>exactly</em> that lifetime, but rather, that lifetime or <em>longer</em>.</p>
<p>Therefore it&rsquo;s rare to require multiple lifetimes and is only required when we need to track them separately through the borrow checker.</p>
<p>In our case, all of these operations will always be kept together in the neural network, so the compiler finding a single lifetime (the shortest one) for all the operations here is enough.</p>
<p>We can then initiate our weights, bias, and input:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> input <span style="color:#f92672">=</span> Array::from_shape_vec((<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>), vec![<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">3.0</span>, <span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">0.25</span>, <span style="color:#ae81ff">0.0</span>]).unwrap();
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> weights <span style="color:#f92672">=</span> Array::from_shape_vec((<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>), vec![<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">1.0</span>]).unwrap();
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> bias <span style="color:#f92672">=</span> Array::from_shape_vec((<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), vec![<span style="color:#ae81ff">1.0</span>]).unwrap();</span></span></code></pre></div>
<p>Then pass the weights and bias to the neural network creation function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> net <span style="color:#f92672">=</span> create_network(weights, bias);</span></span></code></pre></div>
<p>This needs to be mutable due to us wanting to call the forward and backward functions on the operations in this Vec.</p>
<p>In order to run through the chain of functions we can use a fold. The initial state of the fold is just the input array. The fold operation will return the final result which is the output of the forward pass:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> output <span style="color:#f92672">=</span> net.iter_mut().fold(input, <span style="color:#f92672">|</span>state, op<span style="color:#f92672">|</span> op.forward(<span style="color:#f92672">&amp;</span>state));</span></span></code></pre></div>
<p>For the backward pass we need an initial array for the output gradient. In this example we&rsquo;ll just use an array of 1&rsquo;s. We also know that the shape of the output gradient and output must match, so we can just make an output gradient of the same shape:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> output_gradient <span style="color:#f92672">=</span> Array::ones(output.raw_dim());</span></span></code></pre></div>
<p>The backward pass is the same, we can use a fold again. This time starting with the output_gradient and running <strong>backwards</strong> through the chain (hence the .rev() call):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> input_gradient <span style="color:#f92672">=</span> net
</span></span><span style="display:flex;"><span>    .iter_mut()
</span></span><span style="display:flex;"><span>    .rev()
</span></span><span style="display:flex;"><span>    .fold(output_gradient, <span style="color:#f92672">|</span>state, op<span style="color:#f92672">|</span> op.backward(<span style="color:#f92672">&amp;</span>state));</span></span></code></pre></div>


  </div>

  <footer class="post-footer">
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://forgottenmaster.github.io/">Robin Firth</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
