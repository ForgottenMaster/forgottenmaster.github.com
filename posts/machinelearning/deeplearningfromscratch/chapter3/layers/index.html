<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>3.2 - Layers | Robin Smith</title>
<meta name="keywords" content="">
<meta name="description" content="Overview The lowest level of abstraction as mentioned in the previous post is the operation. These were the lowest level building block that is able to perform a single unit of calculation on the forward pass, and single unit of partial derivative calculation on the backward pass.
During that chapter, we could see how multiple of these operations are able to be chained together, though we don&rsquo;t want to do this by hand.">
<meta name="author" content="">
<link rel="canonical" href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/layers/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.4599eadb9eb2ad3d0a8d6827b41a8fda8f2f4af226b63466c09c5fddbc8706b7.css" integrity="sha256-RZnq256yrT0KjWgntBqP2o8vSvImtjRmwJxf3byHBrc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://forgottenmaster.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://forgottenmaster.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://forgottenmaster.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://forgottenmaster.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://forgottenmaster.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/layers/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="3.2 - Layers" />
<meta property="og:description" content="Overview The lowest level of abstraction as mentioned in the previous post is the operation. These were the lowest level building block that is able to perform a single unit of calculation on the forward pass, and single unit of partial derivative calculation on the backward pass.
During that chapter, we could see how multiple of these operations are able to be chained together, though we don&rsquo;t want to do this by hand." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/layers/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-02-27T17:47:20+00:00" />
<meta property="article:modified_time" content="2022-02-27T17:47:20+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="3.2 - Layers"/>
<meta name="twitter:description" content="Overview The lowest level of abstraction as mentioned in the previous post is the operation. These were the lowest level building block that is able to perform a single unit of calculation on the forward pass, and single unit of partial derivative calculation on the backward pass.
During that chapter, we could see how multiple of these operations are able to be chained together, though we don&rsquo;t want to do this by hand."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://forgottenmaster.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Machine Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Deep Learning From Scratch",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Chapter 3 - Deep Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/"
    }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "3.2 - Layers",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/layers/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "3.2 - Layers",
  "name": "3.2 - Layers",
  "description": "Overview The lowest level of abstraction as mentioned in the previous post is the operation. These were the lowest level building block that is able to perform a single unit of calculation on the forward pass, and single unit of partial derivative calculation on the backward pass.\nDuring that chapter, we could see how multiple of these operations are able to be chained together, though we don\u0026rsquo;t want to do this by hand.",
  "keywords": [
    
  ],
  "articleBody": "Overview The lowest level of abstraction as mentioned in the previous post is the operation. These were the lowest level building block that is able to perform a single unit of calculation on the forward pass, and single unit of partial derivative calculation on the backward pass.\nDuring that chapter, we could see how multiple of these operations are able to be chained together, though we don’t want to do this by hand. Instead, we would like an abstraction that has the same interface as the operations (a forward and backward method) but which internally handles routing the data through the operations that are part of the layer.\nWe define a layer as a series of linear operations, followed by a final (optional) non-linear operation known as the “activation function”.\nIn the context of the simple neural network implemented in the last chapter, we had 5 operations that were:\nA WeightMultiply operation which takes the initial input and selection of weights and multiplies them together, ending with a matrix that represents the weighted sum for each of the “learned features” that we wanted in this “hidden layer”. A BiasAdd operation which added in the bias term to the weighted multiplication. A Sigmoid function that took the results and “squashed” them to a non-linear range between 0 and 1. A WeightMultiply operation which took the inputs from the learned features, and multiplied them with a second set of weights. A BiasAdd operation which added in the bias term to the calculated weighted sum (which was a single value at this point). We can see that there are 2 layers here:\nOperations 1 to 3 comprise the first layer which takes the initial input, operates on it, and produces values for a set of learned features (or as we’ll use here, “neurons”) Operations 4 and 5 will comprise the final layer which takes the learned hidden features to the final output A diagram below shows this grouping of operations (with the loss function tacked onto the end):\nNeurons Rather than looking at the individual operations within the layers, we instead look at neural networks at the abstraction of these layers. A layer is, as mentioned previously a series of non-linear operations followed by a linear one, but another important facet of a layer is the number of neurons it contains.\nIn the brain, neurons will take electrical impulses from other connected neurons and will fire if that energy exceeds some certain threshold (which then causes other neurons to possibly fire, etc.). In the case of artificial neural networks, this activation is modeled by transformation of the input to an output through a non-linear function (the Sigmoid function for example), which we also can call the activation function for a layer. The values coming out of this are the activations for that layer.\nThe number of neurons in an artificial neural network will correspond to the number of features in the input. Therefore in the simple neural network seen in the last chapter, and whose operations are briefed above we can see that:\nThe number of neurons in the input layer is X where X is the number of features in the data set. The number of neurons in the hidden layer is N where N is some number that we define. The output from this layer will be a matrix with the number of rows as in the input, and the number of columns equal to the neuron count for that layer. The number of neurons from the output layer is 1. This is because we’re collapsing the values down to a single prediction for each observation (to allow us to use the loss function to detect error). Deep learning is simply using neural networks with more than one hidden layer.\nA more useful diagram below doesn’t show the operations inside the layers, but abstracts the layers only to show their inputs and outputs:\nA small detour Before delving into the implementation for the Layer abstraction, we first address a need for a function that will be required to be added to the “Operation” type that we implemented in the previous section.\nFor ParameterOperation instances we stored the parameter being used, along with the last computed gradient for the parameter in the backward pass. One thing that we will need and will lift to the Layer abstraction is the ability to access temporarily the parameter and parameter gradients in use.\nIn order to achieve this, we will add the following function to the Operation type:\npub fn parameter_and_gradient(\u0026mut self) -\u003e Option\u003c(\u0026mut Array\u003cT, Ix2\u003e, \u0026Array\u003cT, Ix2\u003e)\u003e { ... } This function will return the parameter as a mutable reference, since we will eventually need to be able to update the parameter, given the gradient. However the gradient doesn’t need to be mutable so we return an immutable reference.\nOne thing to note here is that not all operations have a secondary parameter (those that are BasicOperations) - therefore we return an Option to allow for this possibility.\nThe implementation of this function then look as follows:\nmatch \u0026mut self.data { OperationData::Parameter { parameter, parameter_gradient, .. } =\u003e Some((parameter, parameter_gradient.as_ref().unwrap())), _ =\u003e None, } That is, we must match on the OperationData enum value we have. In the case of it being a parameter operation, we can get the references we want, and return these in a “Some” variant. If not, then we return None as there’s no parameters to get.\nLayerSetup Just as we have the BasicOperation and ParameterOperation traits to allow for customization of the calculations for specific operations, we would also like to define a static structure for a Layer while allowing construction of that layer (determining which operations, etc.) to be customized.\nThis is the purpose of the LayerSetup trait. We will shortly see that the structure of Layer is static while delegating the actual setup of the layer to a trait object implementing this trait.\nThe trait header looks as follows:\npub trait LayerSetup\u003c'a, T\u003e { ... } This again requires the same generic parameters that are required by Operation. That is:\n‘a - This is the lifetime of the borrow for the BasicOperation or ParameterOperation vtables in use T - This is the type of the elements inside the ndarray::Array Looking at the trait itself, it only requires implementors to provide the following function:\nfn layer_setup( \u0026self, num_neurons: usize, input: \u0026Array\u003cT, Ix2\u003e, seed: \u0026Option\u003cu64\u003e, ) -\u003e Vec\u003cOperation\u003c'a, T\u003e\u003e; This is a function that, given the number of neurons/features that the layer should have, along with the input (used to determine the number of features in the input) can return the list of Operations required in that layer.\nThe additional parameter is an optional seed used for random number generation. If this is None then it’s expected that the numbers should be truly randomly generated, if it’s Some then the seed shouild be used to construct the PRNG stream.\nDenseLayerSetup Previously we saw that the layers (the hidden and output layers) in our neural network fully connects the neurons of the previous layers and consists of a sequence of WeightMultiply -\u003e BiasAdd -\u003e ActivationFunction where ActivationFunction is some non-linear function. For the hidden layer we used Sigmoid and for the output layer we used a Linear activation function which is just a passthrough function.\nHowever the basic concept of fully connecting the neurons of the previous layer, and using a weighted sum and bias addition is consistent between layers. The activation function needs to be tweakable.\nThe terminology for these fully connected layers is a dense layer (due to it being densely connected).\nThis, therefore will be known as the DenseLayerSetup.\nAs mentioned above, we will need to modify the activation function in use, so we will keep this as a field. As before, we will need to make it generic over the lifetime, ‘a, and the type T. We know that the activation function should be one that doesn’t take an additional parameter (if this proves an incorrect assumption we can change later), so we can take just the BasicOperation trait object rather than a wrapped Operation.\nThe structure definition therefore is as follows:\npub struct DenseLayerSetup\u003c'a, T\u003e(\u0026'a dyn BasicOperation\u003cT\u003e); Next, we want a simple constructor function for this. We don’t want to make the fields public since they could be changed after construction, so instead we keep the field private and provide a constructor function for external use.\nimpl\u003c'a, T\u003e DenseLayerSetup\u003c'a, T\u003e { pub fn new(activation: \u0026'a dyn BasicOperation\u003cT\u003e) -\u003e Self { Self(activation) } } Finally, we have the actual trait implementation. First of all, we have the header for the impl block:\nimpl\u003c'a, T: LinalgScalar\u003e LayerSetup\u003c'a, T\u003e for DenseLayerSetup\u003c'a, T\u003e where Standard: Distribution\u003cT\u003e, { ... } A couple of things to note here:\nWe require a trait bound of LinalgScalar on the generic parameter, T. This is because the layer needs to use the WeightMultiply and BiasAdd operations that both put a restriction of LinalgScalar on their generic parameter (due to requiring the arithmetic operations, etc.) We are using random generation for the initial parameters on the weight and bias operations. In order to use this random number generation, we require that our type T is such that the structure “Standard” implements Distribution. Next is the actual function implementation. I won’t repeat the function signature here as it was specified in the LayerSetup section.\nThe first step in the implementation is to get the random number generator instance. This is going to depend on if there’s a random seed specified or not. If there is, we will use it to construct the generator. If not, we get a truly random generator. This code looks as followws:\nlet mut r = match seed { Some(seed) =\u003e StdRng::seed_from_u64(*seed), None =\u003e StdRng::from_rng(thread_rng()).unwrap(), }; Next, we need to generate the random weights and bias values to use as parameters to the appropriate operations. The steps for this are as follows:\nWe figure out what dimensions we require for the weights matrix. The number of rows in the weights matrix will match the number of columns/features in the input data - this allows us to perform the matrix multiplication. The number of columns in the weights matrix will then be the number of output neurons we require in the layer. Calculate the number of elements in the matrix which we use for creating the buffer in the next step. This is just the number of rows multiplied by number of columns. Create a vector with a number of elements calculated in step #2. Each element is a randomly generated value using the generator we made previously. In order to make these, we can use the repeat_with function from std::iter which takes a function and calls it repeatedly. We provide a function that just produces random values. Then we use the take adapter to take the appropriate number of elements, and finally collect them into a vector. Finally, we use the Array::from_shape_vec constructor to turn the Vec into an Array. The code for the above looks as follows:\nlet weight_dim = (input.ncols(), num_neurons); let weight_count = weight_dim.0 * weight_dim.1; let weights = repeat_with(|| r.gen::\u003cT\u003e()).take(weight_count).collect(); let weights = Array::from_shape_vec(weight_dim, weights).unwrap(); Creating the bias matrix is the same, except the dimensions are different. The bias matrix always consists of a single row, and the number of columns is the number of neurons we have. This looks as follows:\nlet bias_dim = (1, num_neurons); let bias_count = bias_dim.0 * bias_dim.1; let bias = repeat_with(|| r.gen::\u003cT\u003e()).take(bias_count).collect(); let bias = Array::from_shape_vec(bias_dim, bias).unwrap(); Finally we have the parameters we need to create the layer. We make sure to include the activation function that we stored as a field as the last operation in the sequence:\nvec![ Operation::new_parameter(\u0026WeightMultiply, weights), Operation::new_parameter(\u0026BiasAdd, bias), Operation::new_basic(self.0), ] Layer Finally we can implement the abstraction for a Layer. As with Operation, I’ll split this up into sub-headers to break it up a little.\nDefinition The structure definition looks as follows:\npub struct Layer\u003c'a, T\u003e { num_neurons: usize, operations: Option\u003cVec\u003cOperation\u003c'a, T\u003e\u003e\u003e, setup: \u0026'a dyn LayerSetup\u003c'a, T\u003e, output: Option\u003cArray\u003cT, Ix2\u003e\u003e, seed: Option\u003cu64\u003e, } As always, we need to take the type of elements in the arrays (T) and lifetime of the OperationData vtables (‘a).Following is a description of each parameter within the definition however:\nnum_neurons - This is the number of columns/features in the output of the layer. It’s used to generate weights and bias of a correct size to produce the desired dimension operations - This is an owned vector of the Operations in the layer. It’s an Option here because we can default to None until the first forward pass is performed. We could use an empty Vector but that would not be as explicit. This way if someone tries to use the list of operations without doing a forward pass first then they’ll see it’s invalid by the program panicking setup - A reference to the LayerSetup to use to produce the list of Operations on the first forward pass output - Store the calculated output from the forward pass which can be used then on a backward pass seed - The optional random seed to use for generating weights/bias terms in the layer Factory Functions Secondly we have 2 factory functions to create a Layer. One of which takes the random seed to use, and the other which doesn’t (so the RNG will be different every time). The basic version will take the number of neurons we want in the layer, along with a required LayerSetup implementation.\nEverything else is set to None by default:\npub fn new(num_neurons: usize, setup: \u0026'a dyn LayerSetup\u003c'a, T\u003e) -\u003e Self { Self { num_neurons, operations: None, setup, output: None, seed: None, } } The fixed seed version is more or less the same, except it sets the seed to the provided value:\npub fn new_with_seed(num_neurons: usize, setup: \u0026'a dyn LayerSetup\u003c'a, T\u003e, seed: u64) -\u003e Self { Self { num_neurons, operations: None, setup, output: None, seed: Some(seed), } } Forward Pass Both the forward and backward functions are inside an impl block that looks as follows:\nimpl\u003c'a, T: Clone\u003e Layer\u003c'a, T\u003e { ... } That is, we place the bound of Clone on the type T. The reason for this is simply that the forward method takes “input” by reference, however we will need it by value for folding as we’ll see shortly, so we need to clone it.\nThe signature of the forward method itself is identical to the forward method on Operation. This is not a coincidence as Layer is wrapping multiple Operations while keeping the API the same. This is the Composite design pattern where one larger thing has the same API as the smaller unit, but which encapsulates multiple of those smaller units.\npub fn forward(\u0026mut self, input: \u0026Array\u003cT, Ix2\u003e) -\u003e Array\u003cT, Ix2\u003e { ... } The first step on the forward pass is to make sure that we generate (using the LayerSetup) the list of Operations on the first forward pass. We can actually do this nicely in Rust by using a method on Option called get_or_insert_with. This function takes a callback, and calls it if the Option is currently None. The return value from the function is then set as the value of the Option for further runs. Finally a reference to the got or inserted entry is returned.\nlet operations = self .operations .get_or_insert_with(|| self.setup.layer_setup(self.num_neurons, input, \u0026self.seed)); In order to perform the forward pass, we basically just want to start off with the input we were provided, and then pass it through the chain of operations. We can easily do this with the fold method on iterators. This method takes an initial state (starts as the provided input), and a function that is called on every item in the iterator that can update and return new state. Once we’ve gone through all elements, the final value is returned, which we’ll stash in self.output:\nself.output = Some( operations .into_iter() .fold((*input).clone(), |state, operation| { operation.forward(\u0026state) }), ); Finally we can return a clone of the output (we can’t return the output itself as we need a copy for the backward pass):\n(self.output.as_ref().unwrap()).clone() Backward Pass For the backward pass, just like with Operation, we will first need to validate the shape of the provided output gradient is correct before passing it backwards through the operations list.\nThe signature also is very familiar as it mimics the same for Operation:\npub fn backward(\u0026mut self, output_gradient: \u0026Array\u003cT, Ix2\u003e) -\u003e Array\u003cT, Ix2\u003e { ... } We firstly verify that the shape of the provided output gradient matches that of the output of the layer, as calculated from the forward pass:\nassert_eq!( self.output.as_ref().unwrap().raw_dim(), output_gradient.raw_dim() ); Passing the output gradient through the series of Operations to get an input gradient to return is the same as the forward pass almost. We do need to remember to run backwards through the vector of operations. We can get a reversed iterator with the rev() method:\nself.operations .as_mut() .unwrap() .into_iter() .rev() .fold((*output_gradient).clone(), |state, operation| { operation.backward(\u0026state) }) Parameters And Gradients Finally we need a way to process all the parameters and gradients from ParameterOperations on the layer. This is why we added the parameter_and_gradient method to Operation, so that we can access those references from the layer.\nThis function is a bit tricky to write however, because the reference to the parameter is mutable.\nRust doesn’t allow multiple mutable references to the same item (or even a mutable and immutable reference active at the same time) and so we can’t just return references as returned from parameter_and_gradient. This is because those are tied to the lifetime of the Operation itself, rather than the lifetime of the iterator.\nTo convince Rust that the returned references have the same lifetime as the iterator, we can specify explicitly that the references have the same lifetime as the \u0026mut self parameter to the method. The iterator owns the \u0026mut self borrow until it goes out of scope, so we can tie the lifetime of the references returned as items to the lifetime as the borrow of Self fairly easy:\npub fn parameters_and_gradients\u003c'b\u003e(\u0026'b mut self) -\u003e impl Iterator\u003cItem = (\u0026'b mut Array\u003cT, Ix2\u003e, \u0026'b Array\u003cT, Ix2\u003e)\u003e { ... } Here we are introducing the lifetime parameter ‘b, and saying that this borrow \u0026mut self lasts for that length of time. Then we’re saying that the items in the returned iterator also live for this time too.\nThe next step is that we require a little unsafe….\nThe problem we have now is that the vector of Operation instances we have is of the following type:\nVec",
  "wordCount" : "3339",
  "inLanguage": "en",
  "datePublished": "2022-02-27T17:47:20Z",
  "dateModified": "2022-02-27T17:47:20Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/layers/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Robin Smith",
    "logo": {
      "@type": "ImageObject",
      "url": "https://forgottenmaster.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://forgottenmaster.github.io/" accesskey="h" title="Robin Smith (Alt + H)">Robin Smith</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://forgottenmaster.github.io/resume/" title="Résumé">
                    <span>Résumé</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://forgottenmaster.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/">Posts</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/">Machine Learning</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/">Deep Learning From Scratch</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/">Chapter 3 - Deep Learning</a></div>
    <h1 class="post-title entry-hint-parent">
      3.2 - Layers
    </h1>
    <div class="post-meta"><span title='2022-02-27 17:47:20 +0000 UTC'>February 27, 2022</span>&nbsp;·&nbsp;16 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#overview" aria-label="Overview">Overview</a></li>
                <li>
                    <a href="#neurons" aria-label="Neurons">Neurons</a></li>
                <li>
                    <a href="#a-small-detour" aria-label="A small detour">A small detour</a></li>
                <li>
                    <a href="#layersetup" aria-label="LayerSetup">LayerSetup</a></li>
                <li>
                    <a href="#denselayersetup" aria-label="DenseLayerSetup">DenseLayerSetup</a></li>
                <li>
                    <a href="#layer" aria-label="Layer">Layer</a><ul>
                        
                <li>
                    <a href="#definition" aria-label="Definition">Definition</a></li>
                <li>
                    <a href="#factory-functions" aria-label="Factory Functions">Factory Functions</a></li>
                <li>
                    <a href="#forward-pass" aria-label="Forward Pass">Forward Pass</a></li>
                <li>
                    <a href="#backward-pass" aria-label="Backward Pass">Backward Pass</a></li>
                <li>
                    <a href="#parameters-and-gradients" aria-label="Parameters And Gradients">Parameters And Gradients</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h3>
<p>The lowest level of abstraction as mentioned in the previous post is the <a href="../operations">operation</a>. These were the lowest level building block that is able to perform a single unit of calculation on the forward pass, and single unit of partial derivative calculation on the backward pass.</p>
<p>During that chapter, we could see how multiple of these operations are able to be chained together, though we don&rsquo;t want to do this by hand. Instead, we would like an abstraction that has the same interface as the operations (a forward and backward method) but which internally handles routing the data through the operations that are part of the layer.</p>
<p>We define a layer as a series of linear operations, followed by a final (optional) non-linear operation known as the &ldquo;activation function&rdquo;.</p>
<p>In the context of the simple neural network implemented in the last chapter, we had 5 operations that were:</p>
<ol>
<li>A <strong>WeightMultiply</strong> operation which takes the initial input and selection of weights and multiplies them together, ending with a matrix that represents the weighted sum for each of the &ldquo;learned features&rdquo; that we wanted in this &ldquo;hidden layer&rdquo;.</li>
<li>A <strong>BiasAdd</strong> operation which added in the bias term to the weighted multiplication.</li>
<li>A <strong>Sigmoid</strong> function that took the results and &ldquo;squashed&rdquo; them to a non-linear range between 0 and 1.</li>
<li>A <strong>WeightMultiply</strong> operation which took the inputs from the learned features, and multiplied them with a second set of weights.</li>
<li>A <strong>BiasAdd</strong> operation which added in the bias term to the calculated weighted sum (which was a single value at this point).</li>
</ol>
<p>We can see that there are 2 layers here:</p>
<ol>
<li>Operations 1 to 3 comprise the first layer which takes the initial input, operates on it, and produces values for a set of learned features (or as we&rsquo;ll use here, &ldquo;neurons&rdquo;)</li>
<li>Operations 4 and 5 will comprise the final layer which takes the learned hidden features to the final output</li>
</ol>
<p>A diagram below shows this grouping of operations (with the loss function tacked onto the end):</p>
<p><img loading="lazy" src="layers_diagram.png" alt="Layers Diagram"  />
</p>
<h3 id="neurons">Neurons<a hidden class="anchor" aria-hidden="true" href="#neurons">#</a></h3>
<p>Rather than looking at the individual operations within the layers, we instead look at neural networks at the abstraction of these layers. A layer is, as mentioned previously a series of non-linear operations followed by a linear one, but another important facet of a layer is the number of <em>neurons</em> it contains.</p>
<p>In the brain, neurons will take electrical impulses from other connected neurons and will <em>fire</em> if that energy exceeds some certain threshold (which then causes other neurons to possibly fire, etc.). In the case of artificial neural networks, this activation is modeled by transformation of the input to an output through a non-linear function (the Sigmoid function for example), which we also can call the <em>activation function</em> for a layer. The values coming out of this are the <em>activations</em> for that layer.</p>
<p>The number of neurons in an artificial neural network will correspond to the number of features in the input. Therefore in the simple neural network seen in the last chapter, and whose operations are briefed above we can see that:</p>
<ol>
<li>The number of neurons in the <strong>input</strong> layer is X where X is the number of features in the data set.</li>
<li>The number of neurons in the <strong>hidden</strong> layer is N where N is some number that we define. The output from this layer will be a matrix with the number of rows as in the input, and the number of columns equal to the neuron count for that layer.</li>
<li>The number of neurons from the <strong>output</strong> layer is 1. This is because we&rsquo;re collapsing the values down to a single prediction for each observation (to allow us to use the loss function to detect error).</li>
</ol>
<p>Deep learning is simply using neural networks with <strong>more than one</strong> hidden layer.</p>
<p>A more useful diagram below doesn&rsquo;t show the operations inside the layers, but abstracts the layers only to show their inputs and outputs:</p>
<p><img loading="lazy" src="layers_abstraction.png" alt="Layers Abstraction"  />
</p>
<h3 id="a-small-detour">A small detour<a hidden class="anchor" aria-hidden="true" href="#a-small-detour">#</a></h3>
<p>Before delving into the implementation for the Layer abstraction, we first address a need for a function that will be required to be added to the &ldquo;Operation&rdquo; type that we implemented in the previous section.</p>
<p>For ParameterOperation instances we stored the parameter being used, along with the last computed gradient for the parameter in the backward pass. One thing that we will need and will lift to the Layer abstraction is the ability to access temporarily the parameter and parameter gradients in use.</p>
<p>In order to achieve this, we will add the following function to the Operation type:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">parameter_and_gradient</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self) -&gt; Option<span style="color:#f92672">&lt;</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, <span style="color:#f92672">&amp;</span>Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>)<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">..</span>.
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>This function will return the parameter as a mutable reference, since we will eventually need to be able to update the parameter, given the gradient. However the gradient doesn&rsquo;t need to be mutable so we return an immutable reference.</p>
<p>One thing to note here is that not all operations have a secondary parameter (those that are BasicOperations) - therefore we return an Option to allow for this possibility.</p>
<p>The implementation of this function then look as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">match</span> <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self.data {
</span></span><span style="display:flex;"><span>    OperationData::Parameter {
</span></span><span style="display:flex;"><span>        parameter,
</span></span><span style="display:flex;"><span>        parameter_gradient,
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">..</span>
</span></span><span style="display:flex;"><span>    } <span style="color:#f92672">=&gt;</span> Some((parameter, parameter_gradient.as_ref().unwrap())),
</span></span><span style="display:flex;"><span>    _ <span style="color:#f92672">=&gt;</span> None,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>That is, we must match on the OperationData enum value we have. In the case of it being a parameter operation, we can get the references we want, and return these in a &ldquo;Some&rdquo; variant. If not, then we return None as there&rsquo;s no parameters to get.</p>
<h3 id="layersetup">LayerSetup<a hidden class="anchor" aria-hidden="true" href="#layersetup">#</a></h3>
<p>Just as we have the BasicOperation and ParameterOperation traits to allow for customization of the calculations for specific operations, we would also like to define a static structure for a Layer while allowing construction of that layer (determining which operations, etc.) to be customized.</p>
<p>This is the purpose of the LayerSetup trait. We will shortly see that the structure of Layer is static while delegating the actual setup of the layer to a trait object implementing this trait.</p>
<p>The trait header looks as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">trait</span> LayerSetup<span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">..</span>.
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>This again requires the same generic parameters that are required by Operation. That is:</p>
<ol>
<li><strong>&lsquo;a</strong> - This is the lifetime of the borrow for the BasicOperation or ParameterOperation vtables in use</li>
<li><strong>T</strong> - This is the type of the elements inside the ndarray::Array</li>
</ol>
<p>Looking at the trait itself, it only requires implementors to provide the following function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">layer_setup</span>(
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&amp;</span>self,
</span></span><span style="display:flex;"><span>    num_neurons: <span style="color:#66d9ef">usize</span>,
</span></span><span style="display:flex;"><span>    input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    seed: <span style="color:#66d9ef">&amp;</span>Option<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">u64</span><span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>) -&gt; Vec<span style="color:#f92672">&lt;</span>Operation<span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T<span style="color:#f92672">&gt;&gt;</span>;</span></span></code></pre></div>
<p>This is a function that, given the number of neurons/features that the layer should have, along with the input (used to determine the number of features in the input) can return the list of Operations required in that layer.</p>
<p>The additional parameter is an optional seed used for random number generation. If this is None then it&rsquo;s expected that the numbers should be truly randomly generated, if it&rsquo;s Some then the seed shouild be used to construct the PRNG stream.</p>
<h3 id="denselayersetup">DenseLayerSetup<a hidden class="anchor" aria-hidden="true" href="#denselayersetup">#</a></h3>
<p>Previously we saw that the layers (the hidden and output layers) in our neural network fully connects the neurons of the previous layers and consists of a sequence of <strong>WeightMultiply -&gt; BiasAdd -&gt; ActivationFunction</strong> where ActivationFunction is some non-linear function. For the hidden layer we used Sigmoid and for the output layer we used a Linear activation function which is just a passthrough function.</p>
<p>However the basic concept of fully connecting the neurons of the previous layer, and using a weighted sum and bias addition is consistent between layers. The activation function needs to be tweakable.</p>
<p>The terminology for these fully connected layers is a <em>dense</em> layer (due to it being densely connected).</p>
<p>This, therefore will be known as the DenseLayerSetup.</p>
<p>As mentioned above, we will need to modify the activation function in use, so we will keep this as a field. As before, we will need to make it generic over the lifetime, &lsquo;a, and the type T. We know that the activation function should be one that doesn&rsquo;t take an additional parameter (if this proves an incorrect assumption we can change later), so we can take just the BasicOperation trait object rather than a wrapped Operation.</p>
<p>The structure definition therefore is as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">DenseLayerSetup</span><span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T<span style="color:#f92672">&gt;</span>(<span style="color:#f92672">&amp;</span><span style="color:#a6e22e">&#39;a</span> <span style="color:#66d9ef">dyn</span> BasicOperation<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>);</span></span></code></pre></div>
<p>Next, we want a simple constructor function for this. We don&rsquo;t want to make the fields public since they could be changed after construction, so instead we keep the field private and provide a constructor function for external use.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T<span style="color:#f92672">&gt;</span> DenseLayerSetup<span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">new</span>(activation: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">&#39;a</span> <span style="color:#a6e22e">dyn</span> BasicOperation<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Self</span> {
</span></span><span style="display:flex;"><span>        Self(activation)
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>Finally, we have the actual trait implementation. First of all, we have the header for the impl block:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T: <span style="color:#a6e22e">LinalgScalar</span><span style="color:#f92672">&gt;</span> LayerSetup<span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">for</span> DenseLayerSetup<span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">where</span>
</span></span><span style="display:flex;"><span>    Standard: <span style="color:#a6e22e">Distribution</span><span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">..</span>.
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>A couple of things to note here:</p>
<ol>
<li>We require a trait bound of LinalgScalar on the generic parameter, T. This is because the layer needs to use the WeightMultiply and BiasAdd operations that both put a restriction of LinalgScalar on their generic parameter (due to requiring the arithmetic operations, etc.)</li>
<li>We are using random generation for the initial parameters on the weight and bias operations. In order to use this random number generation, we require that our type T is such that the structure &ldquo;Standard&rdquo; implements Distribution<!-- raw HTML omitted -->.</li>
</ol>
<p>Next is the actual function implementation. I won&rsquo;t repeat the function signature here as it was specified in the LayerSetup section.</p>
<p>The first step in the implementation is to get the random number generator instance. This is going to depend on if there&rsquo;s a random seed specified or not. If there is, we will use it to construct the generator. If not, we get a truly random generator. This code looks as followws:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> r <span style="color:#f92672">=</span> <span style="color:#66d9ef">match</span> seed {
</span></span><span style="display:flex;"><span>    Some(seed) <span style="color:#f92672">=&gt;</span> StdRng::seed_from_u64(<span style="color:#f92672">*</span>seed),
</span></span><span style="display:flex;"><span>    None <span style="color:#f92672">=&gt;</span> StdRng::from_rng(thread_rng()).unwrap(),
</span></span><span style="display:flex;"><span>};</span></span></code></pre></div>
<p>Next, we need to generate the random weights and bias values to use as parameters to the appropriate operations. The steps for this are as follows:</p>
<ol>
<li>We figure out what dimensions we require for the weights matrix. The number of rows in the weights matrix will match the number of columns/features in the input data - this allows us to perform the matrix multiplication. The number of columns in the weights matrix will then be the number of output neurons we require in the layer.</li>
<li>Calculate the number of elements in the matrix which we use for creating the buffer in the next step. This is just the number of rows multiplied by number of columns.</li>
<li>Create a vector with a number of elements calculated in step #2. Each element is a randomly generated value using the generator we made previously. In order to make these, we can use the <strong>repeat_with</strong> function from std::iter which takes a function and calls it repeatedly. We provide a function that just produces random values. Then we use the <strong>take</strong> adapter to take the appropriate number of elements, and finally collect them into a vector.</li>
<li>Finally, we use the <strong>Array::from_shape_vec</strong> constructor to turn the Vec into an Array.</li>
</ol>
<p>The code for the above looks as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> weight_dim <span style="color:#f92672">=</span> (input.ncols(), num_neurons);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> weight_count <span style="color:#f92672">=</span> weight_dim.<span style="color:#ae81ff">0</span> <span style="color:#f92672">*</span> weight_dim.<span style="color:#ae81ff">1</span>;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> weights <span style="color:#f92672">=</span> repeat_with(<span style="color:#f92672">||</span> r.gen::<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>()).take(weight_count).collect();
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> weights <span style="color:#f92672">=</span> Array::from_shape_vec(weight_dim, weights).unwrap();</span></span></code></pre></div>
<p>Creating the bias matrix is the same, except the dimensions are different. The bias matrix always consists of a single row, and the number of columns is the number of neurons we have. This looks as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> bias_dim <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span>, num_neurons);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> bias_count <span style="color:#f92672">=</span> bias_dim.<span style="color:#ae81ff">0</span> <span style="color:#f92672">*</span> bias_dim.<span style="color:#ae81ff">1</span>;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> bias <span style="color:#f92672">=</span> repeat_with(<span style="color:#f92672">||</span> r.gen::<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>()).take(bias_count).collect();
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> bias <span style="color:#f92672">=</span> Array::from_shape_vec(bias_dim, bias).unwrap();</span></span></code></pre></div>
<p>Finally we have the parameters we need to create the layer. We make sure to include the activation function that we stored as a field as the last operation in the sequence:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>vec![
</span></span><span style="display:flex;"><span>    Operation::new_parameter(<span style="color:#f92672">&amp;</span>WeightMultiply, weights),
</span></span><span style="display:flex;"><span>    Operation::new_parameter(<span style="color:#f92672">&amp;</span>BiasAdd, bias),
</span></span><span style="display:flex;"><span>    Operation::new_basic(self.<span style="color:#ae81ff">0</span>),
</span></span><span style="display:flex;"><span>]</span></span></code></pre></div>
<h3 id="layer">Layer<a hidden class="anchor" aria-hidden="true" href="#layer">#</a></h3>
<p>Finally we can implement the abstraction for a Layer. As with Operation, I&rsquo;ll split this up into sub-headers to break it up a little.</p>
<h4 id="definition">Definition<a hidden class="anchor" aria-hidden="true" href="#definition">#</a></h4>
<p>The structure definition looks as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">Layer</span><span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    num_neurons: <span style="color:#66d9ef">usize</span>,
</span></span><span style="display:flex;"><span>    operations: Option<span style="color:#f92672">&lt;</span>Vec<span style="color:#f92672">&lt;</span>Operation<span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T<span style="color:#f92672">&gt;&gt;&gt;</span>,
</span></span><span style="display:flex;"><span>    setup: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">&#39;a</span> <span style="color:#a6e22e">dyn</span> LayerSetup<span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T<span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>    output: Option<span style="color:#f92672">&lt;</span>Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;&gt;</span>,
</span></span><span style="display:flex;"><span>    seed: Option<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">u64</span><span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>As always, we need to take the type of elements in the arrays (T) and lifetime of the OperationData vtables (&lsquo;a).Following is a description of each parameter within the definition however:</p>
<ol>
<li><strong>num_neurons</strong> - This is the number of columns/features in the output of the layer. It&rsquo;s used to generate weights and bias of a correct size to produce the desired dimension</li>
<li><strong>operations</strong> - This is an owned vector of the Operations in the layer. It&rsquo;s an Option here because we can default to None until the first forward pass is performed. We could use an empty Vector but that would not be as explicit. This way if someone tries to use the list of operations without doing a forward pass first then they&rsquo;ll see it&rsquo;s invalid by the program panicking</li>
<li><strong>setup</strong> - A reference to the LayerSetup to use to produce the list of Operations on the first forward pass</li>
<li><strong>output</strong> - Store the calculated output from the forward pass which can be used then on a backward pass</li>
<li><strong>seed</strong> - The optional random seed to use for generating weights/bias terms in the layer</li>
</ol>
<h4 id="factory-functions">Factory Functions<a hidden class="anchor" aria-hidden="true" href="#factory-functions">#</a></h4>
<p>Secondly we have <strong>2</strong> factory functions to create a Layer. One of which takes the random seed to use, and the other which doesn&rsquo;t (so the RNG will be different every time). The basic version will take the number of neurons we want in the layer, along with a required LayerSetup implementation.</p>
<p>Everything else is set to None by default:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">new</span>(num_neurons: <span style="color:#66d9ef">usize</span>, setup: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">&#39;a</span> <span style="color:#a6e22e">dyn</span> LayerSetup<span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Self</span> {
</span></span><span style="display:flex;"><span>    Self {
</span></span><span style="display:flex;"><span>        num_neurons,
</span></span><span style="display:flex;"><span>        operations: None,
</span></span><span style="display:flex;"><span>        setup,
</span></span><span style="display:flex;"><span>        output: None,
</span></span><span style="display:flex;"><span>        seed: None,
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>The fixed seed version is more or less the same, except it sets the seed to the provided value:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">new_with_seed</span>(num_neurons: <span style="color:#66d9ef">usize</span>, setup: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">&#39;a</span> <span style="color:#a6e22e">dyn</span> LayerSetup<span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T<span style="color:#f92672">&gt;</span>, seed: <span style="color:#66d9ef">u64</span>) -&gt; <span style="color:#a6e22e">Self</span> {
</span></span><span style="display:flex;"><span>    Self {
</span></span><span style="display:flex;"><span>        num_neurons,
</span></span><span style="display:flex;"><span>        operations: None,
</span></span><span style="display:flex;"><span>        setup,
</span></span><span style="display:flex;"><span>        output: None,
</span></span><span style="display:flex;"><span>        seed: Some(seed),
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<h4 id="forward-pass">Forward Pass<a hidden class="anchor" aria-hidden="true" href="#forward-pass">#</a></h4>
<p>Both the forward and backward functions are inside an impl block that looks as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T: Clone<span style="color:#f92672">&gt;</span> Layer<span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">..</span>.
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>That is, we place the bound of Clone on the type T. The reason for this is simply that the forward method takes &ldquo;input&rdquo; by reference, however we will need it by value for folding as we&rsquo;ll see shortly, so we need to clone it.</p>
<p>The signature of the forward method itself is identical to the forward method on Operation. This is not a coincidence as Layer is wrapping multiple Operations while keeping the API the same. This is the <em>Composite</em> design pattern where one larger thing has the same API as the smaller unit, but which encapsulates multiple of those smaller units.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">forward</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, input: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">..</span>.
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>The first step on the forward pass is to make sure that we generate (using the LayerSetup) the list of Operations on the first forward pass. We can actually do this nicely in Rust by using a method on Option<!-- raw HTML omitted --> called <strong>get_or_insert_with</strong>. This function takes a callback, and calls it if the Option is currently None. The return value from the function is then set as the value of the Option for further runs. Finally a reference to the got or inserted entry is returned.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> operations <span style="color:#f92672">=</span> self
</span></span><span style="display:flex;"><span>    .operations
</span></span><span style="display:flex;"><span>    .get_or_insert_with(<span style="color:#f92672">||</span> self.setup.layer_setup(self.num_neurons, input, <span style="color:#f92672">&amp;</span>self.seed));</span></span></code></pre></div>
<p>In order to perform the forward pass, we basically just want to start off with the input we were provided, and then pass it through the chain of operations. We can easily do this with the <strong>fold</strong> method on iterators. This method takes an initial state (starts as the provided input), and a function that is called on every item in the iterator that can update and return new state. Once we&rsquo;ve gone through all elements, the final value is returned, which we&rsquo;ll stash in self.output:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>self.output <span style="color:#f92672">=</span> Some(
</span></span><span style="display:flex;"><span>    operations
</span></span><span style="display:flex;"><span>        .into_iter()
</span></span><span style="display:flex;"><span>        .fold((<span style="color:#f92672">*</span>input).clone(), <span style="color:#f92672">|</span>state, operation<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>            operation.forward(<span style="color:#f92672">&amp;</span>state)
</span></span><span style="display:flex;"><span>        }),
</span></span><span style="display:flex;"><span>);</span></span></code></pre></div>
<p>Finally we can return a clone of the output (we can&rsquo;t return the output itself as we need a copy for the backward pass):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>(self.output.as_ref().unwrap()).clone()</span></span></code></pre></div>
<h4 id="backward-pass">Backward Pass<a hidden class="anchor" aria-hidden="true" href="#backward-pass">#</a></h4>
<p>For the backward pass, just like with Operation, we will first need to validate the shape of the provided output gradient is correct before passing it <strong>backwards</strong> through the operations list.</p>
<p>The signature also is very familiar as it mimics the same for Operation:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">backward</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, output_gradient: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>) -&gt; <span style="color:#a6e22e">Array</span><span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">..</span>.
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>We firstly verify that the shape of the provided output gradient matches that of the output of the layer, as calculated from the forward pass:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>assert_eq!(
</span></span><span style="display:flex;"><span>    self.output.as_ref().unwrap().raw_dim(),
</span></span><span style="display:flex;"><span>    output_gradient.raw_dim()
</span></span><span style="display:flex;"><span>);</span></span></code></pre></div>
<p>Passing the output gradient through the series of Operations to get an input gradient to return is the same as the forward pass almost. We do need to remember to run <em>backwards</em> through the vector of operations. We can get a reversed iterator with the <strong>rev()</strong> method:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>self.operations
</span></span><span style="display:flex;"><span>    .as_mut()
</span></span><span style="display:flex;"><span>    .unwrap()
</span></span><span style="display:flex;"><span>    .into_iter()
</span></span><span style="display:flex;"><span>    .rev()
</span></span><span style="display:flex;"><span>    .fold((<span style="color:#f92672">*</span>output_gradient).clone(), <span style="color:#f92672">|</span>state, operation<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>        operation.backward(<span style="color:#f92672">&amp;</span>state)
</span></span><span style="display:flex;"><span>    })</span></span></code></pre></div>
<h4 id="parameters-and-gradients">Parameters And Gradients<a hidden class="anchor" aria-hidden="true" href="#parameters-and-gradients">#</a></h4>
<p>Finally we need a way to process all the parameters and gradients from ParameterOperations on the layer. This is why we added the parameter_and_gradient method to Operation, so that we can access those references from the layer.</p>
<p>This function is a bit tricky to write however, because the reference to the parameter is mutable.</p>
<p>Rust doesn&rsquo;t allow multiple mutable references to the same item (or even a mutable and immutable reference active at the same time) and so we can&rsquo;t just return references as returned from parameter_and_gradient. This is because those are tied to the lifetime of the Operation itself, rather than the lifetime of the iterator.</p>
<p>To convince Rust that the returned references have the same lifetime as the iterator, we can specify explicitly that the references have the same lifetime as the <strong>&amp;mut self</strong> parameter to the method. The iterator owns the &amp;mut self borrow until it goes out of scope, so we can tie the lifetime of the references returned as items to the lifetime as the borrow of Self fairly easy:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">parameters_and_gradients</span><span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;b</span><span style="color:#f92672">&gt;</span>(<span style="color:#f92672">&amp;</span><span style="color:#a6e22e">&#39;b</span> <span style="color:#66d9ef">mut</span> self) -&gt; <span style="color:#a6e22e">impl</span> Iterator<span style="color:#f92672">&lt;</span>Item <span style="color:#f92672">=</span> (<span style="color:#f92672">&amp;</span><span style="color:#a6e22e">&#39;b</span> <span style="color:#66d9ef">mut</span> Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>, <span style="color:#f92672">&amp;</span><span style="color:#a6e22e">&#39;b</span> Array<span style="color:#f92672">&lt;</span>T, Ix2<span style="color:#f92672">&gt;</span>)<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">..</span>.
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>Here we are introducing the lifetime parameter &lsquo;b, and saying that this borrow <strong>&amp;mut self</strong> lasts for that length of time. Then we&rsquo;re saying that the items in the returned iterator also live for this time too.</p>
<p>The next step is that we require a little unsafe&hellip;.</p>
<p>The problem we have now is that the vector of Operation instances we have is of the following type:</p>
<p><strong>Vec&lt;Operation&lt;&lsquo;a, T&raquo;</strong></p>
<p>Now &lsquo;a is the lifetime of the Operation itself (or more specifically the lifetime of the &amp; dyn BasicOperation/&amp; dyn ParameterOperation references inside). So if we were to iterate over that as it is now, then it would produce references with lifetime &lsquo;a.</p>
<p>This means the produced references from the iterator would live for &lsquo;a, but the iterator itself only for &lsquo;b. Thus, there&rsquo;s a conflict and the compiler will complain.</p>
<p>In order to solve this, we can use (unsafely) std::mem::transmute to <em>very carefully</em> change Operation&lt;&lsquo;a, T&gt; into Operation&lt;&lsquo;b, T&gt;.</p>
<p>Now this is perfectly safe in fact because we know that the references <em>actually</em> live for &lsquo;a, which is guaranteed to live at least as long as &lsquo;b. Therefore there&rsquo;s no harm in shortening the lifetime to &lsquo;b on those Operation instances.</p>
<p>The code for that is as follows (note that I&rsquo;m coercing the Vec into a slice and transmuting that, but either works):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> operations <span style="color:#f92672">=</span> <span style="color:#66d9ef">unsafe</span> {
</span></span><span style="display:flex;"><span>    transmute::<span style="color:#f92672">&lt;&amp;</span><span style="color:#a6e22e">&#39;b</span> <span style="color:#66d9ef">mut</span> [Operation<span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span>, T<span style="color:#f92672">&gt;</span>], <span style="color:#f92672">&amp;</span><span style="color:#a6e22e">&#39;b</span> <span style="color:#66d9ef">mut</span> [Operation<span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;b</span>, T<span style="color:#f92672">&gt;</span>]<span style="color:#f92672">&gt;</span>(
</span></span><span style="display:flex;"><span>        self.operations.as_mut().unwrap(),
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>};</span></span></code></pre></div>
<p>Once we&rsquo;ve done this transmutation to shorten the invariant lifetime on Operation, the rest of the function is simple enough and is just a case of doing a <strong>filter_map</strong> and calling parameter_and_gradient on each operation.</p>
<p>parameter_and_gradient returns None if it&rsquo;s a BasicOperation, or Some((param, grad)) if it&rsquo;s a ParameterOperation so we can return this from filter_map directly.</p>
<p>filter_map will only pass forward any Some values and will unwrap them as it runs through them:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>operations
</span></span><span style="display:flex;"><span>    .into_iter()
</span></span><span style="display:flex;"><span>    .filter_map(<span style="color:#f92672">|</span>op<span style="color:#f92672">|</span> op.parameter_and_gradient())</span></span></code></pre></div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://forgottenmaster.github.io/">Robin Smith</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
