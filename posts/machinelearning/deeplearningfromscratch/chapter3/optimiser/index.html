<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>3.6 - Optimiser | Robin Firth</title>
<meta name="keywords" content="" />
<meta name="description" content="Now that the layers of abstraction are done, we need to work on actually running the training of the network. We will be encapsulating the optimisation process to allow for the API user to select the strategy they would like to use to apply the gradients to parameters when training the network. This will be a fairly short post as the actual optimisation process is pretty simple.
The Trait An Optimiser has a single job in the network training process.">
<meta name="author" content="">
<link rel="canonical" href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/optimiser/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css" integrity="sha256-b2AFbUTT9&#43;tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js" integrity="sha256-uVus3DnjejMqn4g7Hni&#43;Srwf3KK8HyZB9V4809q9TWE="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://forgottenmaster.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://forgottenmaster.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://forgottenmaster.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://forgottenmaster.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://forgottenmaster.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.110.0">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="3.6 - Optimiser" />
<meta property="og:description" content="Now that the layers of abstraction are done, we need to work on actually running the training of the network. We will be encapsulating the optimisation process to allow for the API user to select the strategy they would like to use to apply the gradients to parameters when training the network. This will be a fairly short post as the actual optimisation process is pretty simple.
The Trait An Optimiser has a single job in the network training process." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/optimiser/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-05T22:45:21&#43;01:00" />
<meta property="article:modified_time" content="2022-04-05T22:45:21&#43;01:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="3.6 - Optimiser"/>
<meta name="twitter:description" content="Now that the layers of abstraction are done, we need to work on actually running the training of the network. We will be encapsulating the optimisation process to allow for the API user to select the strategy they would like to use to apply the gradients to parameters when training the network. This will be a fairly short post as the actual optimisation process is pretty simple.
The Trait An Optimiser has a single job in the network training process."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://forgottenmaster.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Machine Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Deep Learning From Scratch",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Chapter 3 - Deep Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/"
    }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "3.6 - Optimiser",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/optimiser/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "3.6 - Optimiser",
  "name": "3.6 - Optimiser",
  "description": "Now that the layers of abstraction are done, we need to work on actually running the training of the network. We will be encapsulating the optimisation process to allow for the API user to select the strategy they would like to use to apply the gradients to parameters when training the network. This will be a fairly short post as the actual optimisation process is pretty simple.\nThe Trait An Optimiser has a single job in the network training process.",
  "keywords": [
    
  ],
  "articleBody": "Now that the layers of abstraction are done, we need to work on actually running the training of the network. We will be encapsulating the optimisation process to allow for the API user to select the strategy they would like to use to apply the gradients to parameters when training the network. This will be a fairly short post as the actual optimisation process is pretty simple.\nThe Trait An Optimiser has a single job in the network training process. To apply the calculated partial derivatives for the parameter operations in the network to the parameters themselves. The basic form as we’ll see shortly is just applying some proportion of the gradients to the parameters.\nHowever, it’s entirely possible the API user may wish to update them in a different way, such as updating using a historical average to smooth out the modifications over time.\nAt the end of the day though, the Optimiser trait only needs a single method, which is called to update the provided Network. The trait definition is as follows:\npub trait Optimiser\u003cT\u003e { fn step(\u0026mut self, net: \u0026mut Network\u003cT\u003e); } Again it needs a generic type, T, which is the type of the underlying elements in the arrays within the network.\nThe step function operates on a mutable reference to the Optimiser object itself, because it may want to for example store a history of gradients, or other information. Updating that information stored in the members of the implementor requires mutable access.\nIt takes the Network in by mutable reference also. We don’t want the implementor to own the Network, but merely borrow it for the duration of the step function. We need mutable access because we need access to and to update the parameters within the network.\nStochastic Gradient Descent This is the official terminology for the optimisation algorithm used previously. This simply runs through the parameters and gradients of the network and applies some proportion of the gradient to the parameter. This multiplier is called the learning rate.\nAs far as the structure of the SGD struct itself, we need to store the learning rate that is to be applied. This is an instance of the generic type T since we don’t know what concrete type is being used to store data within the network.\nThis definition then looks fairly trivial:\npub struct SGD\u003cT\u003e { learning_rate: T, } Secondly, we will need a way to construct a new instance of the struct. We will do this by adding a new_boxed method, since we need to store the optimiser as a trait object, thus it will need to be placed behind a box.\nThe implementation of new_boxed then shouldn’t be much of a surprise:\nimpl\u003cT\u003e SGD\u003cT\u003e { pub fn new_boxed(learning_rate: T) -\u003e Box\u003cSelf\u003e { Box::new(Self { learning_rate }) } } For the implementation of the Optimiser trait on SGD, it will look as follows:\nimpl\u003cT: LinalgScalar + ScalarOperand\u003e Optimiser\u003cT\u003e for SGD\u003cT\u003e { ... } These bounds are required due to the implementation of the function requiring arithmetic operations on the elements of the parameters and gradients arrays.\nAs for the implementation of the step function, it’s simply running over each parameter \u0026 gradient pair, and subtracting a small portion of the gradient (dictated by learning_rate) from the parameter:\nnet.parameters_and_gradients() .for_each(|(param, gradient)| { *param = \u0026*param - (gradient * self.learning_rate); }); This runs the following sequence of operations:\nCalls parameters_and_gradients on the network which, as you may recall, returns an impl Iterator",
  "wordCount" : "721",
  "inLanguage": "en",
  "datePublished": "2022-04-05T22:45:21+01:00",
  "dateModified": "2022-04-05T22:45:21+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/optimiser/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Robin Firth",
    "logo": {
      "@type": "ImageObject",
      "url": "https://forgottenmaster.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://forgottenmaster.github.io/" accesskey="h" title="Robin Firth (Alt + H)">Robin Firth</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://forgottenmaster.github.io/resume/" title="Résumé">
                    <span>Résumé</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://forgottenmaster.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/">Posts</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/">Machine Learning</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/">Deep Learning From Scratch</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/">Chapter 3 - Deep Learning</a></div>
    <h1 class="post-title">
      3.6 - Optimiser
    </h1>
    <div class="post-meta"><span title='2022-04-05 22:45:21 +0100 BST'>April 5, 2022</span>&nbsp;·&nbsp;4 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#the-trait" aria-label="The Trait">The Trait</a></li>
                <li>
                    <a href="#stochastic-gradient-descent" aria-label="Stochastic Gradient Descent">Stochastic Gradient Descent</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Now that the layers of abstraction are done, we need to work on actually running the training of the network. We will be encapsulating the optimisation process to allow for the API user to select the strategy they would like to use to apply the gradients to parameters when training the network. This will be a fairly short post as the actual optimisation process is pretty simple.</p>
<h3 id="the-trait">The Trait<a hidden class="anchor" aria-hidden="true" href="#the-trait">#</a></h3>
<p>An Optimiser has a single job in the network training process. To apply the calculated partial derivatives for the parameter operations in the network to the parameters themselves. The basic form as we&rsquo;ll see shortly is just applying some proportion of the gradients to the parameters.</p>
<p>However, it&rsquo;s entirely possible the API user may wish to update them in a different way, such as updating using a historical average to smooth out the modifications over time.</p>
<p>At the end of the day though, the Optimiser trait only needs a single method, which is called to update the provided Network. The trait definition is as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">trait</span> Optimiser<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">step</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, net: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">mut</span> Network<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>);
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>Again it needs a generic type, T, which is the type of the underlying elements in the arrays within the network.</p>
<p>The step function operates on a <strong>mutable</strong> reference to the Optimiser object itself, because it may want to for example store a history of gradients, or other information. Updating that information stored in the members of the implementor requires mutable access.</p>
<p>It takes the Network in by mutable reference also. We don&rsquo;t want the implementor to <em>own</em> the Network, but merely borrow it for the duration of the step function. We need mutable access because we need access to and to update the parameters within the network.</p>
<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent<a hidden class="anchor" aria-hidden="true" href="#stochastic-gradient-descent">#</a></h3>
<p>This is the official terminology for the optimisation algorithm used previously. This simply runs through the parameters and gradients of the network and applies some proportion of the gradient to the parameter. This multiplier is called the <strong>learning rate</strong>.</p>
<p>As far as the structure of the SGD struct itself, we need to store the learning rate that is to be applied. This is an instance of the generic type T since we don&rsquo;t know what concrete type is being used to store data within the network.</p>
<p>This definition then looks fairly trivial:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">SGD</span><span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    learning_rate: <span style="color:#a6e22e">T</span>,
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>Secondly, we will need a way to construct a new instance of the struct. We will do this by adding a <strong>new_boxed</strong> method, since we need to store the optimiser as a trait object, thus it will need to be placed behind a box.</p>
<p>The implementation of new_boxed then shouldn&rsquo;t be much of a surprise:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> SGD<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">new_boxed</span>(learning_rate: <span style="color:#a6e22e">T</span>) -&gt; Box<span style="color:#f92672">&lt;</span>Self<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>        Box::new(Self { learning_rate })
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>For the implementation of the Optimiser trait on SGD, it will look as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">impl</span><span style="color:#f92672">&lt;</span>T: <span style="color:#a6e22e">LinalgScalar</span> <span style="color:#f92672">+</span> ScalarOperand<span style="color:#f92672">&gt;</span> Optimiser<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">for</span> SGD<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">..</span>.
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>These bounds are required due to the implementation of the function requiring arithmetic operations on the elements of the parameters and gradients arrays.</p>
<p>As for the implementation of the step function, it&rsquo;s simply running over each parameter &amp; gradient pair, and subtracting a small portion of the gradient (dictated by learning_rate) from the parameter:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>net.parameters_and_gradients()
</span></span><span style="display:flex;"><span>    .for_each(<span style="color:#f92672">|</span>(param, gradient)<span style="color:#f92672">|</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">*</span>param <span style="color:#f92672">=</span> <span style="color:#f92672">&amp;*</span>param <span style="color:#f92672">-</span> (gradient <span style="color:#f92672">*</span> self.learning_rate);
</span></span><span style="display:flex;"><span>    });</span></span></code></pre></div>
<p>This runs the following sequence of operations:</p>
<ol>
<li>Calls parameters_and_gradients on the network which, as you may recall, returns an <strong>impl Iterator&lt;Item=(&amp;mut Array<!-- raw HTML omitted -->, &amp;Array<!-- raw HTML omitted -->)&gt;</strong> object. That is we don&rsquo;t know what the concrete type is, but we do know that it implements the Iterator trait.</li>
<li>We run code for each parameter and gradient pair. We use the for_each method here rather than a regular for loop in order to communicate the semantics that we are indeed updating every entry.</li>
<li>We can multiply an Array<!-- raw HTML omitted --> by a single value T thanks to the ScalarOperand bound.</li>
<li>We need to do a reborrow in this code since subtraction isn&rsquo;t implemented for a left hand side of a <em>mutable</em> reference to an Array. We also can&rsquo;t just dereference param since that would result in calling the implementation of the operator that takes an owned value on the left hand side. Therefore we need to reborrow the mutable reference as an immutable one and the compiler will find the function correctly.</li>
</ol>


  </div>

  <footer class="post-footer">
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://forgottenmaster.github.io/">Robin Firth</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
