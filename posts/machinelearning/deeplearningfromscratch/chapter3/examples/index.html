<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>3.9 - Examples | Robin Firth</title>
<meta name="keywords" content="" />
<meta name="description" content="Now that we have a basic framework for creating neural networks, we will re-visit the code examples for Linear Regression and Neural Networks that were quickly implemented in chapter 2 and reimplement them as examples for the Eidetic library.
Doing this will ensure that users of our API have some reference examples of how to create and train their networks, as well as ensuring that the API fulfils the required functionality from a users perspective.">
<meta name="author" content="">
<link rel="canonical" href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/examples/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css" integrity="sha256-b2AFbUTT9&#43;tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://forgottenmaster.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://forgottenmaster.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://forgottenmaster.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://forgottenmaster.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://forgottenmaster.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.96.0" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="3.9 - Examples" />
<meta property="og:description" content="Now that we have a basic framework for creating neural networks, we will re-visit the code examples for Linear Regression and Neural Networks that were quickly implemented in chapter 2 and reimplement them as examples for the Eidetic library.
Doing this will ensure that users of our API have some reference examples of how to create and train their networks, as well as ensuring that the API fulfils the required functionality from a users perspective." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/examples/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-18T18:47:56&#43;01:00" />
<meta property="article:modified_time" content="2022-04-18T18:47:56&#43;01:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="3.9 - Examples"/>
<meta name="twitter:description" content="Now that we have a basic framework for creating neural networks, we will re-visit the code examples for Linear Regression and Neural Networks that were quickly implemented in chapter 2 and reimplement them as examples for the Eidetic library.
Doing this will ensure that users of our API have some reference examples of how to create and train their networks, as well as ensuring that the API fulfils the required functionality from a users perspective."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://forgottenmaster.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Machine Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Deep Learning From Scratch",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Chapter 3 - Deep Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/"
    }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "3.9 - Examples",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/examples/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "3.9 - Examples",
  "name": "3.9 - Examples",
  "description": "Now that we have a basic framework for creating neural networks, we will re-visit the code examples for Linear Regression and Neural Networks that were quickly implemented in chapter 2 and reimplement them as examples for the Eidetic library.\nDoing this will ensure that users of our API have some reference examples of how to create and train their networks, as well as ensuring that the API fulfils the required functionality from a users perspective.",
  "keywords": [
    
  ],
  "articleBody": "Now that we have a basic framework for creating neural networks, we will re-visit the code examples for Linear Regression and Neural Networks that were quickly implemented in chapter 2 and reimplement them as examples for the Eidetic library.\nDoing this will ensure that users of our API have some reference examples of how to create and train their networks, as well as ensuring that the API fulfils the required functionality from a users perspective.\nCreating a prelude Most Rust library crates will contain a module named by convention as the prelude. This is located at the root of the crate, and is intended to contain the most important items, re-exported from the library.\nThe usage of the prelude will look as follows\nuse eidetic::prelude::*; This common practice allows us to avoid each user having to import items individually or from multiple sub-modules for basic usage, although allows for optional items to be not included in the prelude and be imported individually still.\nCrates can have multiple preludes if needed, since they’re just regular modules that are named by convention, however for Eidetic we only require one.\nThe prelude for Eidetic can be located HERE\nData wrangling The first thing we’ll need to do is to provide common functionality for the examples that will do the following:\n Open the data file Read the data Parse features as floating point values for training Split the data into training and testing data Separate out the targets from the input data  The data wrangling code can be found HERE for anyone that’s interested but it’s just reading the credit score information found HERE and preparing it into a format that we can use for network training.\nUsing the data wrangler simply requires that the source file be included (with the include! macro) into the example source file.\nThe include! macro in Rust will dump the source contents into the file during compilation (so that the compiler sees it as if that data wrangling source code were written directly into the example source file).\ninclude!(\"credit_score_data_wrangler.rs\"); After including the data wrangling source code, we then just need to call the get_batches and get_targets functions which will each return a tuple containing firstly the training Array, and secondly the testing array.\nThese functions both take a float (should be between 0 and 1) indicating the proportion of the input data rows that are going to be in the training set (for example 0.8 indicates that 80% of the rows in the input data will make up the training data array, and 20% makes up the testing data array).\nThese are used for example as:\nlet (training_batch, testing_batch) = get_batches(TRAINING_BATCH_PROPORTION); let (training_targets, testing_targets) = get_targets(TRAINING_BATCH_PROPORTION); Linear regression example In order to perform a linear regression on the data, we can set up a network with a single dense layer and use a linear activation function to combine the results at the end into a single neuron. The loss function we use is the mean squared error as usual, and for the optimisation strategy we use the stochastic gradient descent as mentioned previously.\nThe code to setup the network then looks like the following:\nlet mut network = Network::new(  vec![Layer::new(  1,  DenseLayerSetup::new_boxed(Linear::new_boxed()),  )],  Loss::new(MeanSquaredError::new_boxed()), ); And to setup the SGD optimiser is simple too, we need to provide a learning rate value though:\nlet mut optimiser = SGD::new(LEARNING_RATE); We can train the network with the training function which we need to pass the network and optimiser into, along with training and testing input data and targets. The final few parameters we need to pass are indicating the number of epochs we are training for, how often we evaluate the training process for early termination, the size of the batches we feed through the network in each epoch, and finally, the random seed (if we’re using one - here it’s None) we’re using. After plugging all of this in, we get the following:\ntrain(  \u0026mut network,  \u0026mut optimiser,  training_batch.clone(),  training_targets.clone(),  testing_batch.clone(),  testing_targets.clone(),  EPOCHS,  EVAL_EVERY,  BATCH_SIZE,  SEED, ); After we finish training the network, we can print out the error/loss of the trained network on both the training and testing data to see how close they are to zero (0 is perfect training with no error), and more importantly how close they are to each other (to show the network generalises well and doesn’t overfit the training data too much).\nIn the Eidetic API we can use the forward_loss function which returns the predictions (that we don’t care about) and the error loss (that we do care about). We can thus just print out the item at index 1 of the tuple (the loss) as such:\nprintln!(  \"Final error (training): {}\",  network.forward_loss(training_batch, training_targets).1 ); println!(  \"Final error (testing): {}\",  network.forward_loss(testing_batch, testing_targets).1 ); When we run this, we see that the error is around what we had without the API when the algorithm was hacked together, and that they are very close to one another:\nFinal error (training): 0.20482454235971267 Final error (testing): 0.20557973252746742 Neural network example Now that we have an example that uses Eidetic to train a network using linear regression, we literally only need to modify the setup of the network to change the linear regression into a deeper neural network.\nThe difference between a neural network and a linear regression is the addition of one or more dense “hidden” layers with multiple neurons and using the sigmoid nonlinear activation function. In the example program, we have the following setup (with 25 hidden neurons on one layer):\nlet mut network = Network::new(  vec![  Layer::new(25, DenseLayerSetup::new_boxed(Sigmoid::new_boxed())),  Layer::new(1, DenseLayerSetup::new_boxed(Linear::new_boxed())),  ],  Loss::new(MeanSquaredError::new_boxed()), ); When we run this, it takes a little longer, but we see similar values as with the linear regression:\nFinal error (training): 0.20514753984418146 Final error (testing): 0.20600618973999624 As mentioned in chapter 2 with the original implementation, the neural network approach is slightly worse than the linear regression approach, but both are around 0.2 indicating that the API seems to be working as intended.\n",
  "wordCount" : "994",
  "inLanguage": "en",
  "datePublished": "2022-04-18T18:47:56+01:00",
  "dateModified": "2022-04-18T18:47:56+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/examples/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Robin Firth",
    "logo": {
      "@type": "ImageObject",
      "url": "https://forgottenmaster.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://forgottenmaster.github.io/" accesskey="h" title="Robin Firth (Alt + H)">Robin Firth</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://forgottenmaster.github.io/resume/" title="Résumé">
                    <span>Résumé</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://forgottenmaster.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/">Posts</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/">Machine Learning</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/">Deep Learning From Scratch</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/chapter3/">Chapter 3 - Deep Learning</a></div>
    <h1 class="post-title">
      3.9 - Examples
    </h1>
    <div class="post-meta"><span title='2022-04-18 18:47:56 +0100 BST'>April 18, 2022</span>&nbsp;·&nbsp;5 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#creating-a-prelude" aria-label="Creating a prelude">Creating a prelude</a></li>
                <li>
                    <a href="#data-wrangling" aria-label="Data wrangling">Data wrangling</a></li>
                <li>
                    <a href="#linear-regression-example" aria-label="Linear regression example">Linear regression example</a></li>
                <li>
                    <a href="#neural-network-example" aria-label="Neural network example">Neural network example</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Now that we have a basic framework for creating neural networks, we will re-visit the code examples for <a href="/posts/machinelearning/deeplearningfromscratch/chapter2/linearregression">Linear Regression</a> and <a href="/posts/machinelearning/deeplearningfromscratch/chapter2/neuralnetworks">Neural Networks</a> that were quickly implemented in chapter 2 and reimplement them as examples for the Eidetic library.</p>
<p>Doing this will ensure that users of our API have some reference examples of how to create and train their networks, as well as ensuring that the API fulfils the required functionality from a users perspective.</p>
<h3 id="creating-a-prelude">Creating a prelude<a hidden class="anchor" aria-hidden="true" href="#creating-a-prelude">#</a></h3>
<p>Most Rust library crates will contain a module named by convention as the <strong>prelude</strong>. This is located at the root of the crate, and is intended to contain the most important items, re-exported from the library.</p>
<p>The usage of the prelude will look as follows</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">use</span> eidetic::prelude::<span style="color:#f92672">*</span>;</span></span></code></pre></div>
<p>This common practice allows us to avoid each user having to import items individually or from multiple sub-modules for basic usage, although allows for optional items to be not included in the prelude and be imported individually still.</p>
<p>Crates can have multiple preludes if needed, since they&rsquo;re just regular modules that are named by convention, however for Eidetic we only require one.</p>
<p>The prelude for Eidetic can be located <a href="https://github.com/ForgottenMaster/eidetic/blob/main/src/prelude.rs">HERE</a></p>
<h3 id="data-wrangling">Data wrangling<a hidden class="anchor" aria-hidden="true" href="#data-wrangling">#</a></h3>
<p>The first thing we&rsquo;ll need to do is to provide common functionality for the examples that will do the following:</p>
<ol>
<li>Open the data file</li>
<li>Read the data</li>
<li>Parse features as floating point values for training</li>
<li>Split the data into training and testing data</li>
<li>Separate out the targets from the input data</li>
</ol>
<p>The data wrangling code can be found <a href="https://github.com/ForgottenMaster/eidetic/blob/main/examples/credit_score_data_wrangler.rs">HERE</a> for anyone that&rsquo;s interested but it&rsquo;s just reading the credit score information found <a href="https://github.com/ForgottenMaster/eidetic/blob/main/examples/data/credit_scores.csv">HERE</a> and preparing it into a format that we can use for network training.</p>
<p>Using the data wrangler simply requires that the source file be included (with the include! macro) into the example source file.</p>
<p>The include! macro in Rust will dump the source contents into the file during compilation (so that the compiler sees it as if that data wrangling source code were written directly into the example source file).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>include!(<span style="color:#e6db74">&#34;credit_score_data_wrangler.rs&#34;</span>);</span></span></code></pre></div>
<p>After including the data wrangling source code, we then just need to call the <strong>get_batches</strong> and <strong>get_targets</strong> functions which will each return a tuple containing firstly the training Array, and secondly the testing array.</p>
<p>These functions both take a float (should be between 0 and 1) indicating the proportion of the input data rows that are going to be in the training set (for example 0.8 indicates that 80% of the rows in the input data will make up the training data array, and 20% makes up the testing data array).</p>
<p>These are used for example as:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> (training_batch, testing_batch) <span style="color:#f92672">=</span> get_batches(TRAINING_BATCH_PROPORTION);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> (training_targets, testing_targets) <span style="color:#f92672">=</span> get_targets(TRAINING_BATCH_PROPORTION);</span></span></code></pre></div>
<h3 id="linear-regression-example">Linear regression example<a hidden class="anchor" aria-hidden="true" href="#linear-regression-example">#</a></h3>
<p>In order to perform a linear regression on the data, we can set up a network with a single dense layer and use a <strong>linear</strong> activation function to combine the results at the end into a single neuron. The loss function we use is the <strong>mean squared error</strong> as usual, and for the optimisation strategy we use the <strong>stochastic gradient descent</strong> as mentioned previously.</p>
<p>The code to setup the network then looks like the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> network <span style="color:#f92672">=</span> Network::new(
</span></span><span style="display:flex;"><span>    vec![Layer::new(
</span></span><span style="display:flex;"><span>        <span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>        DenseLayerSetup::new_boxed(Linear::new_boxed()),
</span></span><span style="display:flex;"><span>    )],
</span></span><span style="display:flex;"><span>    Loss::new(MeanSquaredError::new_boxed()),
</span></span><span style="display:flex;"><span>);</span></span></code></pre></div>
<p>And to setup the SGD optimiser is simple too, we need to provide a learning rate value though:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> optimiser <span style="color:#f92672">=</span> SGD::new(LEARNING_RATE);</span></span></code></pre></div>
<p>We can train the network with the training function which we need to pass the network and optimiser into, along with training and testing input data and targets. The final few parameters we need to pass are indicating the number of epochs we are training for, how often we evaluate the training process for early termination, the size of the batches we feed through the network in each epoch, and finally, the random seed (if we&rsquo;re using one - here it&rsquo;s None) we&rsquo;re using. After plugging all of this in, we get the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>train(
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> network,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> optimiser,
</span></span><span style="display:flex;"><span>    training_batch.clone(),
</span></span><span style="display:flex;"><span>    training_targets.clone(),
</span></span><span style="display:flex;"><span>    testing_batch.clone(),
</span></span><span style="display:flex;"><span>    testing_targets.clone(),
</span></span><span style="display:flex;"><span>    EPOCHS,
</span></span><span style="display:flex;"><span>    EVAL_EVERY,
</span></span><span style="display:flex;"><span>    BATCH_SIZE,
</span></span><span style="display:flex;"><span>    SEED,
</span></span><span style="display:flex;"><span>);</span></span></code></pre></div>
<p>After we finish training the network, we can print out the error/loss of the trained network on both the training and testing data to see how close they are to zero (0 is perfect training with no error), and more importantly how close they are to each other (to show the network generalises well and doesn&rsquo;t overfit the training data too much).</p>
<p>In the Eidetic API we can use the <strong>forward_loss</strong> function which returns the predictions (that we don&rsquo;t care about) and the error loss (that we do care about). We can thus just print out the item at index 1 of the tuple (the loss) as such:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span>println!(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Final error (training): {}&#34;</span>,
</span></span><span style="display:flex;"><span>    network.forward_loss(training_batch, training_targets).<span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>);
</span></span><span style="display:flex;"><span>println!(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Final error (testing): {}&#34;</span>,
</span></span><span style="display:flex;"><span>    network.forward_loss(testing_batch, testing_targets).<span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>);</span></span></code></pre></div>
<p>When we run this, we see that the error is around what we had without the API when the algorithm was hacked together, and that they are very close to one another:</p>
<pre tabindex="0"><code>Final error (training): 0.20482454235971267
Final error (testing): 0.20557973252746742
</code></pre><h3 id="neural-network-example">Neural network example<a hidden class="anchor" aria-hidden="true" href="#neural-network-example">#</a></h3>
<p>Now that we have an example that uses Eidetic to train a network using linear regression, we literally only need to modify the setup of the network to change the linear regression into a deeper neural network.</p>
<p>The difference between a neural network and a linear regression is the addition of one or more dense &ldquo;hidden&rdquo; layers with multiple neurons and using the <strong>sigmoid</strong> nonlinear activation function. In the example program, we have the following setup (with 25 hidden neurons on one layer):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> network <span style="color:#f92672">=</span> Network::new(
</span></span><span style="display:flex;"><span>    vec![
</span></span><span style="display:flex;"><span>        Layer::new(<span style="color:#ae81ff">25</span>, DenseLayerSetup::new_boxed(Sigmoid::new_boxed())),
</span></span><span style="display:flex;"><span>        Layer::new(<span style="color:#ae81ff">1</span>, DenseLayerSetup::new_boxed(Linear::new_boxed())),
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    Loss::new(MeanSquaredError::new_boxed()),
</span></span><span style="display:flex;"><span>);</span></span></code></pre></div>
<p>When we run this, it takes a little longer, but we see similar values as with the linear regression:</p>
<pre tabindex="0"><code>Final error (training): 0.20514753984418146
Final error (testing): 0.20600618973999624
</code></pre><p>As mentioned in chapter 2 with the original implementation, the neural network approach is <em>slightly</em> worse than the linear regression approach, but both are around 0.2 indicating that the API seems to be working as intended.</p>


  </div>

  <footer class="post-footer">
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://forgottenmaster.github.io/">Robin Firth</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
