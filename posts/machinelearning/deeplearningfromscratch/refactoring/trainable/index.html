<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Part 5 - Trainable State | Robin Smith</title>
<meta name="keywords" content="">
<meta name="description" content="Now we&rsquo;ve bound a particular optimiser to the operation(s) it is placed into the trainable state. The optimiser instances will retain additional state for gradients, learning rates, etc. that aren&rsquo;t needed in the previous initialised state.
In this typestate we are able to run training passes (as opposed to just predictions) on the network and also, once finished, to go back to the initialised state again from which we can get the trained parameters.">
<meta name="author" content="">
<link rel="canonical" href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/trainable/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="https://forgottenmaster.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://forgottenmaster.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://forgottenmaster.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://forgottenmaster.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://forgottenmaster.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/trainable/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Part 5 - Trainable State" />
<meta property="og:description" content="Now we&rsquo;ve bound a particular optimiser to the operation(s) it is placed into the trainable state. The optimiser instances will retain additional state for gradients, learning rates, etc. that aren&rsquo;t needed in the previous initialised state.
In this typestate we are able to run training passes (as opposed to just predictions) on the network and also, once finished, to go back to the initialised state again from which we can get the trained parameters." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/trainable/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-26T20:26:21+01:00" />
<meta property="article:modified_time" content="2022-07-26T20:26:21+01:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Part 5 - Trainable State"/>
<meta name="twitter:description" content="Now we&rsquo;ve bound a particular optimiser to the operation(s) it is placed into the trainable state. The optimiser instances will retain additional state for gradients, learning rates, etc. that aren&rsquo;t needed in the previous initialised state.
In this typestate we are able to run training passes (as opposed to just predictions) on the network and also, once finished, to go back to the initialised state again from which we can get the trained parameters."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://forgottenmaster.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Machine Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Deep Learning From Scratch",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Refactoring Eidetic",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/"
    }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "Part 5 - Trainable State",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/trainable/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Part 5 - Trainable State",
  "name": "Part 5 - Trainable State",
  "description": "Now we\u0026rsquo;ve bound a particular optimiser to the operation(s) it is placed into the trainable state. The optimiser instances will retain additional state for gradients, learning rates, etc. that aren\u0026rsquo;t needed in the previous initialised state.\nIn this typestate we are able to run training passes (as opposed to just predictions) on the network and also, once finished, to go back to the initialised state again from which we can get the trained parameters.",
  "keywords": [
    
  ],
  "articleBody": "Now we’ve bound a particular optimiser to the operation(s) it is placed into the trainable state. The optimiser instances will retain additional state for gradients, learning rates, etc. that aren’t needed in the previous initialised state.\nIn this typestate we are able to run training passes (as opposed to just predictions) on the network and also, once finished, to go back to the initialised state again from which we can get the trained parameters.\nTrainableOperation This trait is pretty straightforward actually. Let’s take a look at the definition in it’s entirety for a network/operation that is ready to be trained:\npub trait TrainableOperation: Sealed { type Initialised; fn into_initialised(self) -\u003e Self::Initialised; fn init(\u0026mut self, epochs: u16); fn end_epoch(\u0026mut self); } There is only a single associated type with this one which is the type representing the previous state of operation (the initialised state). For functionality we have, as previously mentioned the two pieces of functionality we would like to use.\nDuring training For the actual training process we will need to initialise all the optimisers in the trainable network with the number of epochs that we’re running over. This is because those optimisers will use things such as learning rate decay, or otherwise be reliant on the total number of epochs we are running. Initialisation also lets them know we’re about to train in epoch 0, so can reset any data that needs to be (such as resetting those decaying learning rates back to starting values).\nAt the end of each epoch, the trainer will then call end_epoch to give any optimisers and such in the network the chance to update state before the next epoch (such as decaying learning rates).\nAfter training After we have finished running our training for however many epochs we’d like, we then need to be able to go back to the previous state in order to extract the trained weights for storage, or to make predictions without the overhead of the optimiser instances.\nThe into_initialised method will do this for us and it’s fairly straightforward. It simply consumes this trainable instance and produces an initialised form. In effect, this unwraps the trainable operation and strips away any additional data used during the training process (e.g. gradient velocities for momentum) and the resulting operation is lean, only containing what it needs to make predictions.\nWhere’s the forward pass? As you may have noticed, the trait defining an operation as being in the trainable state doesn’t actually have any functionality to run a forward pass (and thus to run a backward pass, etc.).\nThe reason for this is simple, we need to put that functionality into a separate trait, muchlike we did with the binding of the optimiser and it’s for the same reason - generics. However the generic parameters here are not generic types, but lifetimes.\nA consuming transition Let’s first take a moment to look at what the API would be if the typestate transition from “trainable” to “forward pass” consumed self in the same way as the transitions so far do:\nlet mut trainable = get_trainable_network(); // run one epoch...we'll pretend we know the output gradients before we've run our // forward pass for brevity. trainable = trainable.forward(input).backward(output_gradients).optimise(); As you can see, the API is a little….clumsy since you have to remember to re-assign to the trainable binding after running an epoch, since each step consumes self. If we didn’t re-assign and instead did something like:\nlet mut trainable = get_trainable_network(); // Epoch 0 trainable.forward(input).backward(output_gradient).optimise(); // Epoch 1 trainable.forward(input).backward(output_gradient).optimise(); // COMPILER ERROR! trainable already used A non-consuming transition Ideally, what we would like to do is to have the forward transition be non-consuming and only borrow the operation for its duration. We would like our API to look more like this:\nlet mut trainable = get_trainable_network(); trainable.forward(input_0).backward(output_gradient_0).optimise(); // Epoch 0 trainable.forward(input_1).backward(output_gradient_1).optimise(); // Epoch 1 // etc., more epochs The problem here of course is that we’re only borrowing trainable for each forward pass, and the borrow will last until it’s been finished with. Rust doesn’t allow multiple mutable borrows in existence at the same time because if it did, we could theoretically write something like this:\nlet mut trainable = get_trainable_network(); let epoch_0 = trainable.forward(input_0).backward(output_gradient_0); // calculate gradients to update parameters with for epoch 0 but don't apply let epoch_1 = trainable.forward(input_1).backward(output_gradient_1); // calculate gradients to update parameters with, but from the same parameters as epoch 0. epoch_0.optimise(); // apply gradients to parameters epoch_1.optimise(); // apply gradients to different parameters used to calculate them! So to avoid this, we take out a mutable borrow for the duration of one epoch which means that we always start the next training pass from a state that we are in sole control over changing - the previous pass has either been dropped or applied.\nThis means we need to store a mutable borrow in the struct as we go through the typestates, and storing a mutable borrow in a struct requires an explicit lifetime.\nWhy can’t we add forward to TrainableOperation? At first you might try to add the forward operation to the trainable trait, which we’d think would look something like this:\npub trait TrainableOperation : Sealed { .. // previous stuff we've covered type Forward; fn forward(\u0026mut self) -\u003e Self::Forward; } However, as mentioned the type of Self::Forward will require an explicit lifetime. The lifetime in this case will be the lifetime of the self borrow to forward. This implies that Forward itself needs to be generic over the lifetime, something like:\npub trait TrainableOperation : Sealed { .. // previous stuff we've covered type Forward\u003c'a\u003e; fn forward\u003c'a\u003e(\u0026'a mut self) -\u003e Self::Forward\u003c'a\u003e; } That is, when we define the associated type Forward, we define it as having a single generic parameter (the lifetime ‘a), and in the forward method we set that lifetime to be the same one that \u0026mut self has.\nThis, unfortunately, is not possible at the time of writing in Rust as it requires a feature called Generic Associated Types which is still being worked on.\nForward trait So, as mentioned before we can do this by putting our forward method into a trait which is itself generic over that lifetime. We can then implement this trait for all lifetimes and when we call “forward” on an implementor, Rust can determine which trait instance should be used.\nThe trait then looks as such:\npub trait Forward\u003c'a\u003e: Sealed + TrainableOperation { type Input; type Output; type Forward; fn forward(\u0026'a mut self, input: Self::Input) -\u003e Result\u003c(Self::Forward, Self::Output)\u003e; } I’ve put an additional bound here to make sure we only implement Forward on those operations that are in the TrainableOperation state. The trait as mentioned is generic over a single lifetime, ‘a, which we then use to explicitly say this is the lifetime of the \u0026mut self borrow.\nFor the function itself, we take in an input and run the forward pass, the result of this will be a tuple where we get access to the output (so we can calculate loss), along with the operation in a state ready to begin the backward pass.\nThe tuple is wrapped in Result because this is a fallible function, because the input provided, as with making predictions, might be incorrectly shaped for the number of neurons expected.\n",
  "wordCount" : "1211",
  "inLanguage": "en",
  "datePublished": "2022-07-26T20:26:21+01:00",
  "dateModified": "2022-07-26T20:26:21+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/trainable/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Robin Smith",
    "logo": {
      "@type": "ImageObject",
      "url": "https://forgottenmaster.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://forgottenmaster.github.io/" accesskey="h" title="Robin Smith (Alt + H)">Robin Smith</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://forgottenmaster.github.io/resume/" title="Résumé">
                    <span>Résumé</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://forgottenmaster.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/">Posts</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/">Machine Learning</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/">Deep Learning From Scratch</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/">Refactoring Eidetic</a></div>
    <h1 class="post-title entry-hint-parent">
      Part 5 - Trainable State
    </h1>
    <div class="post-meta"><span title='2022-07-26 20:26:21 +0100 BST'>July 26, 2022</span>&nbsp;·&nbsp;6 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#trainableoperation" aria-label="TrainableOperation">TrainableOperation</a></li>
                <li>
                    <a href="#during-training" aria-label="During training">During training</a></li>
                <li>
                    <a href="#after-training" aria-label="After training">After training</a></li>
                <li>
                    <a href="#wheres-the-forward-pass" aria-label="Where&rsquo;s the forward pass?">Where&rsquo;s the forward pass?</a></li>
                <li>
                    <a href="#a-consuming-transition" aria-label="A consuming transition">A consuming transition</a></li>
                <li>
                    <a href="#a-non-consuming-transition" aria-label="A non-consuming transition">A non-consuming transition</a></li>
                <li>
                    <a href="#why-cant-we-add-forward-to-trainableoperation" aria-label="Why can&rsquo;t we add forward to TrainableOperation?">Why can&rsquo;t we add forward to TrainableOperation?</a></li>
                <li>
                    <a href="#forward-trait" aria-label="Forward trait">Forward trait</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Now we&rsquo;ve bound a particular optimiser to the operation(s) it is placed into the trainable state. The optimiser instances will retain additional state for gradients, learning rates, etc. that aren&rsquo;t needed in the previous initialised state.</p>
<p>In this typestate we are able to run training passes (as opposed to just predictions) on the network and also, once finished, to go back to the initialised state again from which we can get the trained parameters.</p>
<h1 id="trainableoperation">TrainableOperation<a hidden class="anchor" aria-hidden="true" href="#trainableoperation">#</a></h1>
<p>This trait is pretty straightforward actually. Let&rsquo;s take a look at the definition in it&rsquo;s entirety for a network/operation that is ready to be trained:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">trait</span> TrainableOperation: <span style="color:#a6e22e">Sealed</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">type</span> <span style="color:#a6e22e">Initialised</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">into_initialised</span>(self) -&gt; <span style="color:#a6e22e">Self</span>::Initialised;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">init</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self, epochs: <span style="color:#66d9ef">u16</span>);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">end_epoch</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self);
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>There is only a single associated type with this one which is the type representing the previous state of operation (the initialised state). For functionality we have, as previously mentioned the two pieces of functionality we would like to use.</p>
<h1 id="during-training">During training<a hidden class="anchor" aria-hidden="true" href="#during-training">#</a></h1>
<p>For the actual training process we will need to initialise all the optimisers in the trainable network with the number of epochs that we&rsquo;re running over. This is because those optimisers will use things such as learning rate decay, or otherwise be reliant on the total number of epochs we are running. Initialisation also lets them know we&rsquo;re about to train in epoch 0, so can reset any data that needs to be (such as resetting those decaying learning rates back to starting values).</p>
<p>At the end of each epoch, the trainer will then call <strong>end_epoch</strong> to give any optimisers and such in the network the chance to update state before the next epoch (such as decaying learning rates).</p>
<h1 id="after-training">After training<a hidden class="anchor" aria-hidden="true" href="#after-training">#</a></h1>
<p>After we have finished running our training for however many epochs we&rsquo;d like, we then need to be able to go back to the previous state in order to extract the trained weights for storage, or to make predictions without the overhead of the optimiser instances.</p>
<p>The <strong>into_initialised</strong> method will do this for us and it&rsquo;s fairly straightforward. It simply <em>consumes</em> this trainable instance and produces an initialised form. In effect, this <em>unwraps</em> the trainable operation and strips away any additional data used during the training process (e.g. gradient velocities for momentum) and the resulting operation is lean, only containing what it needs to make predictions.</p>
<h1 id="wheres-the-forward-pass">Where&rsquo;s the forward pass?<a hidden class="anchor" aria-hidden="true" href="#wheres-the-forward-pass">#</a></h1>
<p>As you <em>may</em> have noticed, the trait defining an operation as being in the trainable state doesn&rsquo;t actually have any functionality to run a forward pass (and thus to run a backward pass, etc.).</p>
<p>The reason for this is simple, we need to put that functionality into a <em>separate</em> trait, muchlike we did with the binding of the optimiser and it&rsquo;s for the same reason - generics. However the generic parameters here are not generic types, but <strong>lifetimes</strong>.</p>
<h1 id="a-consuming-transition">A consuming transition<a hidden class="anchor" aria-hidden="true" href="#a-consuming-transition">#</a></h1>
<p>Let&rsquo;s first take a moment to look at what the API would be if the typestate transition from &ldquo;trainable&rdquo; to &ldquo;forward pass&rdquo; consumed self in the same way as the transitions so far do:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> trainable <span style="color:#f92672">=</span> get_trainable_network();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">// run one epoch...we&#39;ll pretend we know the output gradients before we&#39;ve run our
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// forward pass for brevity.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>trainable <span style="color:#f92672">=</span> trainable.forward(input).backward(output_gradients).optimise();</span></span></code></pre></div>
<p>As you can see, the API is a little&hellip;.<em>clumsy</em> since you have to remember to re-assign to the trainable binding after running an epoch, since each step consumes self. If we didn&rsquo;t re-assign and instead did something like:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> trainable <span style="color:#f92672">=</span> get_trainable_network();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">// Epoch 0
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>trainable.forward(input).backward(output_gradient).optimise();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">// Epoch 1
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>trainable.forward(input).backward(output_gradient).optimise(); <span style="color:#75715e">// COMPILER ERROR! trainable already used
</span></span></span></code></pre></div>
<h1 id="a-non-consuming-transition">A non-consuming transition<a hidden class="anchor" aria-hidden="true" href="#a-non-consuming-transition">#</a></h1>
<p>Ideally, what we would like to do is to have the forward transition be non-consuming and only <strong>borrow</strong> the operation for its duration. We would like our API to look more like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> trainable <span style="color:#f92672">=</span> get_trainable_network();
</span></span><span style="display:flex;"><span>trainable.forward(input_0).backward(output_gradient_0).optimise(); <span style="color:#75715e">// Epoch 0
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>trainable.forward(input_1).backward(output_gradient_1).optimise(); <span style="color:#75715e">// Epoch 1
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// etc., more epochs
</span></span></span></code></pre></div>
<p>The problem here of course is that we&rsquo;re only <em>borrowing</em> trainable for each forward pass, and the borrow will last until it&rsquo;s been finished with. Rust doesn&rsquo;t allow multiple <em>mutable</em> borrows in existence at the same time because if it did, we could theoretically write something like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> trainable <span style="color:#f92672">=</span> get_trainable_network();
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> epoch_0 <span style="color:#f92672">=</span> trainable.forward(input_0).backward(output_gradient_0); <span style="color:#75715e">// calculate gradients to update parameters with for epoch 0 but don&#39;t apply
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">let</span> epoch_1 <span style="color:#f92672">=</span> trainable.forward(input_1).backward(output_gradient_1); <span style="color:#75715e">// calculate gradients to update parameters with, but from the same parameters as epoch 0.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>epoch_0.optimise(); <span style="color:#75715e">// apply gradients to parameters
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>epoch_1.optimise(); <span style="color:#75715e">// apply gradients to different parameters used to calculate them!
</span></span></span></code></pre></div>
<p>So to avoid this, we take out a <em>mutable</em> borrow for the duration of one epoch which means that we always start the next training pass from a state that we are in sole control over changing - the previous pass has either been dropped or applied.</p>
<p>This means we need to <em>store</em> a mutable borrow in the struct as we go through the typestates, and storing a mutable borrow in a struct requires an explicit lifetime.</p>
<h1 id="why-cant-we-add-forward-to-trainableoperation">Why can&rsquo;t we add forward to TrainableOperation?<a hidden class="anchor" aria-hidden="true" href="#why-cant-we-add-forward-to-trainableoperation">#</a></h1>
<p>At first you might try to add the forward operation to the trainable trait, which we&rsquo;d think would look something like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">trait</span> TrainableOperation : <span style="color:#a6e22e">Sealed</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">..</span> <span style="color:#75715e">// previous stuff we&#39;ve covered
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">type</span> <span style="color:#a6e22e">Forward</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">forward</span>(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> self) -&gt; <span style="color:#a6e22e">Self</span>::Forward;
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>However, as mentioned the type of Self::Forward will require an explicit lifetime. The lifetime in this case will be the lifetime of the self borrow to forward. This implies that Forward itself needs to be generic over the lifetime, something like:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">trait</span> TrainableOperation : <span style="color:#a6e22e">Sealed</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">..</span> <span style="color:#75715e">// previous stuff we&#39;ve covered
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">type</span> <span style="color:#a6e22e">Forward</span><span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span><span style="color:#f92672">&gt;</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">forward</span><span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span><span style="color:#f92672">&gt;</span>(<span style="color:#f92672">&amp;</span><span style="color:#a6e22e">&#39;a</span> <span style="color:#66d9ef">mut</span> self) -&gt; <span style="color:#a6e22e">Self</span>::Forward<span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span><span style="color:#f92672">&gt;</span>;
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>That is, when we define the associated type Forward, we define it as having a single generic parameter (the lifetime &lsquo;a), and in the forward method we set that lifetime to be the same one that &amp;mut self has.</p>
<p>This, unfortunately, is not possible at the time of writing in Rust as it requires a feature called <strong><a href="https://rust-lang.github.io/rfcs/1598-generic_associated_types.html">Generic Associated Types</a></strong> which is still being worked on.</p>
<h1 id="forward-trait">Forward trait<a hidden class="anchor" aria-hidden="true" href="#forward-trait">#</a></h1>
<p>So, as mentioned before we can do this by putting our forward method into a trait which is <em>itself</em> generic over that lifetime. We can then implement this trait for all lifetimes and when we call &ldquo;forward&rdquo; on an implementor, Rust can determine which trait instance should be used.</p>
<p>The trait then looks as such:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">trait</span> Forward<span style="color:#f92672">&lt;</span><span style="color:#a6e22e">&#39;a</span><span style="color:#f92672">&gt;</span>: <span style="color:#a6e22e">Sealed</span> <span style="color:#f92672">+</span> TrainableOperation {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">type</span> <span style="color:#a6e22e">Input</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">type</span> <span style="color:#a6e22e">Output</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">type</span> <span style="color:#a6e22e">Forward</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">forward</span>(<span style="color:#f92672">&amp;</span><span style="color:#a6e22e">&#39;a</span> <span style="color:#66d9ef">mut</span> self, input: <span style="color:#a6e22e">Self</span>::Input) -&gt; Result<span style="color:#f92672">&lt;</span>(Self::Forward, Self::Output)<span style="color:#f92672">&gt;</span>;
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>I&rsquo;ve put an additional bound here to make sure we only implement Forward on those operations that are in the TrainableOperation state. The trait as mentioned is generic over a single lifetime, &lsquo;a, which we then use to explicitly say this is the lifetime of the <strong>&amp;mut self</strong> borrow.</p>
<p>For the function itself, we take in an input and run the forward pass, the result of this will be a tuple where we get access to the output (so we can calculate loss), along with the operation in a state ready to begin the backward pass.</p>
<p>The tuple is wrapped in Result because this is a fallible function, because the input provided, as with making predictions, might be incorrectly shaped for the number of neurons expected.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://forgottenmaster.github.io/">Robin Smith</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script></body>

</html>
