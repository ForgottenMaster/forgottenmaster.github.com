<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Part 4 - Initialised Operations | Robin Firth</title>
<meta name="keywords" content="" />
<meta name="description" content="After building the network/operation chain and initialising it with weights/parameters either via a known set (through previous training), or via a random seed (using Xavier initialisation) we have a network that is capable of making predictions given some input. We are also at this point able to read out the set of parameters for later use.
We are only able to get an operation into the initialised state by first starting with an uninitialised version and then initialising it, we can&rsquo;t directly construct new instances of this type because it&rsquo;s not publicly visible/nameable.">
<meta name="author" content="">
<link rel="canonical" href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/initialised/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css" integrity="sha256-b2AFbUTT9&#43;tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://forgottenmaster.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://forgottenmaster.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://forgottenmaster.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://forgottenmaster.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://forgottenmaster.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.96.0" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="Part 4 - Initialised Operations" />
<meta property="og:description" content="After building the network/operation chain and initialising it with weights/parameters either via a known set (through previous training), or via a random seed (using Xavier initialisation) we have a network that is capable of making predictions given some input. We are also at this point able to read out the set of parameters for later use.
We are only able to get an operation into the initialised state by first starting with an uninitialised version and then initialising it, we can&rsquo;t directly construct new instances of this type because it&rsquo;s not publicly visible/nameable." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/initialised/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-24T21:06:33&#43;01:00" />
<meta property="article:modified_time" content="2022-07-24T21:06:33&#43;01:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Part 4 - Initialised Operations"/>
<meta name="twitter:description" content="After building the network/operation chain and initialising it with weights/parameters either via a known set (through previous training), or via a random seed (using Xavier initialisation) we have a network that is capable of making predictions given some input. We are also at this point able to read out the set of parameters for later use.
We are only able to get an operation into the initialised state by first starting with an uninitialised version and then initialising it, we can&rsquo;t directly construct new instances of this type because it&rsquo;s not publicly visible/nameable."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://forgottenmaster.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Machine Learning",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Deep Learning From Scratch",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Refactoring Eidetic",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/"
    }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "Part 4 - Initialised Operations",
      "item": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/initialised/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Part 4 - Initialised Operations",
  "name": "Part 4 - Initialised Operations",
  "description": "After building the network/operation chain and initialising it with weights/parameters either via a known set (through previous training), or via a random seed (using Xavier initialisation) we have a network that is capable of making predictions given some input. We are also at this point able to read out the set of parameters for later use.\nWe are only able to get an operation into the initialised state by first starting with an uninitialised version and then initialising it, we can\u0026rsquo;t directly construct new instances of this type because it\u0026rsquo;s not publicly visible/nameable.",
  "keywords": [
    
  ],
  "articleBody": "After building the network/operation chain and initialising it with weights/parameters either via a known set (through previous training), or via a random seed (using Xavier initialisation) we have a network that is capable of making predictions given some input. We are also at this point able to read out the set of parameters for later use.\nWe are only able to get an operation into the initialised state by first starting with an uninitialised version and then initialising it, we can’t directly construct new instances of this type because it’s not publicly visible/nameable.\nThirdly we’re able to take this initialised operation and put it into “training” mode by binding an optimiser with it.\nThe InitialisedOperation trait First we’ll take a look at the overall trait definition:\npub trait InitialisedOperation: Sealed {  type Input;  type Output;  type ParameterIter: IteratorItem = ElementType;   fn iter(\u0026self) - Self::ParameterIter;  fn predict(\u0026self, input: Self::Input) - ResultSelf::Output; } As before, and as with all the traits in Eidetic, this is a Sealed trait to allow us to control implementation details and correctness by implementing it only for operations inside the crate. In fact, it’s likely this won’t be pointed out again on any future traits.\nAs far as associated types go, we need:\n Input. The type of input to the operation, always a tensor type. Output. The type of output from the operation, again a tensor type. ParameterIter. The type of iterator that is produced from the iter method to gain access to the stream of trained weights.  The parameter iterator will produce elements/weights in the same order as they are expected when initialising a network of the exact same architecture via the with_iter method, so this can be used to store those weights off to a file or transmit across a network, or otherwise store for later use.\nThe other capability is the ability to make predictions for the operation via the predict method. This will take an input of the appropriate type, and produce an output. Note that this is a fallible method because the input may not be correctly shaped at runtime for the operation (e.g. an incorrect number of columns/features), so it returns a Result type which can either be propagated or unwrapped as needed.\nWhat’s the next step? You may have noticed that this trait does not have any way to move the type state over to the next state (the trainable state). This is because the output type will depend on the specific optimiser chosen to use to put this operation into a trainable state.\nThis means the trainable state is generic over the optimiser in use, since it needs to hold onto an instance of an optimiser to support adaptive optimisation and things like learning rate decay.\nSince each type can only have one implementation of a given trait, and since we want the initialised operation to be moved to trainable with any compatible optimiser, we need a separate trait that itself is generic over the optimiser type.\nThe WithOptimiser trait This secondary trait is the WithOptimiser trait which looks as follows:\npub trait WithOptimiserT: Sealed {  type Trainable;   fn with_optimiser(self, optimiser: T) - Self::Trainable; } T in this case is the optimiser type we’re using (actually, we’ll see later it’s an optimiser factory because it needs to support making an optimiser that’s generic over a particular type). This lets us implement versions of this trait for each optimiser type in existance which lets the caller pick the optimiser to use.\nThe Trainable type here is the type which is used for this operation that is bound to an instance of that optimiser. For operations that don’t have any parameters and thus, no need for optimisation, they will always produce the same type to represent their trainable form regardless of the optimiser chosen.\nFor types that do require storing an optimiser for later use during the optimisation stage, the correct implementation can be chosen here.\nRust will pick the correct version of the trait to use when the with_optimiser function is called because it will be able to see what the type of T is, due to it requiring an instance into the with_optimiser function. Therefore all of the following calls will be correct:\nlet null = initialised.with_optimiser(NullOptimiser::new()); let sgd = initialised.with_optimiser(SGD::new(FixedLearningRateHandler::new(0.01))); let sgd_momentum = initialised.with_optimiser(SGDMomentum::new(FixedLearningRateHandler::new(0.01), 0.9)); Why the optimiser isn’t an optimiser? As mentioned, the type T that is passed into the with_optimiser function (e.g. NullOptimiser, SGD, etc.) is not implementing the Optimiser trait directly but another trait called OptimiserFactory. We’ll look into more details when we cover the optimisation stage, but the reason for this is because an optimiser type will require storing state that is dependant on the type of parameter for a given optimisation (e.g. Tensor of a certain rank).\nHowever, for operation chains/composite operations we want to be able to take the type T and pass it to both/all operations in the chain - but they might store different parameter types. Therefore the type with_optimiser takes is a factory which can produce optimiser instances of various types when required by those operations.\n",
  "wordCount" : "847",
  "inLanguage": "en",
  "datePublished": "2022-07-24T21:06:33+01:00",
  "dateModified": "2022-07-24T21:06:33+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/initialised/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Robin Firth",
    "logo": {
      "@type": "ImageObject",
      "url": "https://forgottenmaster.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://forgottenmaster.github.io/" accesskey="h" title="Robin Firth (Alt + H)">Robin Firth</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://forgottenmaster.github.io/resume/" title="Résumé">
                    <span>Résumé</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://forgottenmaster.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://forgottenmaster.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/">Posts</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/">Machine Learning</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/">Deep Learning From Scratch</a>&nbsp;»&nbsp;<a href="https://forgottenmaster.github.io/posts/machinelearning/deeplearningfromscratch/refactoring/">Refactoring Eidetic</a></div>
    <h1 class="post-title">
      Part 4 - Initialised Operations
    </h1>
    <div class="post-meta"><span title='2022-07-24 21:06:33 +0100 BST'>July 24, 2022</span>&nbsp;·&nbsp;4 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#the-initialisedoperation-trait" aria-label="The InitialisedOperation trait">The InitialisedOperation trait</a></li>
                <li>
                    <a href="#whats-the-next-step" aria-label="What&amp;rsquo;s the next step?">What&rsquo;s the next step?</a></li>
                <li>
                    <a href="#the-withoptimiser-trait" aria-label="The WithOptimiser trait">The WithOptimiser trait</a></li>
                <li>
                    <a href="#why-the-optimiser-isnt-an-optimiser" aria-label="Why the optimiser isn&amp;rsquo;t an optimiser?">Why the optimiser isn&rsquo;t an optimiser?</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>After building the network/operation chain and initialising it with weights/parameters either via a known set (through previous training), or via a random seed (using Xavier initialisation) we have a network that is capable of making predictions given some input. We are also at this point able to read out the set of parameters for later use.</p>
<p>We are only able to get an operation into the initialised state by first starting with an uninitialised version and then initialising it, we can&rsquo;t directly construct new instances of this type because it&rsquo;s not publicly visible/nameable.</p>
<p>Thirdly we&rsquo;re able to take this initialised operation and put it into &ldquo;training&rdquo; mode by binding an optimiser with it.</p>
<h1 id="the-initialisedoperation-trait">The InitialisedOperation trait<a hidden class="anchor" aria-hidden="true" href="#the-initialisedoperation-trait">#</a></h1>
<p>First we&rsquo;ll take a look at the overall trait definition:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">trait</span> InitialisedOperation: <span style="color:#a6e22e">Sealed</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">type</span> <span style="color:#a6e22e">Input</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">type</span> <span style="color:#a6e22e">Output</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">type</span> <span style="color:#a6e22e">ParameterIter</span>: Iterator<span style="color:#f92672">&lt;</span>Item <span style="color:#f92672">=</span> ElementType<span style="color:#f92672">&gt;</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">iter</span>(<span style="color:#f92672">&amp;</span>self) -&gt; <span style="color:#a6e22e">Self</span>::ParameterIter;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">predict</span>(<span style="color:#f92672">&amp;</span>self, input: <span style="color:#a6e22e">Self</span>::Input) -&gt; Result<span style="color:#f92672">&lt;</span>Self::Output<span style="color:#f92672">&gt;</span>;
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>As before, and as with all the traits in Eidetic, this is a Sealed trait to allow us to control implementation details and correctness by implementing it only for operations inside the crate. In fact, it&rsquo;s likely this won&rsquo;t be pointed out again on any future traits.</p>
<p>As far as associated types go, we need:</p>
<ol>
<li><strong>Input</strong>. The type of input to the operation, always a tensor type.</li>
<li><strong>Output</strong>. The type of output from the operation, again a tensor type.</li>
<li><strong>ParameterIter</strong>. The type of iterator that is produced from the iter method to gain access to the stream of trained weights.</li>
</ol>
<p>The parameter iterator will produce elements/weights in the same order as they are expected when initialising a network of the exact same architecture via the with_iter method, so this can be used to store those weights off to a file or transmit across a network, or otherwise store for later use.</p>
<p>The other capability is the ability to make predictions for the operation via the <strong>predict</strong> method. This will take an input of the appropriate type, and produce an output. Note that this is a <em>fallible</em> method because the input may not be correctly shaped at runtime for the operation (e.g. an incorrect number of columns/features), so it returns a Result type which can either be propagated or unwrapped as needed.</p>
<h1 id="whats-the-next-step">What&rsquo;s the next step?<a hidden class="anchor" aria-hidden="true" href="#whats-the-next-step">#</a></h1>
<p>You may have noticed that this trait does not have any way to move the type state over to the next state (the trainable state). This is because the output type will depend on the specific optimiser chosen to use to put this operation into a trainable state.</p>
<p>This means the trainable state is generic over the optimiser in use, since it needs to hold onto an instance of an optimiser to support adaptive optimisation and things like learning rate decay.</p>
<p>Since each type can only have one implementation of a given trait, and since we want the initialised operation to be moved to trainable with any compatible optimiser, we need a separate trait that itself is generic over the optimiser type.</p>
<h1 id="the-withoptimiser-trait">The WithOptimiser trait<a hidden class="anchor" aria-hidden="true" href="#the-withoptimiser-trait">#</a></h1>
<p>This secondary trait is the WithOptimiser trait which looks as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">trait</span> WithOptimiser<span style="color:#f92672">&lt;</span>T<span style="color:#f92672">&gt;</span>: <span style="color:#a6e22e">Sealed</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">type</span> <span style="color:#a6e22e">Trainable</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">with_optimiser</span>(self, optimiser: <span style="color:#a6e22e">T</span>) -&gt; <span style="color:#a6e22e">Self</span>::Trainable;
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>T in this case is the optimiser type we&rsquo;re using (actually, we&rsquo;ll see later it&rsquo;s an optimiser <em>factory</em> because it needs to support <em>making</em> an optimiser that&rsquo;s generic over a particular type). This lets us implement versions of this trait for each optimiser type in existance which lets the caller pick the optimiser to use.</p>
<p>The Trainable type here is the type which is used for this operation that is bound to an instance of that optimiser. For operations that don&rsquo;t have any parameters and thus, no need for optimisation, they will always produce the same type to represent their trainable form regardless of the optimiser chosen.</p>
<p>For types that do require storing an optimiser for later use during the optimisation stage, the correct implementation can be chosen here.</p>
<p>Rust will pick the correct version of the trait to use when the with_optimiser function is called because it will be able to see what the type of T is, due to it requiring an instance into the with_optimiser function. Therefore all of the following calls will be correct:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-rust" data-lang="rust"><span style="display:flex;"><span><span style="color:#66d9ef">let</span> null <span style="color:#f92672">=</span> initialised.with_optimiser(NullOptimiser::new());
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> sgd <span style="color:#f92672">=</span> initialised.with_optimiser(SGD::new(FixedLearningRateHandler::new(<span style="color:#ae81ff">0.01</span>)));
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">let</span> sgd_momentum <span style="color:#f92672">=</span> initialised.with_optimiser(SGDMomentum::new(FixedLearningRateHandler::new(<span style="color:#ae81ff">0.01</span>), <span style="color:#ae81ff">0.9</span>));</span></span></code></pre></div>
<h1 id="why-the-optimiser-isnt-an-optimiser">Why the optimiser isn&rsquo;t an optimiser?<a hidden class="anchor" aria-hidden="true" href="#why-the-optimiser-isnt-an-optimiser">#</a></h1>
<p>As mentioned, the type T that is passed into the with_optimiser function (e.g. NullOptimiser, SGD, etc.) is not implementing the Optimiser trait directly but another trait called OptimiserFactory. We&rsquo;ll look into more details when we cover the optimisation stage, but the reason for this is because an optimiser type will require storing state that is dependant on the type of parameter for a given optimisation (e.g. Tensor of a certain rank).</p>
<p>However, for operation chains/composite operations we want to be able to take the type T and pass it to both/all operations in the chain - but they might store different parameter types. Therefore the type with_optimiser takes is a factory which can produce optimiser instances of various types when required by those operations.</p>


  </div>

  <footer class="post-footer">
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://forgottenmaster.github.io/">Robin Firth</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
